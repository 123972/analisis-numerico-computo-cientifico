## Equipo 1

### Integrantes:

* Daniel Sharp

* Christian Challu

* Elizabeth Solis

### Título del proyecto:
`Implementación de método de optimización convexa con mínimos cuadrados, a través de descenso en gradiente estocástico`

### Objetivo del proyecto:

### Trabajo escrito:
https://www.sharelatex.com/read/kbgxbypyfjvk

### Presentación
(proximamente)

### Implementación:
En cada avance agregamos una carpeta `codigo` en la cual se encuentra el avance en la implementación correspondiente a esa semana.

#### Referencias del proyecto:  

* Boyd, S. P., & Vandenberghe, L. (2009). Convex optimization. Cambridge: Cambridge University Press.  
  
* Golub, G. H., & F., V. L. (1993). Matrix computations. Baltimore: The John Hopkins University Press.  
  
* Zinkevich, Weimer, Smola, Li. (2010). Parallelized Stochastic Gradient Descent. Neural Information Processing Systems (NIPS) Conference.  

* Kirk, D., & Hwu, W. (2010). Programming massively parallel processors: A hands-on approach. Burlington (Mass.): Morgan Kaufmann.  
  
* John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learningand stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011.  
  
* Diederik P. Kingma and Jimmy Ba.  ADAM: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.  
  
* Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA, second edition, 2006.  
  
* Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of ADAM and beyond. In International Conference on Learning Representations, 2018.  
  
* Sheldon M. Ross. Introductory Statistics. Elsevier, San Diego, CA, USA, second edition, 2005.  
  
* Sebastian U. Stich, Anant Raj, and Martin Jaggi.  Safe adaptive importance sampling. CoRR, abs/1711.02637, 2017.  
  
* Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701, 2012

