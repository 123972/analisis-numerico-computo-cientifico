{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/qb3swgkpaps7yba/4.1.Introduccion_optimizacion_convexa.pdf?dl=0), [liga2](https://www.dropbox.com/s/6isby5h1e5f2yzs/4.2.Problemas_de_optimizacion_convexa.pdf?dl=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización Numérica y machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimización de código ¿es optimización numérica?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este módulo hemos invertido buena parte del tiempo del curso en la eficiente implementación en el hardware que poseemos. Revisamos lo que estudia el análisis numérico o cómputo científico, definiciones de funciones, derivadas, integrales y métodos o algoritmos numéricos para su aproximación. Consideramos *bottlenecks* que pueden surgir en la implementación de los métodos o algoritmos y revisamos posibles opciones para encontrarlos y minimizarlos (**optimización de código**). Lo anterior lo resumimos con el uso de herramientas como: perfilamiento, integración de R y Python con C++ o C, cómputo en paralelo y uso del caché de forma eficiente al usar niveles altos en operaciones de BLAS ([módulo I: cómputo científico y análisis numérico](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico), [módulo II: cómputo en paralelo](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/temas/II.computo_paralelo), [módulo III: cómputo matricial](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/temas/III.computo_matricial)). \n",
    "\n",
    "Si bien la optimización de código no le compete a la **optimización numérica**, sí se apoya enormemente de ella para la implementación de sus métodos o algoritmos en la(s) máquina(s) para resolver problemas que surgen en tal rama de las **matemáticas aplicadas**. A la implementación y simulación en el desarrollo de los métodos o algoritmos del análisis numérico o cómputo científico típicamente se le acompaña de estudios que realizan [benchmarks](https://en.wikipedia.org/wiki/Benchmark_(computing)), mediciones de recursos (tiempo y memoria por ejemplo) y perfilamiento con el objetivo de tener **software confiable y eficiente** en la práctica. Esto lo encontramos también en la rama de optimización numérica con los métodos o algoritmos que son desarrollados e implementados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Métodos o algoritmos numéricos en *big data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de los métodos o algoritmos en el contexto de **grandes cantidades de datos** o *big data* es **crítica** al ir a la práctica pues de esto depende que nuestra(s) máquina(s) tarde meses, semanas, días u horas para resolver problemas que se presentan en este contexto. La ciencia de datos apunta al desarrollo de técnicas y se apoya de aplicaciones de *machine learning* para la extracción de conocimiento útil y toma como fuente de información las grandes cantidades de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Problemas de optimización numérica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una gran cantidad de aplicaciones plantean problemas de optimización matemática o numérica. Tenemos problemas básicos que se presentan en cursos iniciales de cálculo:\n",
    "\n",
    "*Una caja con base y tapa cuadradas debe tener un volumen de $100 cm^3$. Encuentre las dimensiones de la caja que minimicen la cantidad de material.*\n",
    "\n",
    "Y tenemos más especializados que encontramos en áreas como estadística, ingeniería, finanzas o *machine learning*:\n",
    "\n",
    "* Ajustar un modelo de regresión lineal a un conjunto de datos.\n",
    "\n",
    "* Buscar la mejor forma de invertir un capital en un conjunto de activos.\n",
    "\n",
    "* Elección del ancho y largo de un dispositivo en un circuito electrónico.\n",
    "\n",
    "* Ajustar un modelo que clasifique un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general un problema de optimización matemática o numérica tiene la forma:\n",
    "\n",
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} f_i(x) \\leq b_i, i=1,\\dots, m$$\n",
    "\n",
    "donde: $x=(x_1,x_2,\\dots, x_n)^T$ es la **variable de optimización del problema**, la función $f_o: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ es la **función objetivo**, las funciones $f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}, i=1,\\dots,m$ son las **funciones de restricción** (aquí se colocan únicamente desigualdades pero pueden ser sólo igualdades o bien una combinación de ellas) y las constantes $b_1,b_2,\\dots, b_m$ son los **límites o cotas de las restricciones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un vector $x^* \\in \\mathbb{R}^n$ es nombrado **óptimo** o solución del problema anterior si tiene el valor más pequeño de entre todos los vectores $x \\in \\mathbb{R}^n$ que satisfacen las restricciones. Por ejemplo, si $z \\in \\mathbb{R}^n$ satisface $f_1(z) \\leq b_1, f_2(z) \\leq b_2, \\dots, f_m(z) \\leq b_m$ y $x^*$ es óptimo entonces $f_o(z) \\geq f_o(x^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario:** en el curso revisamos métodos o algoritmos de optimización numérica que consideran funciones objetivo $f_o: \\mathbb{R} \\rightarrow \\mathbb{R}^n$. Sin embargo, hay formulaciones que utilizan $f_o: \\mathbb{R}^n \\rightarrow \\mathbb{R}^q$. Tales formulaciones pueden hallarlas en la optimización multicriterio, multiobjetivo, vectorial o Pareto. Ver [Multi objective optimization](https://en.wikipedia.org/wiki/Multi-objective_optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**\n",
    "\n",
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^n} ||x||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} Ax \\leq b$$\n",
    "\n",
    "\n",
    "con $A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m$. En este problema buscamos el vector $x$ que es solución del problema $Ax \\leq b$ con mínima norma Euclidiana. La función objetivo es $f_o(x)=||x||_2$, las funciones de restricción son las desigualdades lineales $f_i(x) = a_i^Tx \\leq b_i$ con $a_i$ $i$-ésimo renglón de $A$ y $b_i$ $i$-ésima componente de $b$, $\\forall i=1,\\dots,m$.\n",
    "\n",
    "Un problema similar al anterior lo podemos encontrar en resolver el sistema de ecuaciones lineales $Ax=b$ *underdetermined* en el que $m < n$ y se busca el vector $x$ con mínima norma Euclidiana que satisfaga tal sistema. Tal sistema puede tener infinitas soluciones o ninguna solución, ver [3.3.Solucion_de_SEL_y_FM](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/III.computo_matricial/3.3.Solucion_de_SEL_y_FM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Machine learning, statistical machine learning y optimización numérica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección relacionamos a *machine learning* con la optimización matemática o numérica y se describen diferentes enfoques que se han propuesto para aplicaciones de *machine learning* con métodos de optimización numérica. Lo siguiente **no** pretende ser una exposición extensa **ni** completa sobre *machine learning*, ustedes llevan materias que se enfocan esencialmente a definir esta área, sus objetivos y conceptos más importantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la ciencia de datos se utilizan las aplicaciones desarrolladas en *machine learning* por ejemplo:\n",
    "\n",
    "* Clasificación de documentos o textos: detección de *spam*.\n",
    "\n",
    "* Procesamiento de lenguaje natural:  [named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition).\n",
    "\n",
    "* Reconocimiento de voz.\n",
    "\n",
    "* [Visión por computadora](https://en.wikipedia.org/wiki/Computer_vision): reconocimiento de rostros o imágenes.\n",
    "\n",
    "* Detección de fraude.\n",
    "\n",
    "* Diagnóstico médico.\n",
    "\n",
    "* Sistemas de recomendación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las aplicaciones anteriores involucran problemas como son:\n",
    "\n",
    "* Clasificación.\n",
    "\n",
    "* Regresión.\n",
    "\n",
    "* *Ranking*.\n",
    "\n",
    "* *Clustering*.\n",
    "\n",
    "* Reducción de la dimensionalidad.\n",
    "\n",
    "En cada una de las aplicaciones o problemas anteriores se utilizan **funciones de pérdida** que guían el proceso de aprendizaje. Tal proceso involucra **optimización numérica de parámetros** de la función de pérdida. Por ejemplo, si la función de pérdida en un problema de regresión es una pérdida cuadrática $\\mathcal{L}(y,\\hat{y}) = (\\hat{y}-y)^2$ con $\\hat{y} = \\hat{\\beta}_0 + \\beta_1x$, entonces el vector de parámetros a optimizar (aprender) es $\n",
    "\\beta=\n",
    "\\left[ \\begin{array}{c}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning* no sólo se apoya de la optimización matemática o numérica pues es un área de Inteligencia Artificial\\* que utiliza técnicas estadísticas para el diseño de sistemas capaces de aplicaciones como las escritas anteriormente, de modo que hoy en día tenemos *statistical machine learning*. No obstante, uno de los **pilares** de *machine learning* o *statistical machine learning* es la optimización matemática o numérica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*La IA o inteligencia artificial es una rama de las ciencias de la computación que atrajo un gran interés en $1950$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning* o *statistical machine learning* se apoya de las formulaciones y algoritmos en optimización matemática o numérica. Sin embargo, también ha contribuido a ésta área desarrollando nuevos enfoques en los métodos o algoritmos numéricos de optimización para el tratamiento de las grandes cantidades de datos o *big data* y estableciendo retos significativos no presentes en problemas clásicos de optimización matemática o numérica. De hecho, al revisar literatura que intersecta estas dos disciplinas encontramos comunidades científicas que desarrollan o utilizan métodos o algoritmos exactos (ver [Exact algorithm](https://en.wikipedia.org/wiki/Exact_algorithm)) y otras que utilizan métodos de optimización estocástica (ver [Stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_optimization)) basados en métodos o algoritmos aproximados (ver [Approximation algorithm](https://en.wikipedia.org/wiki/Approximation_algorithm))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Large scale machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *machine learning* uno de los objetivos bien definidos es encontrar soluciones que generalicen y provean una explicación no compleja del fenómeno en estudio\\*. Esto sigue el principio de la navaja de Occam, ver [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor): para cualquier conjunto de observaciones en general se prefieren explicaciones simples a explicaciones más complicadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*La regularización es una técnica que sigue el principio de Occam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El inicio del siglo XXI estuvo marcado, entre otros temas, por un incremento significativo en la generación de información. Esto puede contrastarse con el desarrollo de los procesadores de las máquinas como se revisó en el  módulo II del curso en el tema [2.1.Un_poco_de_historia_y_generalidades](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.1.Un_poco_de_historia_y_generalidades.ipynb). Asimismo, las mejoras en el performance y precio de dispositivos de almacenamiento o *storage* y sistemas de networking abarató costos de almacenamiento y permitió tal incremento de información.  En este contexto, los modelos y métodos de *statistical machine learning* se vieron limitados por el tiempo de cómputo y no por el tamaño de muestra. La conclusión de esto fue el diseño de métodos o modelos para procesar grandes cantidades de datos usando recursos computacionales comparativamente menores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch algoritmhs and online algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tradicionalmente en la optimización matemática o numérica la búsqueda del (o los) **óptimo(s)** involucran el cálculo de información de primer o segundo orden (ver [1.4.Polinomios_de_Taylor_y_diferenciacion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.4.Polinomios_de_Taylor_y_diferenciacion_numerica.ipynb)) de la función $f_o$. Este cálculo se realiza por lotes o *batch*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**\n",
    "\n",
    "1) Calcular $\\nabla f(x), \\nabla^2f(x)$ con $f: \\mathbb{R}^4 \\rightarrow \\mathbb{R}$, dada por $f(x) = (x_1-2)^2+(2-x_2)^2+x_3^2+x_4^4$ en el punto $x_0=(1.5,1.5,1.5,1.5)^T$. \n",
    "\n",
    "**Solución:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x) = \n",
    "\\left[ \\begin{array}{c}\n",
    "2(x_1-2)\\\\\n",
    "-2(2-x_2)\\\\\n",
    "2x_3\\\\\n",
    "4x_4^3\n",
    "\\end{array}\n",
    "\\right] ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla^2f(x)=\n",
    " \\left[\\begin{array}{cccc}\n",
    "2 & 0 & 0 & 0\\\\\n",
    "0 & 2 & 0 & 0\\\\\n",
    "0 & 0 &2 & 0\\\\\n",
    "0 & 0 &0 &12x_3^2\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x_0) = \n",
    "\\left[ \\begin{array}{c}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "3\\\\\n",
    "\\frac{27}{2}\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla^2f(x_0)=\n",
    " \\left[\\begin{array}{cccc}\n",
    "2 &0&0&0\\\\\n",
    "0&2&0&0\\\\\n",
    "0 &0&2&0\\\\\n",
    "0&0&0&27\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Información de primer y segundo orden lo constituyen el gradiente de $f$, $\\nabla f$, y la matriz Hessiana de $f$, $\\nabla^2f(x)$. Obsérvese que el almacenamiento de la Hessiana involucra $\\mathcal{O}(n^2)$ entradas y en los métodos de optimización matemática o numérica se utiliza para encontrar el mínimo de una función resolviendo un sistema de ecuaciones lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Encontrar el mínimo de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([1.5,1.5,1.5,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf= lambda x: np.array([2*(x[0]-2),\n",
    "                        -2*(2-x[1]),\n",
    "                        2*x[2],\n",
    "                        4*x[3]**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hf = lambda x: np.array([[2, 0, 0 ,0],\n",
    "                         [0, 2, 0, 0],\n",
    "                         [0, 0, 2, 0],\n",
    "                         [0, 0, 0, 27]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1. , -1. ,  3. , 13.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  0,  0],\n",
       "       [ 0,  2,  0,  0],\n",
       "       [ 0,  0,  2,  0],\n",
       "       [ 0,  0,  0, 27]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hf(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $f$ es una función convexa (definida abajo) se tiene que su óptimo se obtiene igualando $\\nabla f(x) = 0$ :\n",
    "\n",
    "$$\\nabla f(x) = \n",
    "\\left[ \\begin{array}{c}\n",
    "2(x_1-2) \\\\\n",
    "-2(2-x_2)\\\\\n",
    "2x_3\\\\\n",
    "4x_4^3\n",
    "\\end{array}\n",
    "\\right]\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dado por $x^* \\in \\mathbb{R}^4:$\n",
    "\n",
    "$$x^*=\n",
    "\\left[ \\begin{array}{c}\n",
    "2\\\\\n",
    "2\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numéricamente se puede utilizar un método iterativo en el que iniciamos con un punto inicial $x^{(0)}$ y las actualizaciones se realizan con el gradiente:\n",
    "\n",
    "$$x^{(k)} = x^{(k-1)} - \\nabla f(x^{(k-1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para $k=1,2,\\dots,$. En el ejemplo anterior tomando $x^{(0)} = (0,0,0,0)^T$ se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0 - gf(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = x_1 - gf(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = x_2 - gf(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_4 = x_3 - gf(x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y aquí nos quedaremos ciclando hasta el infinito..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es utilizar la información de segundo orden con la Hessiana y considerar una actualización:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(k)} = x^{(k-1)} - \\nabla^2 f \\left (x^{(k-1)} \\right )^{-1} \\nabla f\\left(x^{(k-1)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(recordamos que no calculmos inversas de matrices por mayor costo computacional que resolver un sistema de ecuaciones lineales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0 - np.linalg.solve(Hf(x_0),gf(x_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** de acuerdo al ejemplo anterior:\n",
    "\n",
    "* Utilizar información de primer o segundo orden nos ayuda a encontrar óptimo(s) de funciones.\n",
    "\n",
    "* Encontrar al óptimo involucró un método iterativo.\n",
    "\n",
    "* Con la información de primer orden no alcanzamos al óptimo pero con la de segundo orden sí lo alcanzamos en una iteración y tuvimos que resolver un sistema de ecuaciones lineales.\n",
    "\n",
    "* Si consideramos una actualización de la forma:\n",
    "\n",
    "$$x^{(k)} = x^{(k-1)} - t_k\\nabla f(x^{(k-1)})$$\n",
    "\n",
    "con $t_1=0.5$ llegamos al óptimo en una iteración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0 - t_1*gf(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calcular el gradiente involucra menos almacenamiento en memoria que el cálculo de la Hessiana: $\\mathcal{O}(n)$ vs $\\mathcal{O}(n^2)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Optimización numérica convexa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
