{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas para contenedor de docker:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "```\n",
    "docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_numerical -p 8888:8888 -d palmoreck/jupyterlab_numerical:1.1.0\n",
    "```\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "```\n",
    "docker stop jupyterlab_numerical\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación de la imagen de docker `palmoreck/jupyterlab_numerical:1.1.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/numerical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga1](https://drive.google.com/file/d/1zCIHNAxe5Shc36Qo0XjehHgwrafKSJ_t/view), [liga2](https://drive.google.com/file/d/12L7rOCgW7NEKl_xJbIGZz05XXVrOaPBz/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas de optimización convexa con restricciones lineales de igualdad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota se consideran resolver problemas de optimización con restricciones lineales de igualdad de la forma:\n",
    "\n",
    "$$\\min f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} Ax=b$$\n",
    "\n",
    "con variable de optimización $x \\in \\mathbb{R}^{n}$ y $A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p$ conocidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asume lo siguiente:\n",
    "\n",
    "* $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ convexa y $\\mathcal{C}^2(\\text{dom}f_o)$.\n",
    "\n",
    "* $rank(A) = p < n$: tenemos menos restricciones que variables y los renglones de $A$ son linealmente independientes. \n",
    "\n",
    "* Existe un punto óptimo $x^*$ por lo que el problema tiene solución y el valor óptimo se denota por $p^* = f_o(x^*) = \\inf f_o(x)$.\n",
    "\n",
    "* Los puntos iniciales $x^{(0)}$ de los métodos iterativos están en $\\text{dom}f_o$ y los conjuntos $f_o(x^{(0)})$-subnivel son conjuntos cerrados.\n",
    "\n",
    "Ver [1.4.Polinomios_de_Taylor_y_diferenciacion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.4.Polinomios_de_Taylor_y_diferenciacion_numerica.ipynb) y [4.1.Optimizacion_numerica_y_machine_learning](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.1.Optimizacion_numerica_y_machine_learning.ipynb) para definiciones utilizadas en esta nota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos para optimización convexa con restricciones de igualdad (Constrained Equality Convex Optimization: CECO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton aplicado a las [condiciones de Karush-Kuhn-Tucker](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) (KKT) de optimalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema del inicio lo nombramos **primal** (**se recomienda leer el ápendice de esta nota donde se dan definiciones utilizadas en esta sección.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de optimización sin restricciones **la condición** que nos ayudó a determinar si **un punto $x^*$ es óptimo** fue $\\nabla f(x^*) = 0$ (ver definiciones al final de [4.1.Optimizacion_numerica_y_machine_learning](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.1.Optimizacion_numerica_y_machine_learning.ipynb) y la nota [4.2.Algoritmos_para_UCO](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Algoritmos_para_UCO.ipynb)) . Tal condición es **necesaria y suficiente** por la suposición que $f_o$ es convexa diferenciable. Esta condición es en **general** un conjunto de $n$ **ecuaciones no lineales** en $n$ variables que resuelve el problema sin restricciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los problemas de **optimización convexos** con **restricciones lineales o afín de igualdad** las **condiciones de Karush-Kuhn-Tucker de optimalidad** constituyen un conjunto de condiciones **necesarias y suficientes** que ayudan a resolver al problema primal. Tales condiciones KKT de optimalidad para este problema son:\n",
    "\n",
    "$$Ax^* = b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x^*) + A^T\\nu^*=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** \n",
    "\n",
    "* La **ecuación de factibilidad primal** ($1$a ecuación anterior) es una ecuación **lineal**.\n",
    "\n",
    "* La **ecuación de factibilidad dual** ($2$a ecuación anterior) en general es una ecuación **no lineal**.\n",
    "\n",
    "* Por las condiciones KKT de optimalidad: $x^* \\in \\text{dom}f_o$ es óptimo **si y sólo si** $Ax^*=b$ y  $\\exists \\nu^* \\in \\mathbb{R}^p$ tal que $\\nabla f(x^*) + A^T\\nu^*=0$. Entonces resolver el sistema de ecuaciones anteriores para las variables $x^*$ y $\\nu ^*$ equivale a resolver el problema de optimización primal.\n",
    "\n",
    "* Los métodos para resolver el problema primal consisten en:\n",
    "\n",
    "    * Eliminar las restricciones de igualdad a partir de transformaciones al problema primal y utilizar [4.2.Algoritmos_para_UCO](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Algoritmos_para_UCO.ipynb).\n",
    "    * Plantear y resolver el **problema dual** y utilizar [4.2.Algoritmos_para_UCO](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Algoritmos_para_UCO.ipynb).\n",
    "    * **Extender** el método de Newton para problemas sin restricciones (ver [4.2.Metodo_de_Newton_Python](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Metodo_de_Newton_Python.ipynb)) y resolver directamente las condiciones KKT de optimalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota utilizamos el enfoque número $3$: extender el método de Newton. Para una revisión de los otros dos enfoques ver [liga2](https://drive.google.com/file/d/12L7rOCgW7NEKl_xJbIGZz05XXVrOaPBz/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton con punto inicial factible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Newton sin restricciones revisado en [4.2.Metodo_de_Newton_Python](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Metodo_de_Newton_Python.ipynb) se extiende  para el caso de restricciones de igualdad considerando:\n",
    "\n",
    "* El punto inicial $x^{(0)}$ **debe ser factible:** $x^{(0)} \\in \\text{dom}f_o$ y $Ax^{(0)}=b$ (aunque esta restricción puede eliminarse con el método de Newton para **puntos no factibles**, ver [4.4.Metodo_de_Newton_para_puntos_iniciales_no_factibles_Python](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.4.Metodo_de_Newton_para_puntos_iniciales_no_factibles_Python.ipynb)).\n",
    "\n",
    "* El paso de Newton $\\Delta x_{\\text{nt}}$ **debe modificarse** de modo que satisfaga la ecuación de factibilidad primal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Newton con punto inicial factible puede obtenerse de varias formas. Ejemplos son:\n",
    "\n",
    "* Aproximación de segundo orden vía el teorema de Taylor.\n",
    "\n",
    "* Linearización de las condiciones KKT de optimalidad también vía el teorema de Taylor.\n",
    "\n",
    "Para una introducción al teorema de Taylor ver [1.4.Polinomios_de_Taylor_y_diferenciacion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.4.Polinomios_de_Taylor_y_diferenciacion_numerica.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximación de segundo orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supóngase que $x$ es punto factible del problema primal. Por las suposiciones del inicio se cumple por el teorema de Taylor:\n",
    "\n",
    "$$\\hat{f}(x+v) = f(x) + \\nabla f(x)^Tv + \\frac{1}{2}v^T \\nabla ^2 f(x) v$$\n",
    "\n",
    "es una aproximación de **segundo orden** a $f(x)$ con centro en $x$ y variable $v$.\n",
    "\n",
    "El método de Newton para puntos factibles consiste en considerar resolver el problema:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{v \\in \\mathbb{R}^n} \\hat{f}(x+v)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } A(x+v) = b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la variable de optimización $v \\in \\mathbb{R}^n$. Tal problema es un problema de optimización convexa con **función objetivo cuadrática**.\n",
    "\n",
    "Diferentes condiciones determinan si el problema anterior tiene solución y si es única. Tales condiciones tienen una **relación directa** con las condiciones KKT de optimalidad que son:\n",
    "\n",
    "$A(x + v^*) = b$  y por tanto $Av^*=0$ (factibilidad primal) y $\\nabla ^2f(x) v^* + \\nabla f(x) + A^T w^* = 0$ para $w^*$ variable óptima dual asociada al problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribiendo las condiciones anteriores en un sistema nombrado **sistema KKT para el problema primal** con **matriz KKT** se tiene:\n",
    "\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\left[ \\begin{array}{cc}\n",
    "\\nabla^2f(x) & A^T\\\\\n",
    "A & 0\n",
    "\\end{array}\n",
    "\\right] \\cdot \n",
    "\\left[\\begin{array}{c}\n",
    "v\\\\\n",
    "w\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\\begin{array}{c}\n",
    "-\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* El **paso de Newton**, $\\Delta x _{\\text{nt}}$ se define como la **solución única** del sistema anterior con matriz KKT **no singular**. En este caso $v^* = \\Delta x_{\\text{nt}}$ y la solución **analítica** es:\n",
    "\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta x_{\\text{nt}}\\\\\n",
    "w^*\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[ \\begin{array}{cc}\n",
    "\\nabla^2f(x) & A^T\\\\\n",
    "A & 0\n",
    "\\end{array}\n",
    "\\right] ^{-1}\n",
    "\\left[\\begin{array}{c}\n",
    "-\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "* Si la función objetivo $f_o$ del problema primal es cuadrática, entonces la actualización $x + \\Delta x_{\\text{nt}}$ resuelve de forma **exacta** tal problema y la variable $w$ es la variable óptima dual. Si la función objetivo $f_o$ es **cercana** a ser cuadrática, $x + \\Delta x_{\\text{nt}}$ y $w$ deben ser **muy buenas estimaciones** del óptimo primal $x^*$ y del óptimo dual $\\nu ^*$ respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falta incluir decremento de Newton y el algoritmo del método de Newton "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiciones utilizadas en el curso (algunas de ellas...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de esta sección consideramos problemas de optimización de la forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\min f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } f_i(x) \\leq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(x) = 0 \\quad \\forall i=1,\\dots,p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde: $f_o$, $f_i \\forall i=1,\\dots,m$ y $h_i \\forall i=1,\\dots,p$ son funciones diferenciables. Este problema lo nombramos **problema primal de optimización**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** obsérvese que **no** estamos asumiendo que tenemos un problema convexo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La función Lagrangiana $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la **función Lagrangiana** asociada al problema de optimización (primal) como: $\\mathcal{L}: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ con:\n",
    "\n",
    "$$\\mathcal{L}(x, \\lambda , \\nu) = f_o(x) + \\displaystyle \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x)$$\n",
    "\n",
    "y $\\text{dom} \\mathcal{L} = \\mathcal{D} \\times \\mathbb{R}^m \\times \\mathbb{R}^p$ donde: $\\mathcal{D}$ es el dominio del problema de optimización. Ver el apéndice de la nota [4.1.Optimizacion_numerica_y_machine_learning](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.1.Optimizacion_numerica_y_machine_learning.ipynb) para definición del dominio de un problema de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* $\\lambda _i$ se le nombra **multiplicador de Lagrange** asociado con la $i$-ésima restricción de desigualdad $f_i(x) \\leq 0$. \n",
    "\n",
    "* $\\nu_i$ se le nombra **multiplicador de Lagrange** asociado con la $i$-ésima restricción de igualdad $h_i(x)=0$.\n",
    "\n",
    "* Los vectores $\\lambda = (\\lambda_i)_{i=1}^m$ y  $\\nu = (\\nu_i)_{i=1}^p \\in \\mathbb{R}^p$ se les nombran **variables duales** o **vectores de multiplicadores de Lagrange** asociados con el problema de optimización. El vector $x \\in \\mathcal{D}$ se le nombra **variable primal**.\n",
    "\n",
    "\n",
    "**Consideramos en lo que continúa la restricción $\\lambda_i \\geq 0 \\forall i=1,\\dots, m$**.\n",
    "\n",
    "\n",
    "Podemos interpretar a la función Lagrangiana como una **subestimación relajada** del problema primal de optimización. Tal problema puede reescribirse en uno **sin restricciones** de la forma:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min f_o(x) + \\displaystyle \\sum_{i=1}^m I_{-}(f_i(x)) + \\sum_{i=1}^p I_o(h_i(x))$$\n",
    "\n",
    "donde:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I_{-}(u) = \\begin{cases}\n",
    "0 & \\text{si } u \\leq 0,\\\\\n",
    "\\infty & \\text{si } u > 0\\\\\n",
    "\\end{cases}\n",
    "\\quad \n",
    "I_o(u) = \\begin{cases}\n",
    "0 & \\text{si } u=0,\\\\\n",
    "\\infty & \\text{si } u \\neq 0\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** para ver la equivalencia entre este problema **sin restricciones** y el primal piénsese en que a la función $f_o$ se le está añadiendo una **regularización** en donde se penaliza fuertemente a los vectores $x$ que **no** cumplen con $f_i(x) \\leq 0$ o $h_i(x) = 0$. Ver [4.3.Minimos_cuadrados_R](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.3.Minimos_cuadrados_R.ipynb) para regularización en un problema de mínimos cuadrados y la relación de la regularización con problemas *bi-criterion* de optimización.\n",
    "\n",
    "Como $\\lambda_i \\geq 0$ se tiene: \n",
    "\n",
    "\n",
    "$$\\lambda_i f_i(x) \\leq I_{-}(f_i(x)) \\quad \\forall i=1,\\dots,m$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nu_ih_i(x) \\leq I_o(h_i(x)) \\quad \\forall i=1,\\dots,p.$$\n",
    "\n",
    "La Lagrangiana $\\mathcal{L}(x, \\nu) = f_o(x) + \\displaystyle \\sum_{i=1}^m \\nu_i h_i(x)$ entonces cumple $\\mathcal{L}(x, \\nu) \\leq f_o(x) + \\displaystyle \\sum_{i=1}^p I_o(h_i(x))$ $\\forall (x, \\nu ) \\in \\text{dom}\\mathcal{L}$. Esto es, $\\mathcal{L}(x,\\nu)$ subestima a $f_o(x) + \\displaystyle \\sum_{i=1}^m \\lambda_i f_i(x) +  \\sum_{i=1}^m \\nu_i h_i(x)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función dual de Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función dual de Lagrange se define como $g: \\mathbb{R}^m \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ con:\n",
    "\n",
    "$$g(\\lambda, \\nu) = \\displaystyle \\inf_{x \\in \\mathcal{D}} \\mathcal{L}(x,\\lambda, \\nu)$$\n",
    "\n",
    "**Comentarios:**\n",
    "\n",
    "* Recuérdese que $\\inf$ es el máximo de las cotas inferiores, ver [1.3.Condicion_de_un_problema_y_estabilidad_de_un_algoritmo](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.3.Condicion_de_un_problema_y_estabilidad_de_un_algoritmo.ipynb) para definición de supremo e ínfimo. En esta definción se está tomando el ínfimo de la Lagrangiana para cada punto $x \\in \\mathcal{D}$ (ínfimo puntual).\n",
    "\n",
    "* Por la definición de la función dual de Lagrange y el punto anterior, se tiene: $g(\\lambda, \\nu)  \\leq \\mathcal{L}(\\tilde{x},\\lambda, \\nu)$ para $\\tilde{x}$ factible.\n",
    "\n",
    "* La función dual de Lagrange para cada par $(\\lambda, \\nu)$ con $\\lambda \\geq 0$ da una **subestimación de $p^*$, el valor óptimo del problema primal**. Esto es: $g(\\lambda, \\nu) \\leq p^*$ para $\\lambda \\geq 0, \\nu \\in \\mathbb{R}^p$ y $p^*$ valor óptimo del problema de optimización primal. \n",
    "\n",
    "* Si la Lagrangiana no es acotada por debajo entonces la función dual de Lagrange se asigna $-\\infty$.\n",
    "\n",
    "* Se prueba que la función dual de Lagrange **siempre es una función cóncava**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema dual de Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociado a **todo** problema de optimización existe su **problema dual** el cual se define como:\n",
    "\n",
    "$$\\displaystyle \\max g(\\lambda,\\nu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } \\lambda \\geq 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "\n",
    "* Si $(\\lambda, \\nu)$ satisface $\\lambda \\geq 0$ y $g(\\lambda, \\nu)$ no es $-\\infty$ la pareja $(\\lambda, \\nu)$ se nombra **dual factible**.\n",
    "\n",
    "* El **valor óptimo del problema dual** se denota con $d^*$. **El valor óptimo se alcanza** si existen $(\\lambda^*, \\nu^*)$ tales que $d^* = g(\\lambda^*, \\nu^*)$. \n",
    "\n",
    "* La pareja $(\\lambda^*, \\nu^*)$ se nombra **óptimo dual** o **pareja óptima de multiplicadores de Lagrange** si satisface:\n",
    "\n",
    "    * $\\lambda^* \\geq 0$, \n",
    "    * $g(\\lambda^*, \\nu^*)$ no es $-\\infty$ y \n",
    "    * $(\\lambda^*, \\nu^*)$ resuelve el problema dual. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para distinguir a los óptimos duales de los puntos óptimos $x^*$ del problema primal, a éste últimos se les nombra **óptimos primales**.\n",
    "\n",
    "* El problema dual tiene como interpretación **la mejor cota inferior de la función Lagrangiana como función de $x$ que se obtiene con la función dual de Lagrange**.\n",
    "\n",
    "* El problema dual de Lagrange **siempre es un problema de optimización convexo aún si el problema primal no es convexo.**\n",
    "\n",
    "* A la diferencia $p^* - d^*$ se le nombra ***optimal duality gap*.** Si es positiva se tiene un **problema de optimización en el que se cumple dualidad débil**. Si es igual a cero se tiene un **problema de optimización en el que se cumple dualidad fuerte**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dualidad fuerte y holgura complementaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un problema de optimización en el que los valores óptimos primal y dual se alcanzan y en el que se cumple dualidad fuerte se tiene la propiedad que el **punto óptimo primal minimiza $\\mathcal{L}(x,\\lambda^*, \\nu^*)$** pues:\n",
    "\n",
    "$$f_o(x^*)  = p^* = d^* = g(\\lambda^*, \\nu^*) = \\inf_{x \\in \\mathcal{D}} \\mathcal{L}(x, \\lambda^*, \\nu^*)$$\n",
    "\n",
    "**Comentario:** lo anterior prueba que $\\lambda^*_i f_i(x^*)=0 \\quad  \\forall i=1, \\dots, m$. Tal ecuación se conoce con el nombre de **holgura complementaria**. La holgura complementaria indica que si el $i$-ésimo multiplicador de Lagrange es positivo entonces la $i$-ésima función de restricción de desigualdad $f_i$ es activa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda^*_i > 0 \\implies f_i(x^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(x^*) < 0 \\implies \\lambda^*_i = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condiciones KKT para problemas de optimización (convexos o no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sean $x^*, (\\lambda^*, \\nu^*)$ un par primal-dual de puntos óptimos en los que la *optimal duality gap* es cero o bien: $d^* = p^*$. Entonces $x^*$ minimiza $\\mathcal{L}(x, \\lambda^*, \\nu^*)$ y por factibilidad y holgura complementaria se cumple:\n",
    "\n",
    "$$\\nabla_x \\mathcal{L}(x^*, \\lambda^*, \\nu^*) = \\nabla f_o(x^*) + \\displaystyle \\sum_{i=1}^m \\lambda^*_i f_i(x^*)  + \\sum_{i=1}^p \\nu_i^* \\nabla h_i(x^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(x^*) \\leq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(x^*)=0 \\quad \\forall i = 1,\\dots, p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_i^* \\geq 0 \\quad \\forall i =1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda^*_if_i(x^*) = 0 \\quad \\forall i=1,\\dots,m.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las $5$ condiciones anteriores se nombran **condiciones KKT de optimalidad** para el problema de optimización del inicio. \n",
    "\n",
    "**Comentarios:**\n",
    "\n",
    "* Las condiciones KKT de optimalidad se satisfacen para **cualquier problema de optimización con función objetivo  y funciones de restricción diferenciables evaluadas en un par primal-dual de puntos óptimos en el que se cumple la dualidad fuerte**. \n",
    "\n",
    "* La notación $\\nabla_x$ se refiere a la derivada de $\\mathcal{L}$ respecto a la variable $x$.\n",
    "\n",
    "*  Son condiciones necesarias.\n",
    "\n",
    "* La ecuación $h_i(x^*)=0 \\forall i=1,\\dots,p$  y $f_i(x^*) \\leq 0 \\forall i=1,\\dots,m$ se nombran **ecuaciones de factibilidad primal** y $\\nabla_x \\mathcal{L}(x^*,\\lambda^*, \\nu^*)=0$ se nombra **ecuación de factibilidad dual**.\n",
    "\n",
    "\n",
    "* La dualidad fuerte y que el óptimo del problema dual se alcance puede establecerse a partir de condiciones teóricas sobre el problema de optimización primal. Un ejemplo son las de Slater, ver [Slater's condition](https://en.wikipedia.org/wiki/Slater%27s_condition). Para condiciones más generales aplicables a problemas de optimización primal no necesariamente convexo existen las **condiciones de regularidad**, ver [regurlarity conditions or constraint qualifications](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condiciones KKT para problemas de optimización convexos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el problema **primal** es convexo, esto es, $f_o, f_1, \\dots, f_m$ son funciones convexas y $h_i$ es afín $\\forall i,\\dots, p$ entonces las condiciones KKT son **suficientes** para puntos primal-dual óptimos:\n",
    "\n",
    "Sean $\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}$ puntos que satisfacen las condiciones KKT de optimalidad:\n",
    "\n",
    "$$\\nabla_x \\mathcal{L}(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}) = \\nabla f_o(\\tilde{x})  +  \\displaystyle \\sum_{i=1}^m \\tilde{\\lambda}_i f_i(\\tilde{x}) + \\sum_{i=1}^p \\tilde{\\nu}_i \\nabla h_i(\\tilde{x}) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(\\tilde{x}) \\leq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(\\tilde{x})=0 \\quad \\forall i = 1,\\dots, p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{\\lambda_i} \\geq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{\\lambda}_if_i(\\tilde{x}) = 0 \\quad \\forall i=1,\\dots,m.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**entonces el par $(\\tilde{x},\\tilde{\\nu})$ son puntos primal-dual óptimos y en el problema de optimización se cumple la dualidad fuerte.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario:** en algunos casos especiales es posible resolver las condiciones KKT de optimalidad de manera analítica y en general muchos algoritmos de optimización resultan o pueden interpretarse como métodos que las resuelven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "* S. P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
