{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas para contenedor de docker:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "```\n",
    "docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_numerical -p 8888:8888 -d palmoreck/jupyterlab_numerical:1.1.0\n",
    "```\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "```\n",
    "docker stop jupyterlab_numerical\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación de la imagen de docker `palmoreck/jupyterlab_numerical:1.1.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/numerical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga1](https://drive.google.com/file/d/1zCIHNAxe5Shc36Qo0XjehHgwrafKSJ_t/view), [liga2](https://drive.google.com/file/d/12L7rOCgW7NEKl_xJbIGZz05XXVrOaPBz/view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts futurize and pasteurize are installed in '/home/miuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user -q cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas de optimización convexa con restricciones lineales de igualdad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota se consideran resolver problemas de optimización con restricciones lineales de igualdad de la forma:\n",
    "\n",
    "$$\\min f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} Ax=b$$\n",
    "\n",
    "con variable de optimización $x \\in \\mathbb{R}^{n}$ y $A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p$ conocidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asume lo siguiente:\n",
    "\n",
    "* $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ convexa y $\\mathcal{C}^2(\\text{dom}f_o)$.\n",
    "\n",
    "* $rank(A) = p < n$: tenemos menos restricciones que variables y los renglones de $A$ son linealmente independientes. \n",
    "\n",
    "* Existe un punto óptimo $x^*$ por lo que el problema tiene solución y el valor óptimo se denota por $p^* = f_o(x^*) = \\inf f_o(x)$.\n",
    "\n",
    "* Los puntos iniciales $x^{(0)}$ de los métodos iterativos están en $\\text{dom}f_o$ y los conjuntos $f_o(x^{(0)})$-subnivel son conjuntos cerrados.\n",
    "\n",
    "Ver [1.4.Polinomios_de_Taylor_y_diferenciacion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.4.Polinomios_de_Taylor_y_diferenciacion_numerica.ipynb) y [4.1.Optimizacion_numerica_y_machine_learning](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.1.Optimizacion_numerica_y_machine_learning.ipynb) para definiciones utilizadas en esta nota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton aplicado a las [condiciones de Karush-Kuhn-Tucker](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) (KKT) de optimalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema del inicio lo nombramos **primal** (**se recomienda leer el ápendice de esta nota donde se dan definiciones utilizadas en esta sección.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de optimización sin restricciones **la condición** que nos ayudó a determinar si **un punto $x^*$ es óptimo** fue $\\nabla f(x^*) = 0$ (ver definiciones al final de [4.1.Optimizacion_numerica_y_machine_learning](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.1.Optimizacion_numerica_y_machine_learning.ipynb) y la nota [4.2.Algoritmos_para_UCO](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Algoritmos_para_UCO.ipynb)) . Tal condición es **necesaria y suficiente** por la suposición que $f_o$ es convexa diferenciable. Esta condición es en **general** un conjunto de $n$ **ecuaciones no lineales** en $n$ variables que resuelve el problema sin restricciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los problemas de **optimización convexos** con **restricciones lineales o afín de igualdad** las **condiciones de Karush-Kuhn-Tucker de optimalidad** constituyen un conjunto de condiciones **necesarias y suficientes** que ayudan a resolver al problema primal. Tales condiciones KKT de optimalidad para este problema son:\n",
    "\n",
    "$$Ax^* = b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x^*) + A^T\\nu^*=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** \n",
    "\n",
    "* La **ecuación de factibilidad primal** ($1$a ecuación anterior) es una ecuación **lineal**.\n",
    "\n",
    "* La **ecuación de factibilidad dual** ($2$a ecuación anterior) en general es una ecuación **no lineal**.\n",
    "\n",
    "* Por las condiciones KKT de optimalidad: $x^* \\in \\text{dom}f_o$ es óptimo **si y sólo si** $Ax^*=b$ y  $\\exists \\nu^* \\in \\mathbb{R}^p$ tal que $\\nabla f(x^*) + A^T\\nu^*=0$. Entonces resolver el sistema de ecuaciones anteriores para las variables $x^*$ y $\\nu ^*$ equivale a resolver el problema de optimización primal.\n",
    "\n",
    "* Los métodos para resolver el problema primal consisten en:\n",
    "\n",
    "    * Eliminar las restricciones de igualdad a partir de transformaciones al problema primal y utilizar [4.2.Algoritmos_para_UCO](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Algoritmos_para_UCO.ipynb).\n",
    "    * Plantear y resolver el **problema dual** y utilizar [4.2.Algoritmos_para_UCO](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Algoritmos_para_UCO.ipynb).\n",
    "    * **Extender** el método de Newton para problemas sin restricciones (ver [4.2.Metodo_de_Newton_Python](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Metodo_de_Newton_Python.ipynb)) y resolver directamente las condiciones KKT de optimalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota utilizamos el enfoque número $3$: extender el método de Newton. Para una revisión de los otros dos enfoques ver [liga2](https://drive.google.com/file/d/12L7rOCgW7NEKl_xJbIGZz05XXVrOaPBz/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton con punto inicial factible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Newton sin restricciones revisado en [4.2.Metodo_de_Newton_Python](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.2.Metodo_de_Newton_Python.ipynb) se extiende  para el caso de restricciones de igualdad considerando:\n",
    "\n",
    "* El punto inicial $x^{(0)}$ **debe ser factible:** $x^{(0)} \\in \\text{dom}f_o$ y $Ax^{(0)}=b$ (aunque esta restricción puede eliminarse con el método de Newton para **puntos no factibles**, ver [4.4.Metodo_de_Newton_para_puntos_iniciales_no_factibles_Python](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.4.Metodo_de_Newton_para_puntos_iniciales_no_factibles_Python.ipynb)).\n",
    "\n",
    "* El paso de Newton $\\Delta x_{\\text{nt}}$ **debe modificarse** de modo que satisfaga la ecuación de factibilidad primal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Newton con punto inicial factible puede obtenerse de varias formas. Ejemplos son:\n",
    "\n",
    "* Aproximación de segundo orden vía el teorema de Taylor.\n",
    "\n",
    "* Linearización de las condiciones KKT de optimalidad también vía el teorema de Taylor.\n",
    "\n",
    "Para una introducción al teorema de Taylor ver [1.4.Polinomios_de_Taylor_y_diferenciacion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.4.Polinomios_de_Taylor_y_diferenciacion_numerica.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximación de segundo orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supóngase que $x$ es punto factible del problema primal. Por las suposiciones del inicio se cumple por el teorema de Taylor:\n",
    "\n",
    "$$\\hat{f}(x+v) = f(x) + \\nabla f(x)^Tv + \\frac{1}{2}v^T \\nabla ^2 f(x) v$$\n",
    "\n",
    "es una aproximación de **segundo orden** a $f(x)$ con centro en $x$ y variable $v$.\n",
    "\n",
    "El método de Newton para puntos factibles consiste en considerar resolver el problema:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{v \\in \\mathbb{R}^n} \\hat{f}(x+v)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } A(x+v) = b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la variable de optimización $v \\in \\mathbb{R}^n$. Tal problema es un problema de optimización convexa con **función objetivo cuadrática**.\n",
    "\n",
    "Diferentes condiciones determinan si el problema anterior tiene solución y si es única. Tales condiciones tienen una **relación directa** con las condiciones KKT de optimalidad que son:\n",
    "\n",
    "$A(x + v^*) = b$  y por tanto $Av^*=0$ (factibilidad primal) y $\\nabla ^2f(x) v^* + \\nabla f(x) + A^T w^* = 0$ para $w^*$ variable óptima dual asociada al problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribiendo las condiciones anteriores en un sistema nombrado **sistema KKT para el problema primal** con **matriz KKT** se tiene:\n",
    "\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\left[ \\begin{array}{cc}\n",
    "\\nabla^2f(x) & A^T\\\\\n",
    "A & 0\n",
    "\\end{array}\n",
    "\\right] \\cdot \n",
    "\\left[\\begin{array}{c}\n",
    "v\\\\\n",
    "w\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\\begin{array}{c}\n",
    "-\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* El **paso de Newton**, $\\Delta x _{\\text{nt}}$ se define como la **solución única** del sistema anterior con matriz KKT **no singular**. En este caso $v^* = \\Delta x_{\\text{nt}}$ y la solución **analítica** es:\n",
    "\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta x_{\\text{nt}}\\\\\n",
    "w^*\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[ \\begin{array}{cc}\n",
    "\\nabla^2f(x) & A^T\\\\\n",
    "A & 0\n",
    "\\end{array}\n",
    "\\right] ^{-1}\n",
    "\\left[\\begin{array}{c}\n",
    "-\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "* Si la función objetivo $f_o$ del problema primal es cuadrática, entonces la actualización $x + \\Delta x_{\\text{nt}}$ resuelve de forma **exacta** tal problema y la variable $w$ es la variable óptima dual. Si la función objetivo $f_o$ es **cercana** a ser cuadrática, $x + \\Delta x_{\\text{nt}}$ y $w$ deben ser **muy buenas estimaciones** del óptimo primal $x^*$ y del óptimo dual $\\nu ^*$ respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Equality Convex Optimization (CECO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comparan los resultados del **paquete [cvxpy](https://github.com/cvxgrp/cvxpy)** con los obtenidos en la implementación hecha por el prof en [algoritmos/Python](algoritmos/Python), en específico [algoritmos/Python/algorithms_for_ceco.py](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/algoritmos/Python/algorithms_for_ceco.py) para problemas tipo CECO (*Constrained Equality Convex Optimization*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_alg_python = '/algoritmos/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cur_directory + dir_alg_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import algorithms_for_ceco\n",
    "from line_search import line_search_for_residual_by_backtracking\n",
    "from utils import compute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^2} \\quad x_1^2 + 2x_1x_2 + x_2^2-2x_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } x_1 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = lambda x: x[0]**2 + 2*x[0]*x[1]+x[1]**2-2*x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1,0],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ast=np.array([0,1], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto inicial $x^{(0)}$ factible ($Ax^{(0)}=b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0,-2],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tNormgf \tNewton Decrement\tError x_ast\tError p_ast\tline search\tCondHf\n",
      "0\t7.21e+00\t1.80e+01\t3.00e+00\t9.00e+00\t---\t\t9.00e+03\n",
      "1\t7.21e+00\t1.15e-05\t2.40e-03\t5.76e-06\t1.00e+00\t9.00e+03\n",
      "2\t7.21e+00\t7.55e-15\t6.68e-08\t4.44e-15\t1.00e+00\t9.00e+03\n",
      "Error of x with respect to x_ast: 6.68e-08\n",
      "Approximate solution: [4.33680869e-19 9.99999933e-01]\n"
     ]
    }
   ],
   "source": [
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=fo(x_ast)\n",
    "[x,total_of_iterations,\n",
    " Err_plot,x_plot]=algorithms_for_ceco.Newtons_method_feasible_init_point(fo,A, x_0,tol, \n",
    "                                                                         tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.33680869e-19, 9.99999933e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_of_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  4.33680869e-19],\n",
       "       [-2.00000000e+00,  1.00239973e+00,  9.99999933e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.682210107467057e-08"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(x_ast,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En el siguiente ejemplo si usamos el algoritmo anterior para punto inicial no factible ($Ax^{(0)} \\neq b$) obsérvese que no funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([1,-2],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tNormgf \tNewton Decrement\tError x_ast\tError p_ast\tline search\tCondHf\n",
      "0\t4.47e+00\t8.01e+00\t3.16e+00\t6.00e+00\t---\t\t2.03e+07\n",
      "1\t4.47e+00\t5.12e-06\t1.41e+00\t2.00e+00\t1.00e+00\t2.03e+07\n",
      "2\t4.47e+00\t9.86e-16\t1.41e+00\t2.00e+00\t1.00e+00\t2.03e+07\n",
      "Error of x with respect to x_ast: 1.41e+00\n",
      "Approximate solution: [ 1.00000000e+00 -3.34455919e-08]\n"
     ]
    }
   ],
   "source": [
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=fo(x_ast)\n",
    "[x,total_of_iterations,\n",
    " Err_plot,x_plot]=algorithms_for_ceco.Newtons_method_feasible_init_point(fo,A, x_0,tol, \n",
    "                                                                         tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135860226999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(x_ast,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error relativo muy alto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segundo ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^2} \\quad \\frac{1}{2}(x_1^2 + x_2^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } 2x_1 -x_2 = 5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = lambda x: 1/2*(x[0]**2  + x[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2,-1],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ast = np.array([2,-1],dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto inicial $x^{(0)}$ factible ($Ax^{(0)}=b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0,-5],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tNormgf \tNewton Decrement\tError x_ast\tError p_ast\tline search\tCondHf\n",
      "0\t5.00e+00\t2.00e+01\t2.00e+00\t4.00e+00\t---\t\t1.00e+00\n",
      "1\t5.00e+00\t3.94e-06\t8.88e-04\t7.89e-07\t1.00e+00\t1.00e+00\n",
      "2\t5.00e+00\t3.56e-12\t8.63e-07\t7.45e-13\t1.00e+00\t1.00e+00\n",
      "Error of x with respect to x_ast: 8.63e-07\n",
      "Approximate solution: [ 1.99999914 -1.00000173]\n"
     ]
    }
   ],
   "source": [
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=fo(x_ast)\n",
    "[x,total_of_iterations,\n",
    " Err_plot,x_plot]=algorithms_for_ceco.Newtons_method_feasible_init_point(fo,A, x_0,tol, \n",
    "                                                                         tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.99999914, -1.00000173])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.631750624932266e-07"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(x_ast,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tercer ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^2} \\quad x_1^2 + x_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a :} x_1+x_2 = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = lambda x: x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ast = np.array([.5,.5],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1,1],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto inicial $x^{(0)}$ factible ($Ax^{(0)}=b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([2,-1],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tNormgf \tNewton Decrement\tError x_ast\tError p_ast\tline search\tCondHf\n",
      "0\t4.47e+00\t9.00e+00\t3.00e+00\t9.00e+00\t---\t\t1.00e+00\n",
      "1\t4.47e+00\t7.11e-08\t2.67e-04\t7.11e-08\t1.00e+00\t1.00e+00\n",
      "2\t4.47e+00\t1.81e-16\t2.46e-12\t1.11e-16\t1.00e+00\t1.00e+00\n",
      "Error of x with respect to x_ast: 2.46e-12\n",
      "Approximate solution: [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=fo(x_ast)\n",
    "[x,total_of_iterations,\n",
    " Err_plot,x_plot]=algorithms_for_ceco.Newtons_method_feasible_init_point(fo,A, x_0,tol, \n",
    "                                                                         tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4647506264441902e-12"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(x_ast,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuarto ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^2} \\quad e^{x_1+3x_2-0.1} + e^{x_1 -3x_2-0.1} + e^{-x_1-0.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} x_1 + 3x_2 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = lambda x: math.exp(x[0] + 3*x[1]-0.1) + math.exp(x[0]  -3*x[1]-0.1) + math.exp(-x[0]-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ast = np.array([-0.23104907880100917,0.0770163596518852],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1,3],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto inicial $x^{(0)}$ factible ($Ax^{(0)}=b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0,0],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tNormgf \tNewton Decrement\tError x_ast\tError p_ast\tline search\tCondHf\n",
      "0\t9.05e-01\t1.81e-01\t1.00e+00\t3.81e-02\t---\t\t6.00e+00\n",
      "1\t9.05e-01\t3.30e-03\t1.34e-01\t6.37e-04\t1.00e+00\t6.00e+00\n",
      "2\t9.05e-01\t8.46e-07\t2.15e-03\t1.62e-07\t1.00e+00\t6.00e+00\n",
      "3\t9.05e-01\t7.41e-14\t6.52e-07\t1.98e-11\t1.00e+00\t6.00e+00\n",
      "Error of x with respect to x_ast: 6.52e-07\n",
      "Approximate solution: [-0.23104893  0.07701631]\n"
     ]
    }
   ],
   "source": [
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=fo(x_ast)\n",
    "[x,total_of_iterations,\n",
    " Err_plot,x_plot]=algorithms_for_ceco.Newtons_method_feasible_init_point(fo,A, x_0,tol, \n",
    "                                                                         tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.524588707146708e-07"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(x_ast, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quinto ejemplo: con más restricciones de igualdad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolver: \n",
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^3} \\quad ||x||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: }\\begin{array}{l}\n",
    "\\begin{array}{c}\n",
    "x_1 + x_2 + x_3 = 1 \\\\\n",
    "x_1 + x_2 + 2x_3 = 3\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = lambda x: x.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ast = np.array([-0.5,-0.5,2. ], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 1, 1],\n",
    "              [1, 1, 2]],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto inicial $x^{(0)}$ factible ($Ax^{(0)}=b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([3,-4,2], dtype=float)\n",
    "#x_0 = np.array([2,-3,2], dtype=float) #este es otro punto factible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "p_ast=fo(x_ast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tNormgf \tNewton Decrement\tError x_ast\tError p_ast\tline search\tCondHf\n",
      "0\t1.08e+01\t4.90e+01\t2.33e+00\t5.44e+00\t---\t\t1.00e+00\n",
      "1\t1.08e+01\t3.14e-05\t1.87e-03\t3.48e-06\t1.00e+00\t1.00e+00\n",
      "2\t1.08e+01\t5.83e-13\t2.52e-07\t6.45e-14\t1.00e+00\t1.00e+00\n",
      "Error of x with respect to x_ast: 2.52e-07\n",
      "Approximate solution: [-0.49999962 -0.50000038  2.        ]\n"
     ]
    }
   ],
   "source": [
    "[x,total_of_iterations,\n",
    " Err_plot,x_plot]=algorithms_for_ceco.Newtons_method_feasible_init_point(fo,A, x_0,tol, \n",
    "                                                                         tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.518419818612709e-07"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(x_ast,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación del quinto ejemplo con [cvxpy](https://github.com/cvxgrp/cvxpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cp.Variable(3)\n",
    "A = np.array([[1, 1, 1],\n",
    "              [1, 1, 2]])\n",
    "b = np.array([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = cp.Minimize(cp.norm(x,2))\n",
    "\n",
    "constraints = [A@x == b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = cp.Problem(obj, constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1213203277662585"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal value 2.1213203277662585\n"
     ]
    }
   ],
   "source": [
    "print(\"optimal value\", prob.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal var [-0.5 -0.5  2. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"optimal var\", x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.121320343558957"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros puntos iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = [1,-2,2]\n",
    "A@vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = [2,-3,2]\n",
    "A@vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.123105625617661"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = [3,-4,2]\n",
    "A@vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.385164807134504"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiciones utilizadas en el curso (algunas de ellas...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de esta sección consideramos problemas de optimización de la forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\min f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } f_i(x) \\leq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(x) = 0 \\quad \\forall i=1,\\dots,p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde: $f_o$, $f_i \\forall i=1,\\dots,m$ y $h_i \\forall i=1,\\dots,p$ son funciones diferenciables. Este problema lo nombramos **problema primal de optimización**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** obsérvese que **no** estamos asumiendo que tenemos un problema convexo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La función Lagrangiana $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la **función Lagrangiana** asociada al problema de optimización (primal) como: $\\mathcal{L}: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ con:\n",
    "\n",
    "$$\\mathcal{L}(x, \\lambda , \\nu) = f_o(x) + \\displaystyle \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x)$$\n",
    "\n",
    "y $\\text{dom} \\mathcal{L} = \\mathcal{D} \\times \\mathbb{R}^m \\times \\mathbb{R}^p$ donde: $\\mathcal{D}$ es el dominio del problema de optimización. Ver el apéndice de la nota [4.1.Optimizacion_numerica_y_machine_learning](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.1.Optimizacion_numerica_y_machine_learning.ipynb) para definición del dominio de un problema de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* $\\lambda _i$ se le nombra **multiplicador de Lagrange** asociado con la $i$-ésima restricción de desigualdad $f_i(x) \\leq 0$. \n",
    "\n",
    "* $\\nu_i$ se le nombra **multiplicador de Lagrange** asociado con la $i$-ésima restricción de igualdad $h_i(x)=0$.\n",
    "\n",
    "* Los vectores $\\lambda = (\\lambda_i)_{i=1}^m$ y  $\\nu = (\\nu_i)_{i=1}^p \\in \\mathbb{R}^p$ se les nombran **variables duales** o **vectores de multiplicadores de Lagrange** asociados con el problema de optimización. El vector $x \\in \\mathcal{D}$ se le nombra **variable primal**.\n",
    "\n",
    "\n",
    "**Consideramos en lo que continúa la restricción $\\lambda_i \\geq 0 \\forall i=1,\\dots, m$**.\n",
    "\n",
    "\n",
    "Podemos interpretar a la función Lagrangiana como una **subestimación relajada** del problema primal de optimización. Tal problema puede reescribirse en uno **sin restricciones** de la forma:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min f_o(x) + \\displaystyle \\sum_{i=1}^m I_{-}(f_i(x)) + \\sum_{i=1}^p I_o(h_i(x))$$\n",
    "\n",
    "donde:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I_{-}(u) = \\begin{cases}\n",
    "0 & \\text{si } u \\leq 0,\\\\\n",
    "\\infty & \\text{si } u > 0\\\\\n",
    "\\end{cases}\n",
    "\\quad \n",
    "I_o(u) = \\begin{cases}\n",
    "0 & \\text{si } u=0,\\\\\n",
    "\\infty & \\text{si } u \\neq 0\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** para ver la equivalencia entre este problema **sin restricciones** y el primal piénsese en que a la función $f_o$ se le está añadiendo una **regularización** en donde se penaliza fuertemente a los vectores $x$ que **no** cumplen con $f_i(x) \\leq 0$ o $h_i(x) = 0$. Ver [4.3.Minimos_cuadrados_R](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.3.Minimos_cuadrados_R.ipynb) para regularización en un problema de mínimos cuadrados y la relación de la regularización con problemas *bi-criterion* de optimización.\n",
    "\n",
    "Como $\\lambda_i \\geq 0$ se tiene: \n",
    "\n",
    "\n",
    "$$\\lambda_i f_i(x) \\leq I_{-}(f_i(x)) \\quad \\forall i=1,\\dots,m$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nu_ih_i(x) \\leq I_o(h_i(x)) \\quad \\forall i=1,\\dots,p.$$\n",
    "\n",
    "La Lagrangiana $\\mathcal{L}(x, \\nu) = f_o(x) + \\displaystyle \\sum_{i=1}^m \\nu_i h_i(x)$ entonces cumple $\\mathcal{L}(x, \\nu) \\leq f_o(x) + \\displaystyle \\sum_{i=1}^p I_o(h_i(x))$ $\\forall (x, \\nu ) \\in \\text{dom}\\mathcal{L}$. Esto es, $\\mathcal{L}(x,\\nu)$ subestima a $f_o(x) + \\displaystyle \\sum_{i=1}^m \\lambda_i f_i(x) +  \\sum_{i=1}^m \\nu_i h_i(x)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función dual de Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función dual de Lagrange se define como $g: \\mathbb{R}^m \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ con:\n",
    "\n",
    "$$g(\\lambda, \\nu) = \\displaystyle \\inf_{x \\in \\mathcal{D}} \\mathcal{L}(x,\\lambda, \\nu)$$\n",
    "\n",
    "**Comentarios:**\n",
    "\n",
    "* Recuérdese que $\\inf$ es el máximo de las cotas inferiores, ver [1.3.Condicion_de_un_problema_y_estabilidad_de_un_algoritmo](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.3.Condicion_de_un_problema_y_estabilidad_de_un_algoritmo.ipynb) para definición de supremo e ínfimo. En esta definción se está tomando el ínfimo de la Lagrangiana para cada punto $x \\in \\mathcal{D}$ (ínfimo puntual).\n",
    "\n",
    "* Por la definición de la función dual de Lagrange y el punto anterior, se tiene: $g(\\lambda, \\nu)  \\leq \\mathcal{L}(\\tilde{x},\\lambda, \\nu)$ para $\\tilde{x}$ factible.\n",
    "\n",
    "* La función dual de Lagrange para cada par $(\\lambda, \\nu)$ con $\\lambda \\geq 0$ da una **subestimación de $p^*$, el valor óptimo del problema primal**. Esto es: $g(\\lambda, \\nu) \\leq p^*$ para $\\lambda \\geq 0, \\nu \\in \\mathbb{R}^p$ y $p^*$ valor óptimo del problema de optimización primal. \n",
    "\n",
    "* Si la Lagrangiana no es acotada por debajo entonces la función dual de Lagrange se asigna $-\\infty$.\n",
    "\n",
    "* Se prueba que la función dual de Lagrange **siempre es una función cóncava**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema dual de Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociado a **todo** problema de optimización existe su **problema dual** el cual se define como:\n",
    "\n",
    "$$\\displaystyle \\max g(\\lambda,\\nu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a: } \\lambda \\geq 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "\n",
    "* Si $(\\lambda, \\nu)$ satisface $\\lambda \\geq 0$ y $g(\\lambda, \\nu)$ no es $-\\infty$ la pareja $(\\lambda, \\nu)$ se nombra **dual factible**.\n",
    "\n",
    "* El **valor óptimo del problema dual** se denota con $d^*$. **El valor óptimo se alcanza** si existen $(\\lambda^*, \\nu^*)$ tales que $d^* = g(\\lambda^*, \\nu^*)$. \n",
    "\n",
    "* La pareja $(\\lambda^*, \\nu^*)$ se nombra **óptimo dual** o **pareja óptima de multiplicadores de Lagrange** si satisface:\n",
    "\n",
    "    * $\\lambda^* \\geq 0$, \n",
    "    * $g(\\lambda^*, \\nu^*)$ no es $-\\infty$ y \n",
    "    * $(\\lambda^*, \\nu^*)$ resuelve el problema dual. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para distinguir a los óptimos duales de los puntos óptimos $x^*$ del problema primal, a éste últimos se les nombra **óptimos primales**.\n",
    "\n",
    "* El problema dual tiene como interpretación **la mejor cota inferior de la función Lagrangiana como función de $x$ que se obtiene con la función dual de Lagrange**.\n",
    "\n",
    "* El problema dual de Lagrange **siempre es un problema de optimización convexo aún si el problema primal no es convexo.**\n",
    "\n",
    "* A la diferencia $p^* - d^*$ se le nombra ***optimal duality gap*.** Si es positiva se tiene un **problema de optimización en el que se cumple dualidad débil**. Si es igual a cero se tiene un **problema de optimización en el que se cumple dualidad fuerte**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dualidad fuerte y holgura complementaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un problema de optimización en el que los valores óptimos primal y dual se alcanzan y en el que se cumple dualidad fuerte se tiene la propiedad que el **punto óptimo primal minimiza $\\mathcal{L}(x,\\lambda^*, \\nu^*)$** pues:\n",
    "\n",
    "$$f_o(x^*)  = p^* = d^* = g(\\lambda^*, \\nu^*) = \\inf_{x \\in \\mathcal{D}} \\mathcal{L}(x, \\lambda^*, \\nu^*)$$\n",
    "\n",
    "**Comentario:** lo anterior prueba que $\\lambda^*_i f_i(x^*)=0 \\quad  \\forall i=1, \\dots, m$. Tal ecuación se conoce con el nombre de **holgura complementaria**. La holgura complementaria indica que si el $i$-ésimo multiplicador de Lagrange es positivo entonces la $i$-ésima función de restricción de desigualdad $f_i$ es activa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda^*_i > 0 \\implies f_i(x^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(x^*) < 0 \\implies \\lambda^*_i = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condiciones KKT para problemas de optimización (convexos o no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sean $x^*, (\\lambda^*, \\nu^*)$ un par primal-dual de puntos óptimos en los que la *optimal duality gap* es cero o bien: $d^* = p^*$. Entonces $x^*$ minimiza $\\mathcal{L}(x, \\lambda^*, \\nu^*)$ y por factibilidad y holgura complementaria se cumple:\n",
    "\n",
    "$$\\nabla_x \\mathcal{L}(x^*, \\lambda^*, \\nu^*) = \\nabla f_o(x^*) + \\displaystyle \\sum_{i=1}^m \\lambda^*_i f_i(x^*)  + \\sum_{i=1}^p \\nu_i^* \\nabla h_i(x^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(x^*) \\leq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(x^*)=0 \\quad \\forall i = 1,\\dots, p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_i^* \\geq 0 \\quad \\forall i =1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda^*_if_i(x^*) = 0 \\quad \\forall i=1,\\dots,m.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las $5$ condiciones anteriores se nombran **condiciones KKT de optimalidad** para el problema de optimización del inicio. \n",
    "\n",
    "**Comentarios:**\n",
    "\n",
    "* Las condiciones KKT de optimalidad se satisfacen para **cualquier problema de optimización con función objetivo  y funciones de restricción diferenciables evaluadas en un par primal-dual de puntos óptimos en el que se cumple la dualidad fuerte**. \n",
    "\n",
    "* La notación $\\nabla_x$ se refiere a la derivada de $\\mathcal{L}$ respecto a la variable $x$.\n",
    "\n",
    "*  Son condiciones necesarias.\n",
    "\n",
    "* La ecuación $h_i(x^*)=0 \\forall i=1,\\dots,p$  y $f_i(x^*) \\leq 0 \\forall i=1,\\dots,m$ se nombran **ecuaciones de factibilidad primal** y $\\nabla_x \\mathcal{L}(x^*,\\lambda^*, \\nu^*)=0$ se nombra **ecuación de factibilidad dual**.\n",
    "\n",
    "\n",
    "* La dualidad fuerte y que el óptimo del problema dual se alcance puede establecerse a partir de condiciones teóricas sobre el problema de optimización primal. Un ejemplo son las de Slater, ver [Slater's condition](https://en.wikipedia.org/wiki/Slater%27s_condition). Para condiciones más generales aplicables a problemas de optimización primal no necesariamente convexo existen las **condiciones de regularidad**, ver [regurlarity conditions or constraint qualifications](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condiciones KKT para problemas de optimización convexos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el problema **primal** es convexo, esto es, $f_o, f_1, \\dots, f_m$ son funciones convexas y $h_i$ es afín $\\forall i,\\dots, p$ entonces las condiciones KKT son **suficientes** para puntos primal-dual óptimos:\n",
    "\n",
    "Sean $\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}$ puntos que satisfacen las condiciones KKT de optimalidad:\n",
    "\n",
    "$$\\nabla_x \\mathcal{L}(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}) = \\nabla f_o(\\tilde{x})  +  \\displaystyle \\sum_{i=1}^m \\tilde{\\lambda}_i f_i(\\tilde{x}) + \\sum_{i=1}^p \\tilde{\\nu}_i \\nabla h_i(\\tilde{x}) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(\\tilde{x}) \\leq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(\\tilde{x})=0 \\quad \\forall i = 1,\\dots, p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{\\lambda_i} \\geq 0 \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{\\lambda}_if_i(\\tilde{x}) = 0 \\quad \\forall i=1,\\dots,m.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**entonces el par $(\\tilde{x},\\tilde{\\nu})$ son puntos primal-dual óptimos y en el problema de optimización se cumple la dualidad fuerte.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario:** en algunos casos especiales es posible resolver las condiciones KKT de optimalidad de manera analítica y en general muchos algoritmos de optimización resultan o pueden interpretarse como métodos que las resuelven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "* S. P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
