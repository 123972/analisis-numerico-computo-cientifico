{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota basada en [liga1](https://drive.google.com/file/d/1xtkxPCx05Xg4Dj7JZoQ-LusBDrtYUqOF/view?usp=sharing), [liga2](https://drive.google.com/file/d/16-_PvWNaO0Zc9x04-SRsxCRdn5fxebf2/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas de optimización sin restricciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta nota se consideran resolver problemas de la forma:\n",
    "\n",
    "$$\\min f_o(x)$$\n",
    "\n",
    "con $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ convexa y $f \\in \\mathcal{C}^2(\\text{dom}f)$.\n",
    "\n",
    "También se asume que existe un punto óptimo $x^*$ por lo que el problema tiene solución y el valor óptimo se denota por $p^* = f(x^*) = \\inf f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo anterior una **condición necesaria y suficiente** para que $x^*$ sea óptimo es: $\\nabla f(x^*) = 0$ que **en general** es un conjunto de $n$ **ecuaciones no lineales** en $n$ variables y que resuelve el problema de optimización planteado al inicio. \n",
    "\n",
    "\n",
    "**Ejemplos:**\n",
    "\n",
    "1)$$\\displaystyle \\min_{x \\in \\mathbb{R}^2} x_1^4+2x_1^2x_2+x_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces:\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \n",
    "\\left [\n",
    "\\begin{array}{c}\n",
    "4x_1^3+4x_1x_2\\\\\n",
    "2x_1^2+2x_2\n",
    "\\end{array}\n",
    "\\right ]=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que es una ecuación de dos variables y dos incógnitas **no lineal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $$\\displaystyle \\min_{x \\in \\mathbb{R}^2} \\frac{1}{2}x^TPx+q^Tx+r$$\n",
    "\n",
    "con $P=\\left [\\begin{array}{cc}\n",
    "5 & 4\\\\\n",
    "4 & 5\n",
    "\\end{array}\n",
    "\\right ]$, $q=\\left [\\begin{array}{c}\n",
    "-1\\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$, $r=3$. Obsérvese que haciendo las multiplicaciones de matriz-vector y productos punto se reescribe el problema como:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^2} \\frac{5}{2}x_1^2 + \\frac{5}{2}x_2^2+4x_1x_2 -x_1 + x_2+3$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces:\n",
    "\n",
    "$$\\nabla f(x) = Px +q =\\left [ \\begin{array}{cc}\n",
    "5 & 4\\\\\n",
    "4 & 5\n",
    "\\end{array}\n",
    "\\right ]\n",
    "\\left [ \\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\n",
    "\\right ]\n",
    "+ \\left [ \\begin{array}{c}\n",
    "-1\\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right ]=\n",
    "\\left [ \\begin{array}{cc}\n",
    "5x_1+4x_2-1\\\\\n",
    "4x_1+5x_2+1\n",
    "\\end{array}\n",
    "\\right ]\n",
    "=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que es una ecuación en dos variables con dos incógnitas **lineal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario:** en algunos casos especiales es posible resolver la ecuación no lineal $\\nabla f(x) = 0$ para $x$ de forma analítica o cerrada. Este es el caso del ejemplo $2$ anterior la cual está dada por $x^* = -P^{-1}q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P=np.array([[5,4],[4,5]])\n",
    "q=np.array([-1,1])\n",
    "np.linalg.solve(P,-q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pero típicamente se utiliza un algoritmo iterativo: calcular una secuencia de puntos $x^{(0)}, x^{(1)}, \\dots \\in \\text{dom}f$ con $f(x^{(k)}) \\rightarrow p^*$ si $k \\rightarrow \\infty$. El conjunto de puntos $x^{(0)}, x^{(1)},\\dots$ se nombra **secuencia de minimización** para el problema de optimización. El algoritmo termina si $f(x^{(k)})-p^* \\leq \\epsilon$ con $\\epsilon >0$ una tolerancia dada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de búsqueda de línea por *backtracking*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de descenso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Algoritmo de descenso\n",
    ">> Punto inicial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de máximo de descenso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influencia del número de condición "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_index(vec,index,h):\n",
    "    '''\n",
    "    Auxiliary function for gradient and Hessian computation.\n",
    "    Args:\n",
    "        vec (array): numpy array\n",
    "        index (int): index \n",
    "        h (float):   quantity that vec[index] will be increased\n",
    "    Returns:\n",
    "        vec (array): \n",
    "    '''\n",
    "    vec[index] +=h\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_index(vec,index,h=1):\n",
    "    '''\n",
    "    Auxiliary function for gradient and Hessian computation.\n",
    "    Args:\n",
    "        vec (array): numpy array\n",
    "        index (int): index \n",
    "        h (float):   quantity that vec[index] will be decreased\n",
    "    Returns:\n",
    "        vec (array): \n",
    "    '''\n",
    "    vec[index] -=h\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_approximation(f,x,h=1e-8):\n",
    "    '''\n",
    "    Numerical approximation of gradient for function f using forward differences.\n",
    "    Args:\n",
    "        f (lambda expression): definition of function f\n",
    "        x (array): numpy array that holds values where gradient will be computed\n",
    "        h (float): step size for forward differences, tipically h=1e-8\n",
    "    Returns:\n",
    "        gf (array):\n",
    "    '''\n",
    "    n = x.size\n",
    "    gf = np.zeros(n)\n",
    "    f_x = f(x)\n",
    "    for i in np.arange(n):\n",
    "        inc_index(x,i,h)\n",
    "        gf[i] = f(x) - f_x\n",
    "        dec_index(x,i,h)\n",
    "    return gf/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hessian_approximation(f,x,h=1e-6):\n",
    "    '''\n",
    "    Numerical approximation of Hessian for function f using forward differences.\n",
    "    Args:\n",
    "        f (lambda expression): definition of function f\n",
    "        x (array): numpy array that holds values where Hessian will be computed\n",
    "        h (float): step size for forward differences, tipically h=1e-6\n",
    "    Returns:\n",
    "        Hf (array):\n",
    "    '''\n",
    "    n = x.size\n",
    "    Hf = np.zeros((n,n))\n",
    "    f_x = f(x)\n",
    "    for i in np.arange(n):\n",
    "        inc_index(x,i,h)\n",
    "        f_x_inc_in_i = f(x)\n",
    "        for j in np.arange(i,n):\n",
    "            inc_index(x,j,h)\n",
    "            f_x_inc_in_i_j = f(x)\n",
    "            dec_index(x,i,h)\n",
    "            f_x_inc_in_j = f(x)\n",
    "            dif = f_x_inc_in_i_j-f_x_inc_in_i-f_x_inc_in_j+f_x\n",
    "            Hf[i,j] = dif\n",
    "            if j != i:\n",
    "                Hf[j,i] = dif\n",
    "            dec_index(x,j,h)\n",
    "            inc_index(x,i,h)\n",
    "        dec_index(x,i,h)\n",
    "    return Hf/h**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_by_backtracking(f,dir_desc,x,\n",
    "                                der_direct, alpha=.15, beta=.5):\n",
    "    \"\"\"\n",
    "    Line search that sufficiently minimizes f restricted to a ray.\n",
    "    Args:\n",
    "        alpha (float): parameter in line search with backtracking, tipically .15\n",
    "        beta (float): parameter in line search with backtracking, tipically .5\n",
    "        f (lambda expression): definition of function f\n",
    "        dir_desc (array): descent direction\n",
    "        x (array): numpy array that holds values where line search will be performed\n",
    "        der_direct (float): directional derivative of f\n",
    "    Returns:\n",
    "        t (float):\n",
    "    \"\"\"\n",
    "    t=1\n",
    "    if alpha > 1/2:\n",
    "        print('alpha must be less than or equal to 1/2')\n",
    "        t=-1\n",
    "    if beta>1:\n",
    "        print('beta must be less than 1')\n",
    "        t=-1;   \n",
    "    if t!=-1:\n",
    "        eval1 = f(x+t*dir_desc)\n",
    "        eval2 = f(x) + alpha*t*der_direct\n",
    "        while eval1 > eval2:\n",
    "            t=beta*t\n",
    "            eval1=f(x+t*dir_desc)\n",
    "            eval2=f(x)+alpha*t*der_direct\n",
    "    else:\n",
    "        t=-1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso en gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(x_obj,x_approx):\n",
    "    if np.linalg.norm(x_ast) > np.nextafter(0,1):\n",
    "        Err=np.linalg.norm(x_obj-x_approx)/np.linalg.norm(x_obj)\n",
    "    else:\n",
    "        Err=np.linalg.norm(x_obj-x_approx)\n",
    "    return Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_with_sign(x_obj,x_approx):\n",
    "    if np.linalg.norm(x_ast) > np.nextafter(0,1):\n",
    "        Err=(x_obj-x_approx)/np.linalg.norm(x_obj)\n",
    "    else:\n",
    "        Err=x_obj-x_approx\n",
    "    return Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, x_0, tol, \n",
    "                     tol_backtracking, x_ast=None, p_ast=None, maxiter=30):\n",
    "    '''\n",
    "    Gradient descent to numerically approximate solution of min f\n",
    "    Args:\n",
    "        f (lambda expression):\n",
    "        x_0 (array):\n",
    "        tol (float):\n",
    "        tol_backtracking (float):\n",
    "        x_ast (array):\n",
    "        p_ast (float):\n",
    "        maxiter (int):\n",
    "    Returns:\n",
    "        (list): \n",
    "    '''\n",
    "    iter = 0\n",
    "    x = x_0\n",
    "    \n",
    "    feval = f(x)\n",
    "    #gradient_approximation = lambda x: np.array([2*(x[0]-2),\n",
    "    #                    -2*(2-x[1]),\n",
    "    #                    2*x[2],\n",
    "    #                    4*x[3]**3])\n",
    "    gfeval = gradient_approximation(f,x)\n",
    "    #gfeval = gradient_approximation(x)\n",
    "\n",
    "    normgf = np.linalg.norm(gfeval)\n",
    "    \n",
    "    Err_plot_aux = np.zeros(maxiter)\n",
    "    Err_plot_aux[iter]=math.fabs(feval-p_ast)\n",
    "    \n",
    "    Err = compute_error(x_ast,x)\n",
    "    n = x.size\n",
    "    x_plot_aux = np.zeros((n,maxiter))\n",
    "    x_plot_aux[:,0] = x\n",
    "    \n",
    "    print('Iter   Normagf   Error_x_ast   Error_p_ast   backtracking_result')\n",
    "    print('{}    {:0.2e}    {:0.2e}    {:0.2e}    {}'.format(iter,normgf,Err,Err_plot_aux[iter],\"---\"))\n",
    "    \n",
    "    while(normgf>tol and iter < maxiter):\n",
    "        dir_desc = -gfeval\n",
    "        der_direct = gfeval.dot(dir_desc)\n",
    "        t = line_search_by_backtracking(f,dir_desc,x,der_direct)\n",
    "        x = x + t*dir_desc\n",
    "        feval = f(x)\n",
    "        gfeval = gradient_approximation(f,x)\n",
    "        #gfeval = gradient_approximation(x)\n",
    "        normgf = np.linalg.norm(gfeval)\n",
    "        Err = compute_error(x_ast,x)\n",
    "        iter+=1\n",
    "        Err_plot_aux[iter] = math.fabs(feval-p_ast);\n",
    "        x_plot_aux[:,iter-1] = x\n",
    "        print('{}    {:0.2e}    {:0.2e}    {:0.2e}    {:0.2e}'.format(iter,normgf,Err,Err_plot_aux[iter],t))\n",
    "        if t<tol_backtracking: #valor de backtracking es menor a tolerancia2, revisar motivo...\n",
    "            iter_salida=iter\n",
    "            iter = maxiter\n",
    "        \n",
    "    print('{} {:0.2e}'.format(\"Error utilizando valor de x_ast:\",Err))\n",
    "    print('{} {}'.format(\"Valor aproximado a x_ast:\", x))\n",
    "    cond = Err_plot_aux > np.nextafter(0,1)\n",
    "    Err_plot = Err_plot_aux[cond]\n",
    "    aux_diferencia_x_plot_aux = compute_error_with_sign(x_ast,x)\n",
    "    cond = (np.linalg.norm(aux_diferencia_x_plot_aux,axis=0) > np.nextafter(0,1)) & (np.sum(aux_diferencia_x_plot_aux,axis=0)!=0)\n",
    "    #if np.sum(cond)!=0:\n",
    "    #    x_plot[:,1:1+sum(cond)] = x_plot_aux[:,cond]\n",
    "    if iter == maxiter and t < tol_backtracking:\n",
    "        print(\"Backtracking value less than tol_backtracking, check approximation\")\n",
    "        iter=iter_salida\n",
    "    return [x,iter,Err_plot,x_plot_aux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   Normagf   Error_x_ast   Error_p_ast   backtracking_result\n",
      "0    8.72e+00    1.54e+00    1.90e+01    ---\n",
      "1    3.53e-07    6.10e-08    2.98e-14    5.00e-01\n",
      "2    2.14e-15    3.06e-09    7.50e-17    5.00e-01\n",
      "Error utilizando valor de x_ast: 3.06e-09\n",
      "Valor aproximado a x_ast: [ 2.e+00  2.e+00 -5.e-09  0.e+00]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: (x[0]-2)**2 + (2-x[1])**2 + x[2]**2 + x[3]**4\n",
    "x_ast = np.array([2.0,2.0,0.0,0.0])\n",
    "x_0 = np.array([5.0,5.0,1.0,0.0])\n",
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=f(x_ast)\n",
    "[x,iter,Err_plot,x_plot_aux]=gradient_descent(f, x_0, tol, tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   Normagf   Error_x_ast   Error_p_ast   backtracking_result\n",
      "0    2.55e+00    7.07e-01    7.50e-01    ---\n",
      "1    7.29e-01    3.95e-01    1.09e-01    2.50e-01\n",
      "2    3.22e-01    2.83e-01    4.20e-02    2.50e-01\n",
      "3    2.73e-01    1.48e-01    1.54e-02    5.00e-01\n",
      "4    1.21e-01    1.06e-01    5.91e-03    2.50e-01\n",
      "5    1.02e-01    5.56e-02    2.16e-03    5.00e-01\n",
      "6    4.52e-02    3.98e-02    8.30e-04    2.50e-01\n",
      "7    3.84e-02    2.08e-02    3.04e-04    5.00e-01\n",
      "8    1.70e-02    1.49e-02    1.17e-04    2.50e-01\n",
      "9    1.44e-02    7.82e-03    4.28e-05    5.00e-01\n",
      "10    6.36e-03    5.60e-03    1.64e-05    2.50e-01\n",
      "11    5.41e-03    2.93e-03    6.01e-06    5.00e-01\n",
      "12    2.39e-03    2.10e-03    2.31e-06    2.50e-01\n",
      "13    2.03e-03    1.10e-03    8.46e-07    5.00e-01\n",
      "14    8.95e-04    7.87e-04    3.25e-07    2.50e-01\n",
      "15    7.60e-04    4.12e-04    1.19e-07    5.00e-01\n",
      "16    3.36e-04    2.95e-04    4.57e-08    2.50e-01\n",
      "17    2.85e-04    1.55e-04    1.67e-08    5.00e-01\n",
      "18    1.26e-04    1.11e-04    6.42e-09    2.50e-01\n",
      "19    1.07e-04    5.80e-05    2.35e-09    5.00e-01\n",
      "20    4.72e-05    4.15e-05    9.03e-10    2.50e-01\n",
      "21    4.01e-05    2.17e-05    3.31e-10    5.00e-01\n",
      "22    1.77e-05    1.56e-05    1.27e-10    2.50e-01\n",
      "23    1.50e-05    8.15e-06    4.65e-11    5.00e-01\n",
      "24    6.64e-06    5.83e-06    1.78e-11    2.50e-01\n",
      "25    5.64e-06    3.05e-06    6.55e-12    5.00e-01\n",
      "26    2.49e-06    2.18e-06    2.49e-12    2.50e-01\n",
      "27    2.11e-06    1.14e-06    9.24e-13    5.00e-01\n",
      "28    9.33e-07    8.15e-07    3.47e-13    2.50e-01\n",
      "29    7.93e-07    4.27e-07    1.31e-13    5.00e-01\n",
      "30    3.50e-07    3.02e-07    4.74e-14    2.50e-01\n",
      "31    2.97e-07    1.58e-07    1.88e-14    5.00e-01\n",
      "32    1.31e-07    1.10e-07    6.17e-15    2.50e-01\n",
      "33    1.11e-07    5.76e-08    2.82e-15    5.00e-01\n",
      "34    1.46e-07    3.35e-08    1.68e-15    5.00e-01\n",
      "35    4.18e-08    2.05e-08    5.06e-16    2.50e-01\n",
      "36    5.48e-08    8.14e-09    9.93e-17    5.00e-01\n",
      "37    2.22e-08    4.51e-09    1.21e-17    1.25e-01\n",
      "38    2.22e-08    4.51e-09    1.21e-17    3.47e-18\n",
      "Error utilizando valor de x_ast: 4.51e-09\n",
      "Valor aproximado a x_ast: [ 4.40894393e-09 -9.67595355e-10]\n",
      "Backtracking value less than tol_backtracking, check approximation\n"
     ]
    }
   ],
   "source": [
    "gamma_cte=5;\n",
    "f = lambda x: 1/2*(x[0]**2+gamma_cte*x[1]**2)\n",
    "x_ast=np.array([0.0,0.0])\n",
    "x_0 = np.array([.5,.5])\n",
    "tol=1e-8\n",
    "tol_backtracking=1e-14\n",
    "maxiter=50\n",
    "p_ast=f(x_ast)\n",
    "[x,iter,Err_plot,x_plot_aux]=gradient_descent(f, x_0, tol, tol_backtracking, x_ast, p_ast, maxiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "* S. P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2009.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
