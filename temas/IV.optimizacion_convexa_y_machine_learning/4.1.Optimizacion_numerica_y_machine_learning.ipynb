{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/qb3swgkpaps7yba/4.1.Introduccion_optimizacion_convexa.pdf?dl=0), [liga2](https://www.dropbox.com/s/6isby5h1e5f2yzs/4.2.Problemas_de_optimizacion_convexa.pdf?dl=0), [liga3](https://www.dropbox.com/s/ko86cce1olbtsbk/4.3.1.Teoria_de_convexidad_Conjuntos_convexos.pdf?dl=0), [liga4](https://www.dropbox.com/s/mmd1uzvwhdwsyiu/4.3.2.Teoria_de_convexidad_Funciones_convexas.pdf?dl=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización numérica y machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimización de código ¿es optimización numérica?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este módulo hemos invertido buena parte del tiempo del curso en la eficiente implementación en el hardware que poseemos. Revisamos lo que estudia el análisis numérico o cómputo científico, definiciones de sistema de punto flotante, funciones, derivadas, integrales y métodos o algoritmos numéricos para su aproximación. Consideramos *bottlenecks* que pueden surgir en la implementación de los métodos o algoritmos y revisamos posibles opciones para encontrarlos y minimizarlos (**optimización de código**). Lo anterior lo resumimos con el uso de herramientas como: perfilamiento, integración de R y Python con C++ o C, cómputo en paralelo y uso del caché de forma eficiente al usar niveles altos en operaciones de BLAS ([módulo I: cómputo científico y análisis numérico](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico), [módulo II: cómputo en paralelo](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/temas/II.computo_paralelo), [módulo III: cómputo matricial](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/temas/III.computo_matricial)). \n",
    "\n",
    "La **optimización numérica** no es optimización de código sin embargo se apoya enormemente de ella para la implementación de sus métodos o algoritmos en la(s) máquina(s) para resolver problemas que surgen en tal rama de las **matemáticas aplicadas**. A la implementación y simulación en el desarrollo de los métodos o algoritmos del análisis numérico o cómputo científico típicamente se le acompaña de estudios que realizan [benchmarks](https://en.wikipedia.org/wiki/Benchmark_(computing)) y perfilamiento (mediciones de tiempo y memoria, por ejemplo) con el objetivo de tener **software confiable y eficiente** en la práctica. Esto lo encontramos también en la rama de optimización numérica con los métodos o algoritmos que son desarrollados e implementados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Métodos o algoritmos numéricos en *big data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de los métodos o algoritmos en el contexto de **grandes cantidades de datos** o *big data* es **crítica** al ir a la práctica pues de esto depende que nuestra(s) máquina(s) tarde meses, semanas, días u horas para resolver problemas que se presentan en este contexto. La ciencia de datos apunta al desarrollo de técnicas y se apoya de aplicaciones de *machine learning* para la extracción de conocimiento útil y toma como fuente de información las grandes cantidades de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Problemas de optimización numérica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una gran cantidad de aplicaciones plantean problemas de optimización. Tenemos problemas básicos que se presentan en cursos iniciales de cálculo:\n",
    "\n",
    "*Una caja con base y tapa cuadradas debe tener un volumen de $100 cm^3$. Encuentre las dimensiones de la caja que minimicen la cantidad de material.*\n",
    "\n",
    "Y tenemos más especializados que encontramos en áreas como estadística, ingeniería, finanzas o *machine learning*:\n",
    "\n",
    "* Ajustar un modelo de regresión lineal a un conjunto de datos.\n",
    "\n",
    "* Buscar la mejor forma de invertir un capital en un conjunto de activos.\n",
    "\n",
    "* Elección del ancho y largo de un dispositivo en un circuito electrónico.\n",
    "\n",
    "* Ajustar un modelo que clasifique un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general un problema de optimización matemática o numérica tiene la forma:\n",
    "\n",
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} f_i(x) \\leq b_i, i=1,\\dots, m$$\n",
    "\n",
    "donde: $x=(x_1,x_2,\\dots, x_n)^T$ es la **variable de optimización del problema**, la función $f_o: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ es la **función objetivo**, las funciones $f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}, i=1,\\dots,m$ son las **funciones de restricción** (aquí se colocan únicamente desigualdades pero pueden ser sólo igualdades o bien una combinación de ellas) y las constantes $b_1,b_2,\\dots, b_m$ son los **límites o cotas de las restricciones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un vector $x^* \\in \\mathbb{R}^n$ es nombrado **óptimo** o solución del problema anterior si tiene el valor más pequeño de entre todos los vectores $x \\in \\mathbb{R}^n$ que satisfacen las restricciones. Por ejemplo, si $z \\in \\mathbb{R}^n$ satisface $f_1(z) \\leq b_1, f_2(z) \\leq b_2, \\dots, f_m(z) \\leq b_m$ y $x^*$ es óptimo entonces $f_o(z) \\geq f_o(x^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** \n",
    "\n",
    "* En el módulo IV del curso revisaremos métodos o algoritmos de optimización para funciones objetivo $f_o: \\mathbb{R}^n \\rightarrow \\mathbb{R}$. Sin embargo, hay formulaciones que utilizan $f_o: \\mathbb{R}^n \\rightarrow \\mathbb{R}^q$. Tales formulaciones pueden hallarlas en la optimización multicriterio, multiobjetivo, vectorial o también nombrada Pareto, ver [Multi objective optimization](https://en.wikipedia.org/wiki/Multi-objective_optimization).\n",
    "\n",
    "* Obsérvese que el problema de optimización definido utiliza una forma de minimización y no de maximización. Típicamente en la literatura por convención se consideran problemas de este tipo. Además minimizar $f_o$ y maximizar $-f_o$ son **problemas de optimización equivalentes**\\*.\n",
    "\n",
    "\\*A grandes rasgos dos problemas de optimización son equivalentes si con la solución de uno de ellos se obtiene la solución del otro y viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**\n",
    "\n",
    "1) $$\\displaystyle \\min_{x \\in \\mathbb{R}^n} ||x||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:} Ax \\leq b$$\n",
    "\n",
    "\n",
    "con $A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m$. En este problema buscamos el vector $x$ que es solución del problema $Ax \\leq b$ con mínima norma Euclidiana. La función objetivo es $f_o(x)=||x||_2$, las funciones de restricción son las desigualdades lineales $f_i(x) = a_i^Tx \\leq b_i$ con $a_i$ $i$-ésimo renglón de $A$ y $b_i$ $i$-ésima componente de $b$, $\\forall i=1,\\dots,m$.\n",
    "\n",
    "**Comentario:** un problema similar al anterior lo podemos encontrar en resolver el sistema de ecuaciones lineales $Ax=b$ *underdetermined* en el que $m < n$ y se busca el vector $x$ con mínima norma Euclidiana que satisfaga tal sistema. Tal sistema puede tener infinitas soluciones o ninguna solución, ver [3.3.Solucion_de_SEL_y_FM](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/III.computo_matricial/3.3.Solucion_de_SEL_y_FM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Encuentra el punto en la gráfica de $y=x^2$ que es más cercano al punto $P=(1,0)$ bajo la norma Euclidiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deseamos minimizar la cantidad $||(1,0)-(x,y)||_2$. Además $y = y(x)$ por lo que reescribiendo lo anterior se tiene $||(1,0)-(x,x^2)||_2=||(1-x,-x^2)||_2=\\sqrt{(1-x)^2+x^4}$. Entonces el problema de optimización (sin restricciones) es:\n",
    "\n",
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}}\\sqrt{(1-x)^2+x^4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Machine learning, statistical machine learning y optimización numérica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección relacionamos a *machine learning* con la optimización y se describen diferentes enfoques que se han propuesto para aplicaciones de *machine learning* con métodos de optimización. Lo siguiente **no** pretende ser una exposición extensa **ni** completa sobre *machine learning*, ustedes llevan materias que se enfocan esencialmente a definir esta área, sus objetivos y conceptos más importantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la ciencia de datos se utilizan las aplicaciones desarrolladas en *machine learning* por ejemplo:\n",
    "\n",
    "* Clasificación de documentos o textos: detección de *spam*.\n",
    "\n",
    "* [Procesamiento de lenguaje natural](https://en.wikipedia.org/wiki/Natural_language_processing):  [named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition).\n",
    "\n",
    "* [Reconocimiento de voz](https://en.wikipedia.org/wiki/Speech_recognition).\n",
    "\n",
    "* [Visión por computadora](https://en.wikipedia.org/wiki/Computer_vision): reconocimiento de rostros o imágenes.\n",
    "\n",
    "* Detección de fraude.\n",
    "\n",
    "* [Reconocimiento de patrones](https://en.wikipedia.org/wiki/Pattern_recognition).\n",
    "\n",
    "* Diagnóstico médico.\n",
    "\n",
    "* [Sistemas de recomendación](https://en.wikipedia.org/wiki/Recommender_system).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las aplicaciones anteriores involucran problemas como son:\n",
    "\n",
    "* Clasificación.\n",
    "\n",
    "* Regresión.\n",
    "\n",
    "* *Ranking*.\n",
    "\n",
    "* *Clustering*.\n",
    "\n",
    "* Reducción de la dimensionalidad.\n",
    "\n",
    "En cada una de las aplicaciones o problemas anteriores se utilizan **funciones de pérdida** que guían el proceso de aprendizaje. Tal proceso involucra **optimización parámetros** de la función de pérdida. Por ejemplo, si la función de pérdida en un problema de regresión es una pérdida cuadrática $\\mathcal{L}(y,\\hat{y}) = (\\hat{y}-y)^2$ con $\\hat{y} = \\hat{\\beta}_0 + \\beta_1x$, entonces el vector de parámetros a optimizar (aprender) es $\n",
    "\\beta=\n",
    "\\left[ \\begin{array}{c}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning* no sólo se apoya de la optimización pues es un área de Inteligencia Artificial\\* que utiliza técnicas estadísticas para el diseño de sistemas capaces de aplicaciones como las escritas anteriormente, de modo que hoy en día tenemos *statistical machine learning*. No obstante, uno de los **pilares** de *machine learning* o *statistical machine learning* es la optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*La IA o inteligencia artificial es una rama de las ciencias de la computación que atrajo un gran interés en $1950$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning* o *statistical machine learning* se apoya de las formulaciones y algoritmos en optimización. Sin embargo, también ha contribuido a ésta área desarrollando nuevos enfoques en los métodos o algoritmos para el tratamiento de grandes cantidades de datos o *big data* y estableciendo retos significativos no presentes en problemas clásicos de optimización. De hecho, al revisar literatura que intersecta estas dos disciplinas encontramos comunidades científicas que desarrollan o utilizan métodos o algoritmos exactos (ver [Exact algorithm](https://en.wikipedia.org/wiki/Exact_algorithm)) y otras que utilizan métodos de optimización estocástica (ver [Stochastic optimization](https://en.wikipedia.org/wiki/Stochastic_optimization) y [Stochastic approximation](https://en.wikipedia.org/wiki/Stochastic_approximation)) basados en métodos o algoritmos aproximados (ver [Approximation algorithm](https://en.wikipedia.org/wiki/Approximation_algorithm)). Hoy en día es común encontrar estudios que hacen referencia a **modelos o métodos de aprendizaje**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplo de lo anterior considérese la técnica de **regularización** que en *machine learning* se utiliza para encontrar soluciones que generalicen y provean una explicación no compleja del fenómeno en estudio. La regularización sigue el principio de la navaja de Occam, ver [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor): para cualquier conjunto de observaciones en general se prefieren explicaciones simples a explicaciones más complicadas. Aunque la técnica de regularización es conocida en optimización, han sido varias las aplicaciones de *machine learning* las que la han posicionado como clave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Large scale machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El inicio del siglo XXI estuvo marcado, entre otros temas, por un incremento significativo en la generación de información. Esto puede contrastarse con el desarrollo de los procesadores de las máquinas, que como se revisó en el  módulo II del curso en el tema [2.1.Un_poco_de_historia_y_generalidades](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.1.Un_poco_de_historia_y_generalidades.ipynb), tuvo un menor *performance* al del siglo XX. Asimismo, las mejoras en dispositivos de almacenamiento o *storage* y sistemas de networking abarató costos de almacenamiento y permitió tal incremento de información.  En este contexto, los modelos y métodos de *statistical machine learning* se vieron limitados por el tiempo de cómputo y no por el tamaño de muestra. La conclusión de esto fue una inclinación en la comunidad científica por el diseño o uso de métodos o modelos para procesar grandes cantidades de datos usando recursos computacionales comparativamente menores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de lo anterior se observa en métodos de optimización desarrollados en la década de los $50$'s. Mientras que métodos tradicionales en optimización basados en el cálculo del gradiente y la Hessiana de una función son efectivos para problemas de aprendizaje *small-scale* (en los que  utilizamos un enfoque en ***batch*** o por lote), en el contexto del aprendizaje *large-scale*, el **método de gradiente estocástico**\\* se posicionó en el centro de discusiones a inicios del siglo XXI.\n",
    "\n",
    "\n",
    "\\* El método de gradiente estocástico fue propuesto por Robbins y Monro en 1951, es un **algoritmo estocástico**. Ver [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Información de primer y segundo orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tradicionalmente en optimización, la búsqueda del (o los) **óptimo(s)** involucran el cálculo de información de primer o segundo orden (ver [1.4.Polinomios_de_Taylor_y_diferenciacion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.4.Polinomios_de_Taylor_y_diferenciacion_numerica.ipynb)) de la función $f_o$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**\n",
    "\n",
    "1) Calcular $\\nabla f(x), \\nabla^2f(x)$ con $f: \\mathbb{R}^4 \\rightarrow \\mathbb{R}$, dada por $f(x) = (x_1-2)^2+(2-x_2)^2+x_3^2+x_4^4$ en el punto $x_0=(1.5,1.5,1.5,1.5)^T$. \n",
    "\n",
    "**Solución:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x) = \n",
    "\\left[ \\begin{array}{c}\n",
    "2(x_1-2)\\\\\n",
    "-2(2-x_2)\\\\\n",
    "2x_3\\\\\n",
    "4x_4^3\n",
    "\\end{array}\n",
    "\\right] ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla^2f(x)=\n",
    " \\left[\\begin{array}{cccc}\n",
    "2 & 0 & 0 & 0\\\\\n",
    "0 & 2 & 0 & 0\\\\\n",
    "0 & 0 &2 & 0\\\\\n",
    "0 & 0 &0 &12x_3^2\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x_0) = \n",
    "\\left[ \\begin{array}{c}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "3\\\\\n",
    "\\frac{27}{2}\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla^2f(x_0)=\n",
    " \\left[\\begin{array}{cccc}\n",
    "2 &0&0&0\\\\\n",
    "0&2&0&0\\\\\n",
    "0 &0&2&0\\\\\n",
    "0&0&0&27\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información de primer y segundo orden la constituyen el gradiente de $f$, $\\nabla f(x)$, y la matriz Hessiana de $f$, $\\nabla^2f(x)$. Obsérvese que el almacenamiento de la Hessiana involucra $\\mathcal{O}(n^2)$ entradas. En los métodos clásicos de optimización se utilizan el gradiente y la Hessiana para encontrar el mínimo de funciones. La Hessiana se utiliza para resolver un sistema de ecuaciones lineales asociado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Encontrar el mínimo de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([1.5,1.5,1.5,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf= lambda x: np.array([2*(x[0]-2),\n",
    "                        -2*(2-x[1]),\n",
    "                        2*x[2],\n",
    "                        4*x[3]**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hf = lambda x: np.array([[2, 0, 0 ,0],\n",
    "                         [0, 2, 0, 0],\n",
    "                         [0, 0, 2, 0],\n",
    "                         [0, 0, 0, 27]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1. , -1. ,  3. , 13.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  0,  0],\n",
       "       [ 0,  2,  0,  0],\n",
       "       [ 0,  0,  2,  0],\n",
       "       [ 0,  0,  0, 27]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hf(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $f$ es una función convexa (definida más adelante) se tiene que su óptimo se obtiene igualando y resolviendo la **ecuación no lineal** $\\nabla f(x) = 0$ :\n",
    "\n",
    "$$\\nabla f(x) = \n",
    "\\left[ \\begin{array}{c}\n",
    "2(x_1-2) \\\\\n",
    "-2(2-x_2)\\\\\n",
    "2x_3\\\\\n",
    "4x_4^3\n",
    "\\end{array}\n",
    "\\right]\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El óptimo $x^* \\in \\mathbb{R}^4$ está dado por:\n",
    "\n",
    "$$x^*=\n",
    "\\left[ \\begin{array}{c}\n",
    "2\\\\\n",
    "2\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo encontramos numéricamente el óptimo?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forma1: con el gradiente de $f$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numéricamente se puede utilizar un método iterativo en el que iniciamos con un punto inicial $x^{(0)}$ y las actualizaciones las realizamos con el gradiente:\n",
    "\n",
    "$$x^{(k)} = x^{(k-1)} - \\nabla f(x^{(k-1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para $k=1,2,\\dots,$.\n",
    "\n",
    "**Obs:** A la iteración anterior se le añade la **búsqueda de línea por *backtracking***, ver [Backtracking line search](https://en.wikipedia.org/wiki/Backtracking_line_search) para determinar el máximo paso a realizar en la **dirección de descenso** (que en este caso es el **gradiente**).\n",
    "\n",
    "En el ejemplo tomando $x^{(0)} = (5,5,1,0)^T$ se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([5,5,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0 - gf(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = x_1 - gf(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = x_2 - gf(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_4 = x_3 - gf(x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y aquí nos quedaremos ciclando hasta el infinito..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forma2: con el gradiente y la Hessiana de $f$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es utilizar la información de segundo orden con la Hessiana y considerar una actualización:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(k)} = x^{(k-1)} - \\nabla^2 f \\left (x^{(k-1)} \\right )^{-1} \\nabla f\\left(x^{(k-1)} \\right)$$\n",
    "\n",
    "para $k=1,2,\\dots,$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** A la iteración anterior se le añade la **búsqueda de línea por *backtracking***, ver [Backtracking line search](https://en.wikipedia.org/wiki/Backtracking_line_search) para determinar el máximo paso a realizar en la **dirección de descenso** (que en este caso es la **dirección de Newton**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo tomando $x^{(0)} = (5,5,1,0)^T$ se tiene (recordamos que no calculmos inversas de matrices por mayor costo computacional que el que tiene resolver un sistema de ecuaciones lineales):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([5,5,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0 - np.linalg.solve(Hf(x_0),gf(x_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** de acuerdo al ejemplo anterior:\n",
    "\n",
    "* Utilizar información de primer o segundo orden nos ayuda a encontrar óptimo(s) de funciones.\n",
    "\n",
    "* Encontrar al óptimo involucró un método iterativo.\n",
    "\n",
    "* En términos coloquiales y de forma simplificada, una **dirección de descenso** es aquella que al moverse de un punto a otro en tal dirección, el valor de $f_o$ decrece:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/25bmebx645howjw/direccion_de_descenso_de_Newton_1d.png?dl=0\" heigth=\"600\" width=\"600\">\n",
    "\n",
    "\n",
    "En el dibujo anterior $\\hat{f}$ es un modelo cuadrático, $\\Delta x_{nt}$ es dirección de descenso de Newton y $x^*$ es el óptimo de $f$. Del punto $(x,f(x))$ nos debemos mover al punto $(x+\\Delta x_{nt}, f(x + \\Delta x_{nt}))$ para llegar al óptimo y el valor de $f$ decrece: $f(x+\\Delta x_{nt}) < f(x)$.\n",
    "\n",
    "* Con la información de primer orden no alcanzamos al óptimo (de hecho se cicla el método iterativo propuesto) pero con la de segundo orden sí lo alcanzamos en una iteración y tuvimos que resolver un sistema de ecuaciones lineales.\n",
    "\n",
    "* Si consideramos una reescritura en la actualización de **descenso en gradiente**:\n",
    "\n",
    "$$x^{(k)} = x^{(k-1)} - \\nabla f(x^{(k-1)})$$\n",
    "\n",
    "de la forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(k)} = x^{(k-1)} - t_{k-1}\\nabla f(x^{(k-1)})$$\n",
    "\n",
    "para $t_{k-1} > 0 $. Con $t_0=0.5$ llegamos al óptimo en una iteración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario: más adelante veremos cómo obtener las $t_{k-1}$'s de la búsqueda de línea por backtracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 2, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0 - t_0*gf(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El gradiente involucra menos almacenamiento en memoria que el almacenamiento de la Hessiana: $\\mathcal{O}(n)$ vs $\\mathcal{O}(n^2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La actualización considerando la **dirección de descenso de Newton** (que involucra a la Hessiana) y la búsqueda de línea por *backtracking* es:\n",
    "\n",
    "$$x^{(k)} = x^{(k-1)} - t_{k-1}\\nabla^2 f \\left (x^{(k-1)} \\right )^{-1} \\nabla f\\left(x^{(k-1)} \\right)$$\n",
    "\n",
    "para $k=1,2,\\dots$ y $t_{k-1} >0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch algoritmhs and stochastic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo: regresión lineal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comentario: para este ejemplo la variable de optimización no será $x$, será $\\beta$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supóngase que se han realizado mediciones de un fenómeno de interés en diferentes puntos $x_i$'s resultando en cantidades $y_i$'s $\\forall i=0,1,\\dots, m$ (se tienen $m+1$ puntos) y además las $y_i$'s contienen un ruido aleatorio causado por errores de medición:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/iydpi0m8ndqzb0s/mcuadrados_1.jpg?dl=0\" heigth=\"350\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de los mínimos cuadrados lineales es construir una curva, $f(x|\\beta)$ que \"mejor\" se ajuste a los datos $(x_i,y_i)$, $\\forall i=0,1,\\dots,m$. El término de \"mejor\" se refiere a que la suma: $$\\displaystyle \\sum_{i=0}^m (y_i -f(x_i|\\beta))^2$$ sea lo más pequeña posible, esto es, a que la suma de las distancias verticales entre $y_i$ y $f(x_i|\\beta)$ $\\forall i=0,1,\\dots,m$ al cuadrado sea mínima:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/0dhzv336jj6ep4z/mcuadrados_2.jpg?dl=0\" heigth=\"350\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La notación $f(x|\\beta)$ se utiliza para denotar que $\\beta$ es un vector de parámetros a estimar, en específico $\\beta_0, \\beta_1, \\dots \\beta_n$, esto es: $n+1$ parámetros a estimar.\n",
    "\n",
    "* La variable de optimización es $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tomando $m=3$ y $A \\in \\mathbb{R}^{3 \\times 2}$ geométricamente el problema de **mínimos cuadrados lineales** se puede visualizar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/a6pjx0pdqa3cp60/mc_beta.png?dl=0\" heigth=\"400\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde: $r(\\beta) = b-A\\beta$. Por el dibujo se tiene que cumplir que $A^Tr(\\beta)=0$, esto es: las columnas de $A$ son ortogonales a $r(\\beta)$. La ecuación anterior conduce a las **ecuaciones normales**: \n",
    "\n",
    "$$0=A^Tr(\\beta)=A^T(y-A\\beta)=A^Ty-A^TA\\beta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finalmente, considerando la variable de optimización $\\beta$ y al vector $y$ tenemos: $A^TA \\beta = A^Ty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo en mínimos cuadrados lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los mínimos cuadrados lineales se supone:  $f(x|\\beta) = \\displaystyle \\sum_{j=0}^n\\beta_j\\phi_j(x)$ con $\\phi_j: \\mathbb{R} \\rightarrow \\mathbb{R}$ funciones conocidas por lo que se tiene una gran flexibilidad para el proceso de ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** \n",
    "\n",
    "* Si $n=m$ entonces se tiene un problema de **interpolación**.\n",
    "* x se nombra variable **regresora**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo ajustar el modelo anterior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo siguiente se **asume** $n+1 \\leq m+1$ (tenemos más puntos $(x_i,y_i)$'s que parámetros a estimar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forma 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el ajuste de mínimos cuadrados lineales se utilizan las ecuaciones normales: $$A^TA\\beta=A^Ty$$ donde: $A$ se construye con las $\\phi_j$'s evaluadas en los puntos $x_i$'s, el vector $\\beta$ contiene a los parámetros $\\beta_j$'s a estimar y el vector $y$, la variable **respuesta**, se construye con los puntos $y_i$'s:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\left[\\begin{array}{cccc}\n",
    "\\phi_0(x_0) &\\phi_1(x_0)&\\dots&\\phi_n(x_0)\\\\\n",
    "\\phi_0(x_1) &\\phi_1(x_1)&\\dots&\\phi_n(x_1)\\\\\n",
    "\\vdots &\\vdots& \\vdots&\\vdots\\\\\n",
    "\\phi_0(x_n) &\\phi_1(x_n)&\\dots&\\phi_n(x_n)\\\\\n",
    "\\vdots &\\vdots& \\vdots&\\vdots\\\\\n",
    "\\phi_0(x_{m-1}) &\\phi_1(x_{m-1})&\\dots&\\phi_n(x_{m-1})\\\\\n",
    "\\phi_0(x_m) &\\phi_1(x_m)&\\dots&\\phi_n(x_m)\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{(m+1)x(n+1)},\n",
    "\\beta=\n",
    "\\left[\\begin{array}{c}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots \\\\\n",
    "\\beta_n\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{n+1},\n",
    "y=\n",
    "\\left[\\begin{array}{c}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{m+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y si $A$ es de $rank$ completo (tiene $n+1$ columnas linealmente independientes) se calcula la factorización $QR$ de $A$ : $A = QR$ y entonces: $$A^TA\\beta = A^Ty$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y como $A=QR$ se tiene: $A^TA = (R^TQ^T)(QR)$ y $A^T = R^TQ^T$ por lo que:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(R^TQ^T)(QR) \\beta =  R^TQ^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y usando que $Q$ tiene columnas ortonormales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R^TR\\beta = R^TQ^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $A$ tiene $n+1$ columnas linealmente independientes, la matriz $R$ es invertible por lo que $R^T$ también lo es y finalmente se tiene el **sistema de ecuaciones lineales** a resolver:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R\\beta = Q^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caso regresión lineal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la **regresión lineal** se ajusta un modelo de la forma: $f(x|\\beta) = \\beta_0 + \\beta_1 x$ a los datos $(x_i,y_i)$'s $\\forall i=0,1,\\dots,m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** En este caso se eligen $\\phi_0(x) = 1$, $\\phi_1(x) =x$. Y tenemos que estimar dos parámetros: $\\beta_0, \\beta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pprint\n",
    "from scipy.linalg import solve_triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1989) #para reproducibilidad\n",
    "mpoints = 20\n",
    "x = np.random.randn(mpoints) \n",
    "y = -3*x + np.random.normal(2,1,mpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Los datos ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y, 'r*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Puntos ejemplo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El ajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con numpy podemos usar la función `polyfit` en el paquete de `numpy` para realizar el ajuste: (ver [numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el tercer argumento de polyfit especifica el grado del polinomio a ajustar. \n",
    "#Usaremos ngrado = 1 pues queremos ajustar una recta\n",
    "ngrado = 1\n",
    "coeficientes = np.polyfit(x,y,ngrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-2.65438794,  2.02950714])\n"
     ]
    }
   ],
   "source": [
    "#Una vez realizado el llamado a la función polyfit se regresan los coeficientes de x\n",
    "#ordenados del mayor grado al menor.\n",
    "pprint.pprint(coeficientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces nuestro polinomio es: $$p_{1}(x) = -2.65x + 2.03$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y así tenemos nuestras beta's ajustadas $\\hat{\\beta_0} = 2.03$, $\\hat{\\beta_1} = -2.65$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nos gustaría graficar el modelo en el intervalo $[min(x),max(x)]$ con $min(x)$ la entrada con valor mínimo del numpy array $x$ y $max(x)$ su entrada con valor máximo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lo anterior debemos obtener los valores ajustados al evaluar $p_1(x)$ los valores de $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ajustadas_numpy = coeficientes[1] + coeficientes[0] * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_ajustadas_numpy, 'k-',x, y, 'r*')\n",
    "plt.legend(['modelo lineal','datos'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### También podemos obtener las y's ajustadas con la factorización QR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construimos a la matriz A:\n",
    "A=np.ones((mpoints,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[:,1] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.26371806],\n",
       "       [ 1.        ,  0.08532019],\n",
       "       [ 1.        ,  0.430007  ],\n",
       "       [ 1.        ,  0.89506857],\n",
       "       [ 1.        ,  0.55695552],\n",
       "       [ 1.        ,  0.44112284],\n",
       "       [ 1.        ,  0.37538845],\n",
       "       [ 1.        , -0.1499474 ],\n",
       "       [ 1.        ,  0.77682473],\n",
       "       [ 1.        , -0.02254181],\n",
       "       [ 1.        ,  1.60557773],\n",
       "       [ 1.        , -0.36735574],\n",
       "       [ 1.        ,  0.35554203],\n",
       "       [ 1.        ,  0.16982717],\n",
       "       [ 1.        ,  2.51802379],\n",
       "       [ 1.        ,  0.13922515],\n",
       "       [ 1.        ,  1.15944417],\n",
       "       [ 1.        ,  0.58720721],\n",
       "       [ 1.        , -1.20052527],\n",
       "       [ 1.        , -0.3720616 ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 2.02950714, -2.65438794])\n"
     ]
    }
   ],
   "source": [
    "#Resolvemos el sistema R*beta = Q^T*y\n",
    "beta = solve_triangular(R,Q.T@y)\n",
    "pprint.pprint(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ajustadas_QR = A@beta\n",
    "#obsérvese que la línea anterior es equivalente a realizar:\n",
    "#y_ajustadas_QR = beta[0] + beta[1]*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_ajustadas_QR , 'k-',x, y, 'r*')\n",
    "plt.legend(['modelo lineal','datos'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forma 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos optimización numérica reescribiendo la función objetivo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_o(\\beta)=\\displaystyle \\sum_{i=1}^{20} (y_i -f(x_i|\\beta))^2 = \\displaystyle \\sum_{i=1}^{20} (y_i - (\\beta_0 + \\beta_1 x_i))^2 = \\displaystyle \\sum_{i=1}^{20} (y_i - A[i,:]^T\\beta)^2 = ||y - A \\beta||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con $y \\in \\mathbb{R}^{20}, A \\in \\mathbb{R}^{20 \\times 2}, \\beta \\in \\mathbb{R}^{2 \\times 1}$ y $A[i,:]$ $i$-ésimo renglón de $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planteamos el problema de optimización numérica:\n",
    "\n",
    "$$\\displaystyle \\min_{\\beta \\in \\mathbb{R}^n} ||y - A\\beta||_2^2$$\n",
    "\n",
    "**¿Solución?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribimos $f_o$ como $f_o(\\beta) = ||y-A\\beta||_2^2= (y-A\\beta)^T(y-A\\beta) = y^Ty-2\\beta^TA^Ty + \\beta^TA^TA\\beta$, observa que esta última expresión es un número $\\mathbb{R}$. \n",
    "\n",
    "Por lo anterior: $\\nabla f_o(\\beta) = -2A^Ty + 2A^TA\\beta$. $\\nabla f_o$ es un vector en $\\mathbb{R}^{2 \\times 1}$, de hecho sus entradas son $\\nabla f_o(\\beta) = \n",
    "\\left [\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial f_o(\\beta)}{\\beta_0}\\\\\n",
    "\\frac{\\partial f_o(\\beta)}{\\beta_1}\n",
    "\\end{array}\n",
    "\\right ]\n",
    "$\n",
    ". \n",
    "\n",
    "Si se plantea la **ecuación no lineal**: $\\nabla f_o(\\beta)= 0$ obtenemos una forma **cerrada** de la solución que está dada por $A^TA \\beta=A^Ty$ (y son las ecuaciones normales!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Método numérico?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método por batch o lote vía descenso en gradiente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución como la encontramos en el ejemplo del inicio, es considerar un método iterativo en el que tomamos un punto inicial $\\beta^{(0)}$ y las actualizaciones se realizan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta^{(k)} = \\beta^{(k-1)} - t_{k-1}\\nabla f\\left(\\beta^{(k-1)}\\right), k=1,2,\\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y $t_{k-1}$ obtenida por búsqueda de línea con *backtracking* y es una cantidad positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** como el óptimo de $-2A^Ty + 2A^TA\\beta$ es el mismo que el de $-A^Ty + A^TA\\beta$, utilizamos esta última expresión que corresponde a una **función objetivo** $f_o = \\frac{1}{2}y^Ty-\\beta^TA^Ty + \\frac{1}{2}\\beta^TA^TA\\beta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculamos primero $-A^Ty$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cte=-np.transpose(A)@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.09990114,  23.70269232])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el gradiente de $f_o$: $\\nabla f_o(\\beta) = -A^Ty + A^TA\\beta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = lambda beta_fun: cte + np.transpose(A)@(A@beta_fun)\n",
    "    #observa que no hacemos la multiplicación (A^T*A)*beta, mejor hacemos\n",
    "    #primero A*beta y luego multiplicacmos por A^T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos con las iteraciones dadas por la fórmula de actualización tomando $\\beta_0=(0,0)^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario: más adelante veremos cómo obtener las $t_{k-1}$'s de la búsqueda de línea por backtracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0=.130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = beta_0 - t_0*gf(beta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.61298715, -3.08135   ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1=.0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_2 = beta_1 - t_1*gf(beta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.08962991, -2.9670693 ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_2 = .0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_3 = beta_2 - t_2*gf(beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.16533318, -2.70622534])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_3 = .0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_4 = beta_3 - t_3*gf(beta_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.02056018, -2.7237037 ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_4 = .0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_5 = beta_4 - t_4*gf(beta_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.06518606, -2.65513248])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ajustadas_gradiente = A@beta_5\n",
    "#obsérvese que la línea anterior es equivalente a realizar:\n",
    "#y_ajustadas_gradiente = beta_5[0] + beta_5[1]*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_ajustadas_gradiente , 'k-',x, y, 'r*')\n",
    "plt.legend(['modelo lineal','datos'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método por batch o lote vía descenso por dirección de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es utilizar la información de segundo orden con la Hessiana y considerar una actualización:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta^{(k)} = \\beta^{(k-1)} - \\nabla^2 f \\left (\\beta^{(k-1)} \\right )^{-1} \\nabla f\\left(\\beta^{(k-1)} \\right)$$\n",
    "\n",
    "para $k=1,2,\\dots,$ (omitiendo la búsqueda de línea por *backtracking*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = beta_0 - np.linalg.solve(np.transpose(A)@A,gf(beta_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.02950714, -2.65438794])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ajustadas_Newton = A@beta_1\n",
    "#obsérvese que la línea anterior es equivalente a realizar:\n",
    "#y_ajustadas_Newton = beta_1[0] + beta_1[1]*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXgUZbr+8e+TBEgCBDGgRCOgKPsmhsVBBzcEGZdx+SkCgzgDiKijxxkdjqg4OI6Xy4jHI6iIC0dROSjD4ALiclxQUYEBRaKouIAGhLBvEsjz+6NDm4SEJKS7qzu5P9fVV3e9qVTdVvBJ5a233jJ3R0REEldS0AFERKR6VMhFRBKcCrmISIJTIRcRSXAq5CIiCS4liJ02adLEW7ZsGcSuRUQS1qJFi9a7e9PS7YEU8pYtW7Jw4cIgdi0ikrDM7Luy2tW1IiKS4FTIRUQSnAq5iEiCC6SPXESCV1BQwOrVq9m1a1fQUaSU1NRUsrOzqVOnTqXWVyEXqaVWr15Nw4YNadmyJWYWdBwp4u7k5+ezevVqjj766Ep9j7pWRGqpXbt2kZmZqSIeZ8yMzMzMKv2lpEIuUoupiMenqv5cEqqQvzN9OsuaNGH3998HHUVEJG4kVCFfc/XVtMvPZ0qLFjzxxBNBxxGRONKyZUvWr19f7XXKW/9Xv/pVtfKV5bbbbuPee++t9nYSo5CnpYEZF69fTzIwGrj8979npxnr1q0LOp2I1ALvv/9+0BHKlRiFfOVKGDQI0tMBKExN5WngaOCwww7j+uuvDzSeiFTdt99+S9u2bRk2bBitW7dm8ODBvP766/Tu3ZvjjjuOjz76CIANGzbw29/+ls6dO9OrVy8++eQTAPLz8znzzDPp0KEDw4cPp/jTzp5++ml69OhB165dueKKK9i7d+9++7/vvvvo2LEjHTt25P77768wb4MGDQB46623OOWUU7joooto27YtgwcPDu970aJF9OnThxNOOIF+/fqRl5cHwKOPPkr37t3p0qULF154ITt27KjewSslMYYfZmVBRgbs2gWpqSTt3s2QK6/km6wsbr31ViZMmMCECRP49NNP6dixY9BpRRLOddddx5IlSyK6za5du1ZYIL/66itmzJjB448/Tvfu3XnmmWeYP38+s2fP5u9//zuzZs1i3LhxHH/88cyaNYs333yToUOHsmTJEv76179y0kknceutt/Lyyy/z2GOPAZCbm8v06dN57733qFOnDqNHj2batGkMHTo0vN9FixbxxBNP8OGHH+Lu9OzZkz59+nD88cdX6r/t3//+N5999hlHHHEEvXv35r333qNnz55cc801/Otf/6Jp06ZMnz6dsWPH8vjjj3PBBRcwYsQIAG6++WYee+wxrrnmmoM8svtLjEIOsHYtjBoFI0fC5MmQl8ctkybxxz/+kUMOOQSATp06cdJJJ/H222+TlJQYf2yI1GZHH300nTp1AqBDhw6cfvrpmBmdOnXi22+/BWD+/Pm88MILAJx22mnk5+ezZcsW3nnnHWbOnAnAb37zGxo3bgzAG2+8waJFi+jevTsAO3fu5LDDDiux3/nz53P++edTv359AC644ALefffdShfyHj16kJ2dDYR+YX377bcccsghLFu2jL59+wKwd+9esrKyAFi2bBk333wzmzZtYtu2bfTr1++gjld5EqeQF/3AAJg4MfyxUaNGuDsvvPACF110EfPnzyc5OZkXX3yRs88+O4CgIomnMl0L0VCvXr3w56SkpPByUlISe/bsOahtujuXXXYZd955Z0QylqV47uTkZPbs2YO706FDBz744IP91h82bBizZs2iS5cuPPnkk7z11lsRzVNjTlsvvPBC9uzZQ05ODgDnnHMOqampbNu2LeBkIlIdJ598MtOmTQNC/dNNmjQhIyODX//61zzzzDMAzJkzh40bNwJw+umn8/zzz/PTTz8BoT727777br9tzpo1ix07drB9+3b++c9/cvLJJ1crZ5s2bVi3bl24kBcUFPDZZ58BsHXrVrKysigoKAj/t0RSjSnkEPrN+PHHH7N48WIAfv75Zxo2bMg999wTcDIROVi33XYbixYtonPnzowZM4apU6cCMG7cON555x06dOjAzJkzad68OQDt27fnb3/7G2eeeSadO3emb9++4YuO+3Tr1o1hw4bRo0cPevbsyfDhwyvdrVKeunXr8vzzz/OXv/yFLl260LVr1/BIl9tvv52ePXvSu3dv2rZtW639lMWKX+mNlZycHI/FgyWuvPJKHn744fDyt99+S4sWLaK+X5FEkJubS7t27YKOIeUo6+djZovcPaf0ujXqjLy0hx56qMRv4pYtWzJo0CCC+OUlIhItNbqQAzRr1gx356GHHgLg2WefJSkpKa4H94uIVEWNL+T7jBo1ip07d3L44YcD0Lt3b1q1asXu3bsDTiYiUj21ppBDaLL2NWvW8MYbbwCwcuVK6tWrF754IiKSiGpVId/ntNNOo7CwkHPOOQcIjfE0sypNpiMiEi9qZSGH0Hy/s2fP5ssvvwy3NW3alBtuuCHAVCIiVVdrC/k+xx57LO7ObbfdBsC9996LmYUH8otI7FQ0reusWbNYvnx5DBMlhlpfyPcZN24cmzZtCi937NiRU045hcLCwgBTicSZvDzo0wfWrAlk9yrkZVMhL2bfvC0zZswA4O233yY5OZlXXnkl4GQiceL222H+fBg/PmKbvOOOO2jdujUnnXQSX3zxBVD2tK/vv/8+s2fP5oYbbqBr1658/fXXLFmyhF69etG5c2fOP//88G36DzzwAO3bt6dz584MHDgwYlnjlrvH/HXCCSd4vCsoKPDjjz/eAQc8LS3Nt23bFnQskYhZvnx55VdOTXWH/V+pqdXKsHDhQu/YsaNv377dN2/e7K1atfJ77rnH169fH15n7Nix/sADD7i7+2WXXeYzZswIf61Tp07+1ltvubv7Lbfc4tdee627u2dlZfmuXbvc3X3jxo3VyhiUsn4+wEIvo6bqjLwcKSkpLF68mEWLFgGhqTAbNGjAP/7xj4CTiQSg1MNdSE+HwYPhm2+qtdl3332X888/n/T0dDIyMjj33HOB0LSvJ598Mp06dWLatGllXrPavHkzmzZtok+fPgBcdtllvPPOOwB07tyZwYMH8/TTT5OSkjiTvB4sFfIKdOvWDXdn5MiRAPz5z3/GzPheD4CW2qTUw13YtSu03KxZVHY3bNgwHnzwQT799FPGjRvHrl27qvT9L7/8MldddRWLFy+me/fuBz0lbqKIWCE3s2Qz+7eZvRSpbcaTRx55hB9//DG83KJFC4YMGaJ5W6T22PdwlwULQu8RuOD561//mlmzZrFz5062bt3Kiy++CJQ/7WvDhg3ZunUrELqm1bhxY959910AnnrqKfr06UNhYSGrVq3i1FNP5a677mLz5s01fjrriM1+aGbXAzlAhrsf8IkOsZr9MFomTZrEVVddFV7+4IMP6NWrV4CJRKouXmY/vOOOO5g6dSqHHXYYzZs3p1u3btSvX5+7776bpk2b0rNnT7Zu3cqTTz7Je++9x4gRI6hXrx7PP/88W7duZdSoUezYsYNjjjmGJ554ggYNGnDqqaeyefNm3J0hQ4YwZsyYoP8zq6wqsx9GpJCbWTYwFbgDuL6mF3II9Zm3aNGCdevWAdC6dWuWLVtGnTp1Ak4mUjnxUsilbEFMY3s/cCNQ7qBrMxtpZgvNbOG+4pfI0tLS+Omnn3jttdcAWLFiBXXr1uXpp58OOJmI1DbVLuRmdjbwk7svOtB67j7Z3XPcPadp06bV3W3cOOOMMygsLGTAgAEA/O53v8PMyM/PDziZiNQWkTgj7w2ca2bfAs8Bp5lZrTotNTNefvllVqxYEW5r0qQJf/nLXwJMJVIxXayPT1X9uVS7kLv7f7p7tru3BAYCb7r7kOpuNxEdd9xxuDu33norAHfffTdmRm5ubsDJRPaXmppKfn6+inmccXfy8/NJTU2t9PdE9JmdZnYK8OfacLGzIps2baJx48bh5VNPPZXXX3+dpKQ4HLqflwcDB8L06VEbFyzxp6CggNWrV1d5jLZEX2pqKtnZ2fsNnojqqJWqqg2FfJ/p06eXmOthzpw59O/fP8BEZRg9Gh55BK64AiZNCjqNiJRDhTxAe/bs4YQTTuCTTz4BQjc1rFmzhvR9tzsHJS0tdIdeaampsHNn7POIyAFFe/ihHEBKSgpLly7l448/BkJ3rdWvX58JEyYEGyxK82eISGypkMdQTk4O7s7w4cMBuP766zEzVq9eHUygGM+fISLRoUIegEcffZQffvghvHzUUUcxdOjQYMJEYf4MEYkt9ZEH7MEHH+Saa64JLy9YsICePXsGmEhE4pX6yOPU1VdfzY4dO8jMzASgV69etG/fnoKCgoq/OeDHbolIfFAhjwNpaWmsX7+eefPmAaHJcurWrcszzzxz4G+MwmO3RCTxqGslzrg7AwYMYO7cueG2/Px8Dj300F9W0rBBkVpJXSsJwsyYM2dO+CG0AJmZmdx0002/rKRhgyJSjAp5nGrdujXuztixYwG48847MTM+//xzDRsUkRJUyOPc3/72NzZs2BBebteuHX379sU1bFBEiqiPPIE899xzXHrppeHluXPn0q9fvwATiUgsqY+8Bhg4cCAFBQV07NgRgP79+2NmbNmyJeBkAdMwTKnlVMgTTEpKCp9++ikfffRRuK1Ro0YMHjw4wFQB0zBMqeXUtZLgGjZsyLZt28LLy5cvrz0P1NUwTKll1LVSE+XlsbVbN754++1wU/v27UuOOa/JNAxTBFAhT2xFXQqtn3sOd2fQoEEAbNy4ETNj1qxZAQeMMg3DFAHUtZKYDtClsHnNGg455JASzTt37qzS8/8SygUXhAr6yJEweXLowufMmUGnEokKda3UJAfoUmjUqBHuzqSiR7Y1Az5MS2NYvD1eLlJmzoSJE6FLl9C7irjUQirkiagSXQpXXnkle/fu5RbgJKDHq69iZqxcuTKw2CISHSrkiaqiOzvT0khKTmY0kAyMBhzIatUKM4t9XhGJGhXyRFVRl0IZ3S9PA0cXfdnMePHFF2OZWESiRIW8piqj+2XIlVfy3ldfhVc599xzMTMKCwsDDCoi1aVCXpOV0f3SqlUr3J2zzz47vFpycjJXXXVVgEFFpDo0/LAW27lzJ+n7ul6KrF69miOPPDKgRCJyIBp+KPtJS0vD3bn22mvDbdnZ2boYKpJgVMiF+++/n9J/mZkZs2fPDiiRiFSFCrmEuTvPP/98ePm8887T2blIAlAhlxIuvPDCMs/OR4wYEVAiEamICrmUyd357rvvwstTpkzBzNi+fXuAqUSkLNUu5GZ2lJn9n5ktN7PPzOzair9LEkHz5s1xd9q2bRtua9CggbpbROJMJM7I9wB/cvf2QC/gKjNrH4HtSpzIzc2loKCgRJuZ8XaxedBFJDjVLuTunufui4s+bwVyAQ1ErmFSUlJwd4YOHRpuO+WUU3R2LhIHItpHbmYtgeOBD8v42kgzW2hmC9etWxfJ3UoMTZ06tcyLoVdffXVAiUQkYoXczBoALwDXuft+j3V398nunuPuOU2bNo3UbiUg7s6cOXPCyxMnTsTM2Lt3b4CpRGqniBRyM6tDqIhPc3fN7F9L9O/ff7+z8+yUFN42239aXRGJmkiMWjHgMSDX3e+rfiRJNO7O+vXrAcIPspiUlcU3egiySExE4oy8N/A74DQzW1L0GhCB7UoCyczOxqHEgyyOPuYYdha/GJqXB3366GxdJMIiMWplvrubu3d2965Fr1ciEU4SSKkHWWyH8IMszIx7770Xbr8d5s+H8eODTCpS4+jOTomMUg+yqJ+URMcTT2QtsAP48w03wEMPQWFh6N0M0tKCTi1SI6iQS+SUepBF12bNcHeOAaYROktn3/vgwaA+dJGI0IMlJCZ+uvBCMmfOZDdQF3gEuGT9ejIzM2MXIi8PBg6E6dOhWbPY7VckQvRgCQnUYe4kjx5NL+Bh4HCgSZMmsb0ztCb00euCsZRBZ+QSc7t27SKtVP/4M888w6WXXhqdHaalhfruS0tNhZ07o7PPaBk9Gh55BK64AiZNCjqNxJjOyCVupKam4u7UrVs33DZo0KDonZ2XGlFDenri9dGnpYUuEOuCsZRBhVwC8/PPP5c5b0uHDh3K/6aD6VooNaKGXbtCy4nUT14TfhlJ1KiQS+Dcnfvu++Wm4OXLl5c9b0teHpxwArz7btX7uUuNqEm4Puaa8MtIokZ95BJXyupecfea1c99sC64IFTQR46EyZNDv9hmamqj2qS8PvKUIMKIlMfd+f7772nRokW4bacZZfYEJyXVrq6F4kV74sTgckjcUdeKxJ19j5jbZ98NRQWlV/zd79S1IILOyCWOuTvuTlJSElsI/WPdQ2hSLuvQAbbsN+29SK2kM3KJa2aGu9O1WTMmAScAk4AXPvtM/cMiRXRGLgnhxLw8TgSuNiP8UDkzMjIy2Lx5c4DJRIKnM3JJKO7OBx98EF7esmULZsYWdbNILaZCLgmnV69e+91I1KhRo9jO2yISR1TIJWG5OztLjSE3M+bNmxdQIpFgqJBLQts3b0vbtm3Dbf369dPZudQqKuRSNXE6jWpubm6Z87ZcfPHFASUSiR0VcqmaOJ/T292ZMmVKeHnGjBmYGYWFhQGmEokuzbUilZOAc52UO2+LSILSfORSPQk4jaq7s2rVqhJtZsbXX38dUCKR6FAhl8pJ0GlUs7Oz9zsLP/bYY3UxVGoUFXKpvASe09vd9+snNzMmTJgQUCKRyFEfudQ6I0eO5NFHHy3Rpr5zSQTqI5fgxNmQxcmTJ5c5VLFp06YBJRKpHhVyib44HbLo7rz77rvh5fXr12NmbNu2LcBUIlWnrhWJngQasqihipII1LUisZdAQxbdne3bt5doMzPeeOONgBKJVJ4KuURPgg1ZTE9Px9055phjwm1nnHHG/mfrcdbnLxKRQm5m/c3sCzP7yszGRGKbUkMk4JDFr7/+usyLoUOGDAktxGmfv9Re1e4jN7NkYAXQF1gNfAxc6u7Ly/se9ZFLoPLyYOBAmD69wr8OHn74Ya688koAdgBpZa0Uh33+UjNFs4+8B/CVu690993Ac8B5EdiuSHRU4Yx61KhR4bPzY4BpQLgnPY77/KV2iUQhPxIoPqHF6qK2EsxspJktNLOF69ati8BuRaooLQ3M4KGHoLAw9G4Waq+Au/Phd9+xBUgFdgJ7d+xgW3Jy3Pb5S+0Rs4ud7j7Z3XPcPUc3XkggqjmKpnnz5lx5/vk8DPQCHgZe/Z//0bwtErhIFPIfgKOKLWcXtYnEl0iMopk5k6vcWVJYyNXARUXNZsb//u//RiO1SIUiUcg/Bo4zs6PNrC4wEJgdge2KRF6ERtGYGe7OuHHjwm2XXHKJzs4lEBG5s9PMBgD3A8nA4+5+x4HW16gVibgqjESJhtIF/KyzzuKVV16JeQ6p2aJ6Z6e7v+Lurd29VUVFXCQqAh7b7e4UPzmZM2cOZsZODUuUGNBcK5LY4nA+l9Jn502aNEEjtSQSNNeK1ExxOJ+Lu7Njx47w8r5ZFVesWBFYJqnZVMglscXpfC5paWm4OzfeeGO4rU2bNroYKlGhQi6JL17nc8nL464FC/C8vBLNZsaTTz4ZTCapkdRHLhIto0fDI4/AFVfApEm8+eabnH766SVWKSws1Fm6VJr6yEVipZypAE77zW/2m1UxKSmJc845J6CgUlOokItEWgUXYN29xCiWl156CTNjw4YNQaSVGkCFXCTSKnEBtkmTJrg7Z555ZrgtMzNT3SxyUFTIRaKhkhdgX331VQoLC0u0mRnvvPNOLFJKDaGLnSJxYsqUKYwYMaJEmx4ALcXpYqdInBs+fHiZj5gbO3ZsQIkkUaiQi8QZd2f58l+elPj3v/8dM2P37t0BppJ4pkIuEofatWuHu5ORkRFuq1evHkceud/Dt0RUyEXi2ebNm0vMoPjjjz9iZqyJl7tXJS6okIvEudTUVNyd//iP/wi3ZWVl0aRJkwBTSTxRIRdJEPfdd1+Ji6H5+fkaqiiACrlIwnF3li5dGl7u06dP+NFzUjupkIskoM6dO+PudO/ePdyWlJTEvffeG2AqCYpuCBJJcFu2bKFRo0Yl2rZt20b9+vUDSiTRohuCRGqojIwM3J3bbrst3NagQQNOO+204EJJTOmMXKQGcXeSkkqen+Xm5tK2bduAEkkk6YxcpBbYd9Fz3rx54bZ27dppVsUaToVcpAbq27cv7k7dunXDbWbGjBkzAkwl0aJCLlKD/fzzz6xatSq8fPHFF2Nm7N27N8BUEmkq5CI1XHZ2Nu7O4MGDw20pKSlcddVVAaaSSNLFTpFaZPfu3dSrV69E29q1aznssMMCSiRVoYudIkLdunVxdx5//PFw2+GHH06zYo+hk8SjQi5SC11++eUlbulfu3YtZsZ7770XYCo5WCrkIrWYu7NkyZLw8kknnaR5WxKQCrlILdelSxfcneOPPz7clpSUxH333RdgKqmKahVyM7vHzD43s0/M7J9mdkikgolIbC1evJhNmzaFl//0pz9hZuzYsSPAVFIZ1T0jfw3o6O6dgRXAf1Y/kogEpVGjRrg7t9xyS7itfv369O/fP8BUUpFqFXJ3n+fue4oWFwDZ1Y8kIkEbP348hYWF4eVXX30VM2PFihUBppLyRLKP/PfAnPK+aGYjzWyhmS1ct25dBHcrItGw76Ln3Llzw21t2rTRvC1xqMJCbmavm9myMl7nFVtnLLAHmFbedtx9srvnuHtO06ZNI5NeRKKuX79++82qaGa88MILAaaS4lIqWsHdzzjQ181sGHA2cLprzJJIjbV3716+//57WrRoAcBFF10EwJ49e0hOTg4yWq1X3VEr/YEbgXPdXZe2RWq45s2b4+5ccskl4baUlBT++te/BphKqjXXipl9BdQD8ouaFrj7qIq+T3OtiCS+n3/+mdTU1PBy/fr1Wbt2rR4xF0VRmWvF3Y9196PcvWvRq8IiLiI1Q7169XB3PvroIwC2b99OgwYNmDBhQsDJah/d2Ski1dK9e3fcnREjRgBw/fXXY2Yl5kGX6FIhF5GImDx5Mj/++GN4uXnz5gwdOjTARLWHCrmIRExWVhbuzsSJEwF46qmnMDMWLFgQcLKaTYVcRCJu9OjR7Nixg333jJx44om0bduWgoKCgJPVTCrkIhIVaWlp/PTTT7z22msAfPHFF9StW5dp08q9b1AOkgq5iETVGWecQWFhIQMGDABgyJAhmBkbNmwIOFnNoUIuIlFnZrz88sslJt3KzMxkzJgxAaaqOVTIRSRmjjvuuBLT5N51112YGbm5uQEnS2wq5CISc+PHj2fjxo3h5fbt24e7YKTqVMhFJBCHHHII7s5zzz0HwBtvvEFycjKvvvpqwMkSjwq5iATqkksuoaCggM6dOwPQv39/MjIy9Ii5KlAhF5HApaSksHTpUj7++GMAtm7dSv369fmv//qvgJMlBhVyEYkbOTk5uDt/+MMfALjuuuswM1avXh1wsvimQi4icWfKlCn88MMP4eWjjjqKYcOGBRcozqmQi0hcOuKII3B3/vu//xuAqVOnYmbhaXPlFyrkIhLXrr76anbs2EFmZiYAPXv2pH379pq3pRgVchGJe2lpaaxfv5558+YBkJubS926dXn22WcDThYfVMhFJGH07duXwsJC+vXrB8CgQYM0bwsq5CKSYMyMuXPn8vnnn4fbMjMzuemmmwJMFSwVchFJSG3atMHdwwX8zjvvxMxKFPjaQoVcRBLaHXfcUaJrpV27dpx55pm4e4CpYkuFXEQSXuPGjXH38MXP1157jaSkpPDF0ZpOhVxEaoyBAwdSUFBAhw4dAOjXrx+HHnooO3fuDDhZdKmQi0j15OVBnz6wZk3QSYDQvC3Lli0L3zi0ceNG0tPTeeCBBwJOFj0q5CJSPbffDvPnw/jxQScpoXv37rg7l19+OQDXXnstZlbi1v+aQoVcRA5OWhqYwUMPQWFh6N0s1B5HHn/88RKTbmVnZ/P73/8+wESRp0IuIgdn5UoYNAjS00PL6ekweDB8802wucpw5JFH4u7haXGfeOIJzCw8bW6iUyEXkYOTlQUZGbBrF6Smht4zMqBZs6CTleuPf/wj27dvp1GjRgD06NGDTp06sWfPnoCTVY8KuYgcvLVrYdQoWLAg9B4nFzwPJD09nU2bNoUfKbds2TLq1KnD9OnTA0528CyIQfM5OTm+cOHCmO9XRKQ4d6dfv3689tpr4bYNGzbQuHHjAFOVz8wWuXtO6faInJGb2Z/MzM2sSSS2JyISC2bGvHnzyM3NDbcdeuih3HzzzQGmqrpqF3IzOwo4E/i++nFERGKvbdu2uDtjxowBQrf9mxkrVqwIOFnlROKMfAJwI1B7JjYQkRrpzjvvJD8/P7zcpk0bzjrrrLift6VahdzMzgN+cPellVh3pJktNLOF69atq85uRUSi5tBDD8XdmTZtGgBz584lKSmJ119/PeBk5avwYqeZvQ6UNZ5oLHATcKa7bzazb4Ecd19f0U51sVNEEkFBQQFdunQJ96FnZmayatUq0gK66emgL3a6+xnu3rH0C1gJHA0sLSri2cBiM4vfQaQiIlVQp04dli9fzoIFCwDIz88nPT2dBx98MOBkJR1014q7f+ruh7l7S3dvCawGurl7/A8kFRGpgp49e+LuDB06FIBrrrkGM+PHH38MOFmIbggSEamkqVOnsmrVqvDykUceyfDhwwNMFBKxQl50Zl5h/7iISCLLzs7G3ZkwYQIAjz32GGbGokWLAsukM3IRkYNw3XXXsX37dho2bAhATk4OXbt2DWTeFhVyEZGDlJ6ezpYtW3jllVcAWLp0KXXq1GHGjBkxzaFCLiJSTWeddRZ79+7l9NNPB+Diiy/GzNi0aVNM9q9CLiISAftuGlq+fHm4rXHjxtx6663R33fU9yAiUou0a9cOd+fGG28E4Pbbb8fM+PLLL6O2TxVyEZEouOuuu1i//peBfK1bt+acc86Jyr5UyEVEoiQzMxN356mnngLgpZde4quvvor4flTIRUSibMiQIezevZv334NGnOwAAAUeSURBVH+fVq1aRXz7KRHfooiI7KdOnTqceOKJUdm2zshFRGIlLw/69In4s01VyEVEYuX222H+fBg/PqKbVSEXEYm2tDQwg4cegsLC0LtZqD0CVMhFRKJt5UoYNAjS00PL6ekweDB8801ENq9CLiISbVlZkJEBu3ZBamroPSMDmkXmOTwq5CIisbB2LYwaBQsWhN4jeMFTww9FRGJh5sxfPk+cGNFN64xcRCTBqZCLiCQ4FXIRkQSnQi4ikuBUyEVEEpwKuYhIgjN3j/1OzdYB38V8x79oAqyvcK1gxXvGeM8HyhgpyhgZkcjYwt2blm4MpJAHzcwWuntO0DkOJN4zxns+UMZIUcbIiGZGda2IiCQ4FXIRkQRXWwv55KADVEK8Z4z3fKCMkaKMkRG1jLWyj1xEpCaprWfkIiI1hgq5iEiCqxWF3Mz+n5l9ZmaFZlbu8B8z629mX5jZV2Y2Job5DjWz18zsy6L3xuWst9fMlhS9Zsco2wGPiZnVM7PpRV//0MxaxiJXFTMOM7N1xY7d8Bjne9zMfjKzZeV83czsgaL8n5hZt1jmq2TGU8xsc7FjeGsAGY8ys/8zs+VF/z9fW8Y6gR7LSmaM/LF09xr/AtoBbYC3gJxy1kkGvgaOAeoCS4H2Mcp3NzCm6PMY4K5y1tsW4+NW4TEBRgMPF30eCEyPw4zDgAcD/Pf3a6AbsKycrw8A5gAG9AI+jMOMpwAvBXUMizJkAd2KPjcEVpTxsw70WFYyY8SPZa04I3f3XHf/ooLVegBfuftKd98NPAecF/10ULSfqUWfpwK/jdF+K1KZY1I8+/PA6WZmcZYxUO7+DrDhAKucB/yPhywADjGzrNikC6lExsC5e567Ly76vBXIBY4stVqgx7KSGSOuVhTySjoSWFVseTUx+AEUOdzd84o+rwEOL2e9VDNbaGYLzCwWxb4yxyS8jrvvATYDmTHItt/+i5T3c7uw6E/t583sqNhEq7Qg/+1VxYlmttTM5phZhyCDFHXhHQ98WOpLcXMsD5ARInwsa8yj3szsdaCsJ5mOdfd/xTpPaQfKV3zB3d3MyhsT2sLdfzCzY4A3zexTd/860llroBeBZ939ZzO7gtBfEKcFnCnRLCb072+bmQ0AZgHHBRHEzBoALwDXufuWIDJUpIKMET+WNaaQu/sZ1dzED0DxM7XsoraIOFA+M1trZlnunlf0Z+BP5Wzjh6L3lWb2FqHf9tEs5JU5JvvWWW1mKUAjID+KmUqrMKO7F88zhdA1iXgS1X97kVC8GLn7K2Y2ycyauHtMJ6oyszqECuQ0d59ZxiqBH8uKMkbjWKpr5RcfA8eZ2dFmVpfQhbuYjAwp2s9lRZ8vA/b7C8LMGptZvaLPTYDewPIo56rMMSme/SLgTS+6ohMjFWYs1Ud6LqF+y3gyGxhaNOKiF7C5WFdbXDCzZvuufZhZD0K1I5a/sCna/2NArrvfV85qgR7LymSMyrGM5RXdoF7A+YT6yn4G1gKvFrUfAbxSbL0BhK4yf02oSyZW+TKBN4AvgdeBQ4vac4ApRZ9/BXxKaFTGp8AfYpRtv2MCjAfOLfqcCswAvgI+Ao4J4OdbUcY7gc+Kjt3/AW1jnO9ZIA8oKPp3+AdgFDCq6OsGTCzK/ynljKwKOOPVxY7hAuBXAWQ8CXDgE2BJ0WtAPB3LSmaM+LHULfoiIglOXSsiIglOhVxEJMGpkIuIJDgVchGRBKdCLiKS4FTIRUQSnAq5iEiC+/8qa2qlV6wGIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_ajustadas_Newton , 'k-',x, y, 'r*')\n",
    "plt.legend(['modelo lineal','datos'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hasta este punto comparemos con lo obtenido previamente por QR o *polyfit***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para descenso en gradiente con búsqueda de línea por *backtracking* se tiene un error relativo de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010680295738530715"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(beta_5-beta)/np.linalg.norm(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para descenso con dirección de Newton (sin búsqueda de línea por *backtracking*) se tiene un error relativo de::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3290679549401932e-16"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(beta_1-beta)/np.linalg.norm(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Tenemos alrededor de $2$ dígitos correctos para descenso en gradiente y búsqueda de línea por *backtracking* y máxima precisión con dirección de Newton sin búsqueda de línea por *backtracking*.\n",
    "* El cálculo anterior lo realizamos por lote o *batch*. \n",
    "* El nombre de *batch* o lote se utiliza pues la información de primer o segundo orden es calculada utilizando todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método vía gradiente estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la formulación del método consideramos a la función objetivo $f_o$ escrita en la forma:\n",
    "\n",
    "$$f_o(\\beta) = \\frac{1}{2}\\displaystyle \\sum_{i=1}^{20} (y_i - A[i,:]^T\\beta)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con $A[i,:]$ $i$-ésimo renglón de $A$ y obsérvese que dividimos por $2$ como antes. Calculamos el gradiente de $f_o$:\n",
    "\n",
    "$$\\nabla f_o (\\beta) = -\\displaystyle \\sum_{i=1}^{20} (y_i - A[i,:]^T\\beta)A[i,:]$$\n",
    "\n",
    "**obs:** La expresión anterior considera $A[i,:] \\in \\mathbb{R}^{2 \\times 1}$, \n",
    "$A[i,:]=\\left[\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "x_i\n",
    "\\end{array}\n",
    "\\right ]\n",
    "$  $\\forall i=1,\\dots, 20$ con $x_i$ dato dado en el ejemplo de mínimos cuadrados lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el método estocástico utilizando el gradiente (nombrado **descenso en gradiente estocástico**), utilizamos un punto inicial $\\beta^{(0)}$ y en lugar de usar la actualización:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta^{(k)}&=&\\beta^{(k-1)} - t_{k-1}\\nabla f\\left(\\beta^{(k-1)}\\right) \\nonumber\\\\\n",
    "&=& \\beta^{(k-1)} - t_{k-1}\\left(-\\displaystyle \\sum_{i=1}^{20} (y_i - A[i,:]^T\\beta^{(k-1)})A[i,:]\\right)\\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "para $k=1,2,\\dots$ y $t_{k-1}$ cantidad positiva, obtenida por búsqueda de línea con *backtracking*. **Las actualizaciones en descenso en gradiente estocástico son**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta^{(k)} &=& \\beta^{(k-1)} - \\eta_{{k-1}}\\nabla f_{i_{k-1}}\\left(\\beta^{(k-1)}\\right) \\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "para $k=1,2,\\dots$ con $\\nabla f_{i_{k-1}}(\\cdot)$ el gradiente calculado a partir de una **muestra extraída del conjunto de índices de los renglones: $\\{1,2,\\dots,n\\}$**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La cantidad $\\eta_{k-1}$ en este contexto se le nombra **tasa de aprendizaje** y al igual que en el método de descenso en gradiente es una cantidad positiva.\n",
    "\n",
    "* La muestra de índices para calcular $\\nabla f_{i_{k-1}}(\\cdot)$ puede contener uno o más índices. Si contiene un único índice por ejemplo $i_{k-1} = \\{i\\}$ con $i=1,\\cdots, n$ se tiene una actualización de la forma:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta^{(k)} &=& \\beta^{(k-1)} - \\eta_{{k-1}}\\nabla f_{i_{k-1}}\\left(\\beta^{(k-1)}\\right) \\nonumber\\\\\n",
    "&=& \\beta^{(k-1)} - \\eta_{k-1}(-(y_i - A[i,:]^T\\beta^{(k-1)})A[i,:]) \\nonumber \\\\\n",
    "&=& \\beta^{(k-1)} + \\eta_{k-1}(y_i - A[i,:]^T\\beta^{(k-1)})A[i,:] \\nonumber\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el ejemplo numérico de mínimos cuadrados lineales, iniciamos con las iteraciones dadas por la fórmula de actualización tomando $\\beta_0=(0,0)^T$ y $\\eta_{k-1} = 0.1 \\forall k=1,2,\\dots$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.array([0,0])\n",
    "eta_0 = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos un índice de los renglones de $A$ de forma pseudoaleatoria y hacemos la actualización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1989) #para reproducibilidad\n",
    "i = np.random.randint(mpoints, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = beta_0 + eta_0*(y[i]-A[i,:].dot(beta_0))*A[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36687934, -0.5890533 ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **algoritmo** es:\n",
    "\n",
    "* Repetir el siguiente bloque para $k=1,2,\\dots$ hasta convergencia:\n",
    "\n",
    "    * Extraer una muestra aleatoria $i_{k-1}$ del conjunto de índices de los renglones de $A$: $\\{1,2,\\dots,n\\}$.\n",
    "    * Calcular la tasa de aprendizaje $\\eta_{k-1}$ (positiva).\n",
    "    * Calcular la actualización $\\beta^{(k)} = \\beta^{(k-1)} + \\eta_{k-1}\\nabla f_{i_{k-1}}\\left(\\beta^{(k-1)}\\right)$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Midamos el error relativo con este algoritmo utilizando **sólo un índice** extraído al azar del conjunto de índices de renglones de $A$ para el ejemplo de mínimos cuadrados lineales: $\\beta^{(k)} = \\beta^{(k-1)} + \\eta_{k-1}(y_i - A[i,:]^T\\beta^{(k)})A[i,:]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_relativo(aprox, obj):\n",
    "    return np.linalg.norm(aprox-obj)/np.linalg.norm(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos puntos iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter=5\n",
    "tol=1e-2\n",
    "err=error_relativo(beta_0,beta)\n",
    "beta_k = beta_0\n",
    "k=1\n",
    "eta_k = .1 #constante para todas las iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(err>tol and k <= maxiter):\n",
    "    i = int(np.random.randint(mpoints, size=1))\n",
    "    beta_k = beta_k+eta_k*(y[i]-A[i,:].dot(beta_k))*A[i,:]\n",
    "    k+=1\n",
    "    eta_k=.1\n",
    "    err=error_relativo(beta_k,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235855479140276"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tenemos un error relativo de $92\\%$ con $5$ iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50527765, 0.02895074])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos una **muestra más grande** de índices para calcular $\\nabla f_{i_{k-1}}(\\cdot)$ de modo que las actualizaciones ahora son de la forma:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta^{(k)} &=& \\beta^{(k-1)} - \\eta_{{k-1}}\\nabla f_{i_{k-1}}\\left(\\beta^{(k-1)}\\right) \\nonumber\\\\\n",
    "&=& \\beta^{(k-1)} - \\eta_{k-1}\\displaystyle \\sum_{i \\in i_{k-1}} (-(y_i - A[i,:]^T\\beta^{(k)})A[i,:]) \\nonumber\\\\\n",
    "&=& \\beta^{(k-1)} + \\eta_{k-1}\\displaystyle \\sum_{i \\in i_{k-1}}(y_i - A[i,:]^T\\beta^{(k)})A[i,:] \\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para $k=1,2,\\dots$ e $i_{k-1}$ una muestra de índices extraída de forma aleatoria de los índices de los renglones de $A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos $4$ iteraciones con una muestra de índices de tamaño $5$. Puntos iniciales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter=4\n",
    "tol=1e-2\n",
    "err=error_relativo(beta_0,beta)\n",
    "beta_k = beta_0\n",
    "k=1\n",
    "eta_k = .1 #constante para todas las iteraciones\n",
    "m_sample = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(err>tol and k <= maxiter):\n",
    "    idx = np.random.choice(mpoints, m_sample,replace=False) #muestra de tamaño m_sample sin reemplazo\n",
    "    beta_k = beta_k+eta_k*(y[idx]-A[idx,:]@beta_0)@A[idx,:]\n",
    "    k+=1\n",
    "    eta_k=.1\n",
    "    err=error_relativo(beta_k,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166377257028253"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obsérvese que tenemos un error relativo de $51\\%$ con $4$ iteraciones, una muestra de tamaño $5$ y es menor al que se obtuvo con una muestra de tamaño 1 de índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.21121877, -4.1743939 ])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**  Algunos comentarios generales para el enfoque por *batch* o lote y el estocástico que podemos realizar son:\n",
    "\n",
    "* El enfoque por *batch*:\n",
    "\n",
    "    * Busca **direcciones de descenso** en cada iteración vía las actualizaciones de $\\{\\beta_{k}\\}$ hasta convergencia del mínimo de la función objetivo $f_o$. \n",
    "    * Por el punto anterior, la velocidad de convergencia de los métodos iterativos por *batch* oscila entre lineal, superlineal y cuadrática para ejemplos reales pero es **muy dependiente de la cantidad de datos** que se utilicen.\n",
    "    * La convergencia depende del punto inicial y de las cantidades positivas $t_{k-1}$.\n",
    "    * El cómputo de las direcciones de descenso utilizan toda la información disponible en cada iteración.\n",
    "    * Por la definición de sus actualizaciones, el cómputo de las direcciones de descenso se puede realizar con **cómputo en paralelo**.\n",
    "\n",
    "\n",
    "* El enfoque estocástico:\n",
    "\n",
    "    * Genera $\\beta_{k}$'s cuyo comportamiento está determinado por la secuencia aleatoria $\\{i_{k-1}\\}$. De hecho $\\{\\beta_{k}\\}$ es un [proceso estocástico](https://en.wikipedia.org/wiki/Stochastic_process). \n",
    "    * Además del punto anterior, su convergencia depende de la taza de aprendizaje $\\eta_{k-1}$. \n",
    "    * En el contexto de grandes cantidades de datos la **convergencia es independiente del tamaño de los datos**: si aumentamos más cantidad de datos, su velocidad de convergencia es independiente de esto.\n",
    "    * No necesariamente se obtienen direcciones de descenso con este método. Si en **promedio o en esperanza matemática** se calculan direcciones de descenso con las actualizaciones de $\\{\\beta_k\\}$, entonces la secuencia $\\{\\beta_{k}\\}$ puede guiarse hacia el mínimo de la función objetivo $f_o$.\n",
    "    * Por el punto anterior, en iteraciones iniciales podemos tener un gran avance hacia el mínimo pero en iteraciones no iniciales puede quedarse el método oscilando en una región cercana al mínimo (como una canica en un tazón).\n",
    "    * Es un método más complicado de implementar con cómputo en paralelo que la versión *batch*.\n",
    "    * No utilizamos en cada iteración todos los datos disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Por el error relativo obtenido en el ejemplo anterior parecería que el método estocástico de descenso en gradiente no es una opción fuerte para problemas de optimización. Y esto para problemas de tamaño chico o mediano es cierto pero para grandes cantidades de datos hay razones prácticas, teóricas e intituitivas\\* que posicionan a este método por encima de formulaciones en *batch*. \n",
    "\n",
    "\\*Una razón intuitiva del por qué el método estocástico de descenso en gradiente podría ganarle en velocidad a una formulación en *batch* es la siguiente: supóngase que un **conjunto de entrenamiento** $\\mathcal{S}$ consistiera de renglones redundantes. Imaginemos que $\\mathcal{S}$ consta de $10$ repeticiones de un conjunto más pequeño $\\mathcal{S}_\\text{sub}$. Entonces ejecutar un método *batch* sería $10$ veces más costoso que si solamente tuviéramos una única copia de $\\mathcal{S}_\\text{sub}$. El método estocástico de descenso en gradiente tendría el mismo costo en cualquiera de los dos escenarios ($10$ copias vs $1$ copia) pues elige con misma probabilidad renglones del conjunto $\\mathcal{S}_\\text{sub}$. En la realidad un conjunto de entrenamiento no contiene duplicados exactos de la información pero en muchas aplicaciones *large-scale* existe (de forma aproximada) información redundante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Optimización numérica convexa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaciones de *machine learning* conducen al planteamiento de problemas de optimización convexa y no convexa. \n",
    "\n",
    "Ejemplo de **problemas convexos** los encontramos en la aplicación de clasificación de textos en donde se desea asignar un texto a clases definidas de acuerdo a su contenido (p.ej. determinar si un docu de texto es sobre política). La aplicación anterior puede formularse utilizando **funciones de pérdida convexas**.\n",
    "\n",
    "Como ejemplos de aplicaciones en el ámbito de la **optimización no convexa** están el reconocimiento de voz y reconocimiento de imágenes. El uso de [redes neuronales](https://en.wikipedia.org/wiki/Artificial_neural_network) [profundas](https://en.wikipedia.org/wiki/Deep_learning)\\* ha tenido muy buen desempeño en tales aplicaciones haciendo uso de cómputo en la GPU, ver [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb), [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [2012: A Breakthrough Year for Deep Learning](https://medium.com/limitlessai/2012-a-breakthrough-year-for-deep-learning-2a31a6796e73). En este caso se utilizan **funciones objetivo no lineales y no convexas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*Los tipos de redes neuronales profundas, *deep neural networks*, que han sido mayormente usadas en el inicio del siglo XXI son las mismas que las que eran populares en los años $90$'s. El éxito de éstos tipos y su uso primordialmente se debe a la disponibilidad de *larger datasets* y mayores recursos computacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde los $40$'s se han desarrollado algoritmos para resolver problemas de optimización, se han analizado sus propiedades y se han desarrollado buenas implementaciones de software.  Sin embargo, una clase de problemas de optimización en los que encontramos métodos **efectivos** son los convexos. \n",
    "\n",
    "**En el módulo IV del curso nos enfocamos mayormente a métodos numéricos para resolver problemas convexos principalmente por lo anterior y porque métodos para optimización no convexa utilizan parte de la teoría de convexidad desarrollada en optimización convexa. Además un buen número de problemas de aprendizaje utilizan funciones de pérdida convexas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tablita útil para fórmulas de diferenciación con el operador $\\nabla$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $f,g:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ son diferenciables y $\\alpha_1, \\alpha_2 \\in \\mathbb{R}$, $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$ son fijas. Diferenciando con respecto a la variable $x \\in \\mathbb{R}^n$ se tiene:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|:--:|:--:|\n",
    "|linealidad | $\\nabla(\\alpha_1 f(x) + \\alpha_2 g(x)) = \\alpha_1 \\nabla f(x) + \\alpha_2 \\nabla g(x)$|\n",
    "|producto | $\\nabla(f(x)g(x)) = \\nabla f(x) g(x) + f(x) \\nabla g(x)$|\n",
    "|producto punto|$\\nabla(b^Tx) = b$ \n",
    "|cuadrático|$\\nabla(x^TAx) = 2(A+A^T)x$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiciones utilizadas en el curso (algunas de ellas...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que continúa se considera $f_0 = f_o$ (el subíndice \"0\" y el subíndice \"o\" son iguales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema estándar de optimización\n",
    "\n",
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(x) \\leq 0, \\quad \\forall i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(x) = 0, \\quad \\forall i=1,\\dots,p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con $f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ $\\forall i=0,\\dots,m$, $h_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, $\\forall i=1,\\dots,p$. $f_i$ son las **restricciones de desigualdad**, $h_i$ son las **restricciones de igualdad**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominio del problema de optimización\n",
    "\n",
    "El conjunto de puntos para los que la función objetivo y las funciones de restricción $f_i, h_i$ están definidas se nombra **dominio del problema de optimización**, esto es:\n",
    "\n",
    "$$\\mathcal{D} = \\bigcap_{i=0}^m\\text{dom}f_i \\cap \\bigcap_{i=1}^p\\text{dom}h_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(en github aparecen los símbolos de intersección más abajo de lo que deberían aparecer...abriendo con jupyter el notebook esto no pasa...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Un punto $x \\in \\mathcal{D}$ se nombra **factible** si satisface las restricciones de igualdad y desigualdad. El conjunto de puntos factibles se nombra **conjunto de factibilidad**.\n",
    "\n",
    "* El problema anterior se nombra factible si existe **al menos un punto factible**, si no se cumple lo anterior entonces es infactible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Óptimo del problema de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor óptimo del problema se denota como $p^*$. En notación matemática es:\n",
    "\n",
    "$$p^* = \\inf\\{f_o(x) | f_i(x) \\leq 0, \\forall i=1,\\dots,m, h_i(x) = 0 \\forall i=1,\\dots,p\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Si el problema es **infactible** entonces $p^* = \\infty$.\n",
    "\n",
    "* Si $\\exists x_k$ factible tal que $f_o(x_k) \\rightarrow -\\infty$ para $k \\rightarrow \\infty$ entonces $p^*=-\\infty$ y se nombra **problema de optimización no acotado por debajo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto óptimo del problema de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^*$ es **punto óptimo** si es factible y $f_o(x^*) = p^*$. El conjunto de óptimos se nombra **conjunto óptimo** y se denota:\n",
    "\n",
    "$$X_{\\text{opt}} = \\{x | f_i(x) \\leq 0 \\forall i=1,\\dots,m, h_i(x) =0 \\forall i=1,\\dots,p, f_o(x) = p^*\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Si existe un punto óptimo se dice que el valor óptimo se alcanza y por tanto el problema de optimización tiene solución, es *solvable*.\n",
    "\n",
    "* Si $X_{\\text{opt}} = \\emptyset$ se dice que el valor óptimo no se alcanza. Obsérvese que para problemas no acotados nunca se alcanza el valor óptimo.\n",
    "\n",
    "* Si $x$ es factible y $f_o(x) \\leq p^* + \\epsilon$ con $\\epsilon >0$, $x$ se nombra **$\\epsilon$-subóptimo** y el conjunto de puntos $\\epsilon$-subóptimos se nombra **conjunto $\\epsilon$-subóptimo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Óptimo local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un punto factible se nombra óptimo local si $\\exists R > 0$ tal que:\n",
    "\n",
    "$$f_o(x) = \\inf \\{f_o(z) | f_i(z) \\leq 0 \\forall i=1,\\dots,m, h_i(z) = 0 \\forall i=1,\\dots, p, ||z-x||_2 \\leq R\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, $x$ resuelve:\n",
    "\n",
    "$$\\displaystyle \\min_{z \\in \\mathbb{R}^n} f_o(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(z) \\leq 0, \\forall i =1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(z) =0, \\forall i=1,\\dots,p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||z-x||_2 \\leq R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** la palabra **óptimo** se utiliza para **óptimo global**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricciones activas, no activas y redundantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $x$ es factible y $f_i(x)=0$ entonces la restricción de desigualdad $f_i(x) \\leq 0$ se nombra **restricción activa en $x$**. Se nombra **inactiva en $x$** si $f_i(x) <0$ para alguna $i$.\n",
    "\n",
    "**Comentario:** Las restricciones de igualdad, $h_i(x)$, siempre son activas en el conjunto factible. \n",
    "\n",
    "\n",
    "Una restricción se nombra **restricción redundante** si al quitarla el conjunto factible no se modifica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas de optimización convexa en su forma estándar o canónica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\min_{x \\in \\mathbb{R}^n} f_o(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{sujeto a:}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_i(x) \\leq 0 , i=1,\\dots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i(x)=0, i=1,\\dots,p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde: $f_i$ son **convexas** $\\forall i=0,2,\\dots,m$ y $h_i$ $\\forall i =1,\\dots,p$ son **funciones afín**\\*. \n",
    "\n",
    "\\*Una función afín es de la forma $h(x) = Ax+b$ con $A \\in \\mathbb{R}^{m \\times n}$ y $b \\in \\mathbb{R}^m$. En la definición anterior $h_i(x) = a_i^Tx-b_i$ con $a_i \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}$ $\\forall i=1,\\dots,p$ y geométricamente $h_i(x)$ es un **hiperplano** en $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** Lo siguiente se puede verificar con propiedades teóricas:\n",
    "\n",
    "* El conjunto de factibilidad de un problema de optimización convexa es un conjunto convexo. Esto se puede ver pues es una intersección finita de conjuntos convexos: intersección entre las $x$'s que satisfacen $f_i(x) \\leq 0$, que se nombra **conjunto subnivel**\\*, y las $x$'s que están en un hiperplano.\n",
    "\n",
    "\\*Un conjunto $\\alpha$-subnivel es de la forma $\\{x \\in \\text{dom}f | f(x) \\leq \\alpha\\}$. Si $f$ es convexa el conjunto subnivel es un conjunto convexo.\n",
    "\n",
    "* El conjunto óptimo y los conjuntos $\\epsilon$-subóptimos son convexos.\n",
    "\n",
    "* Si la función objetivo $f_o$ es **fuertemente convexa** entonces el conjunto óptimo contiene a lo más un punto.\n",
    "\n",
    "* Si en el problema anterior se tiene que **maximizar** una $f_o$ función objetivo **cóncava** y se tienen misma forma estándar y $f_i$ convexa, $h_i$ afín entonces también se nombra al problema como **problema de optimización convexa**. Todos los resultados, conclusiones y algoritmos desarrollados para los problemas de minimización son aplicables para maximización. En este caso se puede resolver un problema de maximización al minimizar la función objetivo  $-f_o$ que es convexa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función convexa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ una función con el conjunto $\\text{dom}f$ convexo. $f$ se nombra convexa  (en su $\\text{dom}f$) si $\\forall x,y \\in \\text{dom}f$ y $\\theta \\in [0,1]$ se cumple:\n",
    "\n",
    "$$f(\\theta x + (1-\\theta) y) \\leq \\theta f(x) + (1-\\theta)f(y).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* $\\text{dom}f$ es convexo $\\therefore$ $\\theta x + (1-\\theta)y \\in \\text{dom}f$\n",
    "\n",
    "* La convexidad de $f$ se define para $\\text{dom}f$ aunque para casos en particular se detalla el conjunto en el que $f$ es convexa.\n",
    "\n",
    "* Si la desigualdad se cumple de forma estricta $\\forall x \\neq y$ $f$ se nombra **estrictamente convexa**.\n",
    "\n",
    "* $f$ es **cóncava** si $-f$ es convexa y **estrictamente cóncava** si $-f$ es estrictamente convexa. Otra forma de definir concavidad es con una desigualdad del tipo:\n",
    "\n",
    "$$f(\\theta x + (1-\\theta) y) \\geq \\theta f(x) + (1-\\theta)f(y).$$\n",
    "\n",
    "y mismas definiciones para $x,y, \\theta$ que en la definición de convexidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si $f$ es convexa, geométricamente el segmento de línea que se forma con los puntos $(x,f(x)), (y,f(y))$ está por encima o es igual a $f(\\theta x + (1-\\theta)y) \\forall \\theta \\in [0,1]$ y $\\forall x,y \\in \\text{dom}f$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/fdcx1k150nfwykv/draw_convexity_for_functions.png?dl=0\" heigth=\"300\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La desigualdad que define a funciones convexas se nombra **desigualdad de Jensen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto convexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Línea y segmentos de línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sean $x_1, x_2 \\in \\mathbb{R}^n$ con $x_1 \\neq x_2$. Entonces el punto:\n",
    "\n",
    "$$y = \\theta x_1 + (1-\\theta)x_2$$\n",
    "\n",
    "con $\\theta \\in \\mathbb{R}$ se encuentra en la línea que pasa por $x_1$ y $x_2$. $\\theta$ se le nombra parámetro y si $\\theta \\in [0,1]$ tenemos un segmento de línea:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/dldljf5igy8xt9d/segmento_linea.png?dl=0\" heigth=\"200\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** \n",
    "\n",
    "* $y = \\theta x_1 + (1-\\theta)x_2 = x_2 + \\theta(x_1 -x_2)$ y esta última igualdad se interpreta como \"$y$ es la suma del punto base $x_2$ y la dirección $x_1-x_2$ escalada por $\\theta$\". \n",
    "\n",
    "* Si $\\theta=0$ entonces $y=x_2$. Si $\\theta \\in [0,1]$ entonces $y$ se \"mueve\" en la dirección $x_1-x_2$ hacia $x_1$ y si $\\theta>1$ entonces $y$ se encuentra en la línea \"más allá\" de $x_1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/nbahrio7p1mj4hs/segmento_linea_2.png?dl=0\" heigth=\"350\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto enmedio entre $x_1$ y $x_2$ tiene $\\theta=\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto convexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un conjunto $\\mathcal{C}$ es convexo si el segmento de línea entre cualquier par de puntos de $\\mathcal{C}$ está completamente contenida en $\\mathcal{C}$. Esto se escribe matemáticamente como:\n",
    "\n",
    "$$\\theta x_1 + (1-\\theta) x_2 \\in \\mathcal{C} \\forall \\theta \\in [0,1], \\forall x_1, x_2 \\in \\mathcal{C}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos gráficos de conjuntos convexos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/gj54ism1lqojot6/ej_conj_convexos.png?dl=0\" heigth=\"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos gráficos de conjuntos no convexos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/k37zh5v3iq3kx04/ej_conj_no_convexos.png?dl=0\" heigth=\"350\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:** \n",
    "\n",
    "\n",
    "* El punto $\\displaystyle \\sum_{i=1}^k \\theta_i x_i$ con $\\displaystyle \\sum_{i=1}^k \\theta_i=1$, $\\theta_i \\geq 0 \\forall i=1,\\dots,k$ se nombra **combinación convexa** de los puntos $x_1, x_2, \\dots, x_k$. Una combinación convexa de los puntos $x_1, \\dots, x_k$ puede pensarse como una mezcla o promedio ponderado de los puntos, con $\\theta_i$ la fracción $\\theta_i$ de $x_i$ en la mezcla.\n",
    "\n",
    "* Un conjunto es convexo si y sólo si contiene cualquier combinación convexa de sus puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de funciones convexas y cóncavas\n",
    "\n",
    "* Una función afín es convexa y cóncava en todo su dominio: $f(x) = Ax+b$ con $A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^n$, $\\text{dom}f = \\mathbb{R}^n$.\n",
    "\n",
    "\n",
    "**Obs:** por tanto las funciones lineales también son convexas y cóncavas.\n",
    "\n",
    "* Funciones cuadráticas: $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, $f(x) = \\frac{1}{2} x^TPx + q^Tx + r$ son convexas en su dominio: $\\mathbb{R}^n$. $P \\in \\mathcal{S}_+^n, q \\in \\mathbb{R}^n, r \\in \\mathbb{R}$ con $\\mathbb{S}_+^n$ conjunto de **matrices simétricas positivas semidefinidas**\\*.\n",
    "\n",
    "\\*Una matriz $A$ es positiva semidefinida si $x^TAx \\geq 0$ $\\forall x \\in \\mathbb{R}^n - \\{0\\}$. Si se cumple de forma estricta la desigualdad anterior entonces $A$ es **positiva definida**. Con los eigenvalores podemos caracterizar a las matrices definidas y semidefinidas positivas: $A$ es semidefinida positiva si y sólo si los eigenvalores de $T=\\frac{A+A^T}{2}$ son no negativos. Es definida positiva si y sólo si los eigenvalores de $T$ son positivos. Los conjuntos de matrices que se utilizan para definir a matrices semidefinidas positivas y definidas positivas son $\\mathbb{S}_{+}^n$ y $\\mathbb{S}_{++}^n$ respectivamente.\n",
    "\n",
    "**Obs:** $f$ es estrictamente convexa si y sólo si $P \\in \\mathbb{S}_{++}^n$. $f$ es cóncava si y sólo si $P \\in -\\mathbb{S}_+^n$.\n",
    "\n",
    "* Exponenciales: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x) = e^{ax}$ para cualquier $a \\in \\mathbb{R}$ es convexa en su dominio: $\\mathbb{R}$.\n",
    "\n",
    "* Potencias: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x)=x^a$:\n",
    "\n",
    "    * Si $a \\geq 1$ o $a \\leq 0$ entonces $f$ es convexa en $\\mathbb{R}_{++}$ (números reales positivos).\n",
    "    * Si $0 \\leq a \\leq 1$ entonces $f$ es cóncava en $\\mathbb{R}_{++}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Potencias del valor absoluto: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x)=|x|^p$ con $p \\geq 1$ es convexa en $\\mathbb{R}$.\n",
    "\n",
    "* Logaritmo: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, $f(x) = \\log(x)$ es cóncava en su dominio: $\\mathbb{R}_{++}$.\n",
    "\n",
    "* Entropía negativa: $f(x) = \\begin{cases}\n",
    "x\\log(x) &\\text{ si } x > 0 ,\\\\\n",
    "0 &\\text{ si } x = 0\n",
    "\\end{cases}$ es estrictamente convexa en su dominio: $\\mathbb{R}_+$.\n",
    "\n",
    "* Normas: cualquier norma definida es convexa en sus dominios.\n",
    "\n",
    "* Función máximo: $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $f(x) = \\max\\{x_1,\\dots,x_n\\}$ es convexa.\n",
    "\n",
    "* Función log-sum-exp: $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $f(x)=\\log\\left(\\displaystyle \\sum_{i=1}^ne^{x_i}\\right)$ es convexa en su dominio: $\\mathbb{R}^n$.\n",
    "\n",
    "* La media geométrica: $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, $f(x) = \\left(\\displaystyle \\prod_{i=1}^n x_i \\right)^\\frac{1}{n}$ es cóncava en su dominio: $\\mathbb{R}_{++}^n$.\n",
    "\n",
    "* Función log-determinante: $f: \\mathbb{S}^{n} \\rightarrow \\mathbb{R}^n$, $f(x) = \\log(\\det(X))$ es cóncava en su dominio: $\\mathbb{S}_{++}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados útiles de teoría de convexidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sobre funciones convexas/cóncavas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Sea $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ diferenciable entonces $f$ es convexa si y sólo si $\\text{dom}f$ es un conjunto convexo y se cumple:\n",
    "\n",
    "$$f(y) \\geq f(x) + \\nabla f(x)^T(y-x) \\forall x,y \\in \\text{dom}f.$$\n",
    "\n",
    "Si se cumple de forma estricta la desigualdad $f$ se nombra estrictamente convexa. También si su $\\text{dom}f$ es convexo y se tiene la desigualdad en la otra dirección \"$\\leq$\" entonces $f$ es cóncava.\n",
    "\n",
    "Geométricamente este resultado se ve como sigue para $\\nabla f(x) \\neq 0$:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/e581e22xeejdwu0/convexidad_con_hiperplano_de_soporte.png?dl=0\" heigth=\"350\" width=\"350\">\n",
    "\n",
    "\n",
    "y el hiperplano $f(x) + \\nabla f(x)^T(y-x)$ se nombra **hiperplano de soporte para la función $f$ en el punto $(x,f(x))$**. Obsérvese que si $\\nabla f(x)=0$ se tiene $f(y) \\geq f(x) \\forall y \\in \\text{dom}f$ y por lo tanto $x$ es un mínimo global de $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una función es convexa si y sólo si es convexa al restringirla a cualquier línea que intersecte su dominio, esto es, si $g(t) = f(x + tv)$ es convexa $\\forall x,v \\in \\mathbb{R}^n$, $\\forall t \\in \\mathbb{R}$ talque $x + tv \\in \\text{dom}f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sea $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ dos veces diferenciable, entonces $f$ es convexa en $\\text{dom}f$ si y sólo si $\\text{dom}f$ es convexo y $\\nabla^2f(x)$ es semidefinida positiva. Si $\\nabla^2f(x)$ es definida positiva y $\\text{dom}f$ es convexo entonces $f$ es estrictamente convexa en $\\text{dom}f$ (el recíproco no es verdadero).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sobre problemas de optimización**\n",
    "\n",
    "* Si $f$ es diferenciable y $x^*$ es óptimo entonces $\\nabla f(x^*) = 0$.\n",
    "\n",
    "* Si $f$ es dos veces diferenciable y $x^*$ es mínimo entonces $\\nabla^2 f(x^*)$ es una matriz semidefinida positiva.\n",
    "\n",
    "* Si $\\nabla f(x^*)=0$ y $\\nabla^2f(x^*)$ es una matriz definida positiva entonces $x^*$ es mínimo local.\n",
    "\n",
    "* Una propiedad fundamental de un óptimo local en un problema de optimización convexa es que también es un óptimo global.\n",
    "\n",
    "* Si $f_o$ en un problema de optimización convexa es diferenciable y $X$ es el conjunto de factibilidad entonces $x$ es óptimo si y sólo si $x \\in X$ y $\\nabla f_o(x)^T(y-x) \\geq 0$ $\\forall y \\in X$. Esta propiedad se reduce a la condición: $x$ es óptimo si y sólo si $\\nabla f_o(x) = 0$.\n",
    "\n",
    "Geométricamente el resultado anterior se visualiza para $\\nabla f(x) \\neq 0$ y $-\\nabla f(x)$ apuntando hacia la dirección dibujada:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/0tmpivvo5ob4oox/optimo_convexidad_con_hiperplano_de_soporte.png?dl=0\" heigth=\"550\" width=\"550\">\n",
    "\n",
    "\n",
    "**Comentario:** Por los resultados anteriores los métodos de optimización que revisaremos en el módulo IV buscarán resolver la **ecuación no lineal** $\\nabla f_o(x)=0$. Dependiendo del número de soluciones de la ecuación $\\nabla f_o(x)=0$ se tienen situaciones distintas. Por ejemplo, si no tiene solución entonces el/los óptimos no se alcanza(n) pues el problema puede no ser acotado por debajo o si existe el óptimo éste puede no alcanzarse. Por otro lado, si la ecuación tiene múltiples soluciones entonces cada solución es un mínimo de $f_o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "* S. P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2009.\n",
    "\n",
    "* L. Bottou, F. E. Curtis, J Nocedal, Optimization Methods for Large-Scale Machine Learning, SIAM, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
