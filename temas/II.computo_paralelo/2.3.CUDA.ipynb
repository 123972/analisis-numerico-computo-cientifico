{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Unified Device Architecture (CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un poco de historia..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La industria de videojuegos impulsó el desarrollo de las tarjetas gráficas a una velocidad sin precedente a partir del año 1999 para incrementar el nivel de detalle visual en los juegos de video. Alrededor del 2003 se planteó la posibilidad de utilizar las unidades de procesamiento gráfico para procesamiento en paralelo relacionado con aplicaciones distintas al ambiente de gráficas. A partir del 2006 la empresa [NVIDIA](https://www.nvidia.com/en-us/about-nvidia/) introdujo *CUDA*, una plataforma *GPGPU*\\* y un modelo de programación que facilita el procesamiento en paralelo en las GPU's.\n",
    "\n",
    "\\*GPGPU es un término que se utilizó para referirse a la programación general en unidades de procesamiento gráfico, hoy en día se conoce simplemente como *GPU programming*. Ver [General-purpose computing on graphics processing units](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde entonces las tarjetas gráficas han creado una brecha significativa con las unidades de procesamiento, CPU's. Ver por ejemplo las gráficas que *NVIDIA* publica año tras año y que están relacionadas con el número de operaciones en punto flotante por segundo (FLOPS) y la transferencia de datos en la memoria RAM de la GPU: [from-graphics-processing-to-general-purpose-parallel-computing](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#from-graphics-processing-to-general-purpose-parallel-computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día (2020) se continúa el desarrollo de GPU's con mayor RAM, con mayor capacidad de cómputo y mejor conectividad con la CPU. Ver [samsung-amd-rdna-gpu-2021](https://wccftech.com/samsung-amd-rdna-gpu-2021/), [playstation-5-specifications-revealed-but-design-is-still-a-mystery](https://www.theguardian.com/games/2020/mar/19/playstation-5-specifications-revealed-but-design-is-still-a-mystery), [xbox-series-x-tech](https://news.xbox.com/en-us/2020/03/16/xbox-series-x-tech/) y recientemente [IBM Supercomputer Summit Attacks Coronavirus…](https://www.ibm.com/blogs/nordic-msp/ibm-supercomputer-summit-attacks-coronavirus/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura en la que podemos ubicar a las GPU's es en la de un sistema **MIMD** y **SIMD**. De hecho es [SIMT: Simple Instruction Multiple Thread](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) en un modelo de sistema de memoria compartida pues \"los *threads* en un **warp\\*** cargan la misma instrucción para ser ejecutada\". Ver [2.1.Un_poco_de_historia_y_generalidades](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.1.Un_poco_de_historia_y_generalidades.ipynb) para las clasificación de Flynn en la que se encuentran los sistemas MIMD y SIMD.\n",
    "\n",
    "\\* Un *warp* en el contexto de *GPU programming* es un conjunto de *threads* y que equivale a $32$, esto es, un warp equivale a $32$ *threads*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Diferencia con la CPU multicore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En cuanto al hardware:\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/k11qub01w4nvksi/CPU_multicore.png?dl=0\" heigth=\"500\" width=\"500\">\n",
    "\n",
    "**GPU**\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/lw9kia12qhwp95r/GPU.png?dl=0\" heigth=\"500\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** obsérvese en el dibujo anterior la diferencia en tamaño del cache en la CPU y GPU. También la unidad de control es más pequeña en la GPU.\n",
    "\n",
    "A diferencia de una máquina multicore o multi CPU's con la habilidad de lanzar en un instante de tiempo unos cuantos *threads*, por ejemplo cuatro threads en una máquina *quad core*, la *GPU* puede lanzar cientos o miles de threads en un instante siendo cada core *heavily multithreaded*. Sí hay restricciones en el número de threads que se pueden lanzar en un instante pues las tarjetas gráficas tienen diferentes características (modelo) y arquitecturas (ver [List of NVIDIA GPU's](https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units)) pero la diferencia es grande. Por ejemplo, el modelo **GT 200** (2009) en un instante puede lanzar 30,720 threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver [How Graphics Cards Work](https://computer.howstuffworks.com/graphics-card1.htm) y [How Microprocessors Work](https://computer.howstuffworks.com/microprocessor.htm) para más información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Otras compañías producen tarjetas gráficas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver por ejemplo la lista de GPU's de [Advanced Micro Devices](https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Si tengo una tarjeta gráfica de AMD puedo correr un programa de CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es posible pero entre las alternativas están:\n",
    "\n",
    "* [OpenCl](https://www.khronos.org/opencl/)\n",
    "\n",
    "* [OpenACC](https://www.openacc.org/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Si tengo una tarjeta gráfica de NVIDIA un poco antigua puedo correr un programa de CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las GPU's producidas por NVIDIA desde 2006 son capaces de correr programas basados en **CUDA C**. La cuestión sería revisar qué *compute capability* tiene tu tarjeta. Ver [Compute Capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) para las características que tienen las tarjetas más actuales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es CUDA C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una extensión al lenguaje C de programación en el que se utiliza una nueva sintaxis para procesamiento en la GPU. Contiene también una librería *runtime* que define funciones que se ejecutan desde **host** por ejemplo para alojar y desalojar memoria en **device**, transferir datos entre la memoria host y la memoria device\\* o manejar múltiples devices. La librería *runtime* está hecha encima de una API de C de bajo nivel llamada [NVIDIA CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html) la cual es accesible desde el código. Para información de la API de la librería runtime ver [NVIDIA CUDA Runtime API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* La transferencia de datos entre la memoria del *host* a *device* o viceversa es un *bottleneck* fuerte que nos lleva a replantear la necesidad de tal transferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿A qué se refiere la terminología de host y device?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Host es la máquina multicore o multi CPU's y device es la GPU. Una máquina puede tener múltiples GPU's por lo que tendrá múltiples devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tengo una tarjeta NVIDIA CUDA capable ¿qué debo realizar primero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar instalaciones dependiendo de tu sistema operativo. Ver [Instalación](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/C/extensiones_a_C/CUDA/instalacion) donde además se encontrará información para instalación de [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalé lo necesario y al ejecutar en la terminal `nvcc -V` obtengo la versión... ¿cómo puedo probar mi instalación?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Obteniendo información del NVIDIA driver ejecutando en la terminal:\n",
    "\n",
    "```\n",
    "$nvidia-smi\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 440.26       Driver Version: 440.26       CUDA Version: 10.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  GeForce GTX 750     Off  | 00000000:01:00.0 Off |                  N/A |\n",
    "| 40%   29C    P8     1W /  38W |     55MiB /   978MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID   Type   Process name                             Usage      |\n",
    "|=============================================================================|\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compilando y ejecutando el siguiente programa de *CUDA C*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programa de hello world:** `hello_world.cu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    func<<<2,3>>>(); //2 bloques de 3 threads cada uno\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación: `$nvcc hello_world.cu -o hello_world.out`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución: `$./hello_world.out`:\n",
    "\n",
    "```\n",
    "Hello world del bloque 1 del thread 0!\n",
    "Hello world del bloque 1 del thread 1!\n",
    "Hello world del bloque 1 del thread 2!\n",
    "Hello world del bloque 0 del thread 0!\n",
    "Hello world del bloque 0 del thread 1!\n",
    "Hello world del bloque 0 del thread 2!\n",
    "Hola del cpu thread\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Haciendo un query a la GPU para ver qué características tiene (lo siguiente es posible ejecutar sólo si se instaló el *CUDA toolkit*): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery Starting...\n",
    "\n",
    " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
    "\n",
    "\n",
    "Detected 1 CUDA Capable device(s)\n",
    "\n",
    "Device 0: \"Tesla K80\"\n",
    "  CUDA Driver Version / Runtime Version          9.1 / 9.1\n",
    "  CUDA Capability Major/Minor version number:    3.7\n",
    "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
    "  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores\n",
    "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
    "  Memory Clock rate:                             2505 Mhz\n",
    "  Memory Bus Width:                              384-bit\n",
    "  L2 Cache Size:                                 1572864 bytes\n",
    "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
    "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
    "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
    "  Total amount of constant memory:               65536 bytes\n",
    "  Total amount of shared memory per block:       49152 bytes\n",
    "  Total number of registers available per block: 65536\n",
    "  Warp size:                                     32\n",
    "  Maximum number of threads per multiprocessor:  2048\n",
    "  Maximum number of threads per block:           1024\n",
    "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
    "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
    "  Maximum memory pitch:                          2147483647 bytes\n",
    "  Texture alignment:                             512 bytes\n",
    "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
    "  Run time limit on kernels:                     No\n",
    "  Integrated GPU sharing Host Memory:            No\n",
    "  Support host page-locked memory mapping:       Yes\n",
    "  Alignment requirement for Surfaces:            Yes\n",
    "  Device has ECC support:                        Enabled\n",
    "  Device supports Unified Addressing (UVA):      Yes\n",
    "  Supports Cooperative Kernel Launch:            No\n",
    "  Supports MultiDevice Co-op Kernel Launch:      No\n",
    "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 30\n",
    "  Compute Mode:\n",
    "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
    "\n",
    "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.1, NumDevs = 1\n",
    "Result = PASS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué librerías/paquetes/extensiones se revisarán en el módulo II del curso?\n",
    "\n",
    "* [CUDA_C](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA_C.ipynb). En ésta nota además de revisar las *keywords* al lenguaje C que NVIDIA añadió, se hará una descripción pequeña del hardware de la GPU.\n",
    "\n",
    "Introducción al uso de funciones en:\n",
    "\n",
    "* [CuPy](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.Python_CuPy.ipynb)\n",
    "\n",
    "* [gputools](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.R_gputools.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Por qué usar CUDA y CUDA-C o más general cómputo en la GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NVIDIA como se mencionó al inicio de la nota fue de las primeras compañías en utilizar la GPU para tareas no relacionadas con el área de gráficos y ha colaborado en el avance del conocimiento de las GPU's y desarrollo de algoritmos y tarjetas gráficas. Otra compañía es [Khronos_Group](https://en.wikipedia.org/wiki/Khronos_Group) por ejemplo, quien actualmente desarrolla [OpenCl](https://www.khronos.org/opencl/).\n",
    "\n",
    "* El cómputo en la GPU constituye hoy en día (2020) una alternativa fuerte a la implementación de modelos de machine learning ampliamente utilizada por la comunidad científica. En el terreno de cómputo matricial y *deep learning*\\* se encuentran [tensorflow](https://github.com/tensorflow), [caffe](https://github.com/BVLC/caffe), [Theano](https://github.com/Theano/Theano) (aunque ya no es soportado actualmente: 2020 pero está retomado en [PyMC3](https://github.com/pymc-devs/pymc3)), [Pytorch](https://github.com/pytorch/pytorch) y [keras](https://github.com/keras-team/keras) como ejemplos de lo anterior. \n",
    "\n",
    "\\*Deep learning se ha utilizado para resolver problemas en *machine learning* típicos. Ejemplos de esto son la clasificación de imágenes, de sonidos o análisis de textos, ver por ejemplo [Practical text analysis using deep learning](https://medium.com/@michael.fire/practical-text-analysis-using-deep-learning-5fb0744efdf9).\n",
    "\n",
    "* Sí hay publicaciones científicas para la implementación de *deep learning* en las CPU's, ver por ejemplo el paper reciente de [SLIDE](https://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf), [HashingDeepLearning](https://github.com/keroro824/HashingDeepLearning) y las entradas [An algorithm could make CPUs a cheap way to train AI](https://www.engadget.com/2020/03/03/rice-university-slide-cpu-gpu-machine-learning/) y [Deep learning rethink overcomes major obstacle in AI industry](https://www.sciencedaily.com/releases/2020/03/200305135041.htm) \\*. Sin embargo, esta área se encuentra en activa investigación por lo que se han adoptado el uso de implementaciones del *deep learning* utilizando GPU's. \n",
    "\n",
    "\\*El paper plantea una discusión a realizar con la frase *...change in the state-of-the-art algorithms can render specialized hardware less effective in the future*. Ver por ejemplo [Tensor Cores](https://developer.nvidia.com/tensor-cores), [NVIDIA TENSOR CORES, The Next Generation of Deep Learning](https://www.nvidia.com/en-us/data-center/tensorcore/), [The most powerful computers on the planet](https://www.ibm.com/thought-leadership/summit-supercomputer/)\\* como ejemplos de hardware especializado en aprendizaje automático con Tensorflow.\n",
    "\n",
    "\\*Summit powered by 9,126 IBM Power9 CPUs and over 27,000 NVIDIA V100 Tensor Core GPUS, is able to do 200 quadrillion calculations per second... [IBM Supercomputer Summit Attacks Coronavirus…](https://www.ibm.com/blogs/nordic-msp/ibm-supercomputer-summit-attacks-coronavirus/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión**\n",
    "\n",
    "1)¿Qué factores han determinado un mejor performance de una GPU vs una CPU? (contrasta los diseños de una CPU vs una GPU).\n",
    "\n",
    "2)¿Dentro de qué modelo de arquitectura de máquinas se ubica a la GPU y que puede comparársele con el modelo **Single Program Multiple Data (SPMD)** dentro de la taxonomía de Flynn?\n",
    "\n",
    "3)¿Qué significan las siglas CUDA y detalla qué es CUDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "1. N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.\n",
    "\n",
    "2. [CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/C/extensiones_a_C/CUDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
