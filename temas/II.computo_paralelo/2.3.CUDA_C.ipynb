{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas para contenedor de docker:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "```\n",
    "docker run --gpus all --rm -v $(pwd):/datos --name jupyterlab_nvidia_cuda_c_container -p 8888:8888 -d palmoreck/jupyterlab_nvidia_cuda_c:1.1.0\n",
    "```\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "```\n",
    "docker stop jupyterlab_nvidia_cuda_c_container\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación de la imagen de docker `palmoreck/jupyterlab_nvidia_cuda_c:1.1.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/nvidia/cuda_c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: si se desean ejecutar los ejemplos que se presentan a continuación, es necesario tener una tarjeta gráfica NVIDIA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA C y generalidades de CUDA y GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en extensiones al lenguaje C y en una *runtime library*. Ver [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb) para más información.\n",
    "\n",
    "### Kernel\n",
    "\n",
    "* En CUDA C se define una función que se ejecuta en el device y que se le nombra **kernel**. El *kernel* inicia con la sintaxis:\n",
    "\n",
    "```\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "y siempre es tipo `void` (no hay `return`).\n",
    "\n",
    "* El llamado al *kernel* se realiza desde el host y con una sintaxis en la que se define el número de threads y bloques que serán utilizados para la ejecución del kernel. La sintaxis que se utiliza es con `<<< >>>` y en la primera entrada se coloca el número de bloques y en la segunda entrada el número de *threads*:\n",
    "\n",
    "\n",
    "```\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    int par;\n",
    "    mifun<<<N,5>>> (par); //N bloques de 5 threads\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Programa de hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "}\n",
    "int main(void){\n",
    "    func<<<1,1>>>(); //1 bloque de 1 thread\n",
    "    printf(\"Hello world!\\n\");\n",
    "return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall hello_world.cu -o hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La función `main` se ejecuta en la CPU.\n",
    "\n",
    "* La función `func` es un *kernel* y es ejecutada por *threads* en el *device* (GPU), también llamados **CUDA threads**. Obsérvese que la función `func` inicia con `__global__` para diferenciarla de `main`. En este caso el *thread* que fue lanzado no realiza ninguna acción pues el cuerpo del kernel está vacío.\n",
    "\n",
    "* El *kernel* sólo puede tener un `return` tipo *void*: `__global__ void func` por lo que el *kernel* debe regresar sus resultados a través de sus argumentos.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Programa de hello world 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_2.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    func<<<2,3>>>(); //2 bloques de 3 threads cada uno\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall hello_world_2.cu -o hello_world_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world del bloque 1 del thread 0!\n",
      "Hello world del bloque 1 del thread 1!\n",
      "Hello world del bloque 1 del thread 2!\n",
      "Hello world del bloque 0 del thread 0!\n",
      "Hello world del bloque 0 del thread 1!\n",
      "Hello world del bloque 0 del thread 2!\n",
      "Hola del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La extensión del archivo debe ser `.cu` aunque esto puede modificarse al compilar con `nvcc`: \n",
    "\n",
    "`$nvcc -x cu hello_world.c -o hello_world.out`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El llamado a la ejecución del kernel se realizó en el *host* y se lanzaron $2$ bloques (primera posición), cada uno con $3$ *threads*.\n",
    "\n",
    "* Se utiliza la función [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d) para que el *cpu-thread* espere la finalización de la ejecución del kernel.\n",
    "\n",
    "* Los *CUDA threads* son divididos en bloques, **CUDA blocks** y todos los bloques se encuentran en un **grid**. En el lanzamiento del *kernel*  se debe especificar al hardware cuántos bloques tendrá nuestro *grid* y cuántos *threads* estarán en cada bloque. Las variables `blockIdx` y `threadIdx` hacen referencia a los **id**'s que tienen los bloques y los threads. El *id* del bloque dentro del *grid* y el *id* del thread dentro del bloque. La parte `.x` de `blockIdx.x` y `threadIdx.x` refiere a la **primera coordenada** del bloque en el *grid* y del *thread* en en el bloque. \n",
    "\n",
    "* La elección del número de bloques en un grid o el número de *threads* en un bloque no corresponde a alguna disposición del hardware, esto es, si se lanza un kernel con `<<< 1, 3 >>>` no implica que la GPU tenga en su hardware un bloque o 3 *threads*.\n",
    "\n",
    "* En una GPU podemos definir el *grid* de bloques y el bloque de *threads* utilizando el tipo de dato `dim3` el cual también es parte de CUDA C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world_3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_3.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(2,1,1); //2 bloques en el grid\n",
    "    dim3 dimBlock(3,1,1); //3 threads por bloque\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall hello_world_3.cu -o hello_world_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world del bloque 1 del thread 0!\n",
      "Hello world del bloque 1 del thread 1!\n",
      "Hello world del bloque 1 del thread 2!\n",
      "Hello world del bloque 0 del thread 0!\n",
      "Hello world del bloque 0 del thread 1!\n",
      "Hello world del bloque 0 del thread 2!\n",
      "Hola del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** obsérvese que puede definirse un grid de tres dimensiones y también un bloque de tres dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Programa de suma vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting suma_vectorial.cu\n"
     ]
    }
   ],
   "source": [
    "%%file suma_vectorial.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void suma_vect(int *a, int *b, int *c){\n",
    "    int block_id_x = blockIdx.x;\n",
    "    if(block_id_x<N) //aquí se asume que el valor de N \n",
    "                     //es menor al número máximo de bloques que se pueden lanzar\n",
    "        c[block_id_x] = a[block_id_x]+b[block_id_x];\n",
    "}\n",
    "int main(void){\n",
    "    int a[N], b[N],c[N];\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    //alojando en device\n",
    "    cudaMalloc((void **)&device_a, sizeof(int)*N);\n",
    "    cudaMalloc((void **)&device_b, sizeof(int)*N);\n",
    "    cudaMalloc((void **)&device_c, sizeof(int)*N);\n",
    "    //llenando los arreglos con datos dummy:\n",
    "    for(i=0;i<N;i++){\n",
    "        a[i]=i;\n",
    "        b[i]=i*i;\n",
    "    }\n",
    "    //copiamos arreglos a, b a la GPU\n",
    "    cudaMemcpy(device_a,a,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(device_b,b,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    //mandamos a llamar a suma_vect:\n",
    "    suma_vect<<<N,1>>>(device_a,device_b,device_c); //N bloques de 1 thread\n",
    "    cudaDeviceSynchronize();\n",
    "    //copia del resultado al arreglo c:\n",
    "    cudaMemcpy(c,device_c,N*sizeof(int),cudaMemcpyDeviceToHost);\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",a[i],b[i],c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall suma_vectorial.cu -o suma_vectorial.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./suma_vectorial.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obsérvese que se están utilizando apuntadores en la línea:\n",
    "\n",
    "```\n",
    "    int *device_a, *device_b, *device_c;\n",
    "```\n",
    "\n",
    "pero estos apuntadores no apuntan a una dirección de memoria en el *device* pues aunque NVIDIA añadió el *feature* de [Unified Memory](https://devblogs.nvidia.com/unified-memory-cuda-beginners/) (un espacio de memoria accesible para la CPU y la GPU) aquí no se está usando tal *feature*. Más bien se están utilizando los apuntadores anteriores para apuntar a un [struct](https://en.wikipedia.org/wiki/Struct_(C_programming_language)) de C en el que uno de sus tipos de datos es una dirección de memoria en la GPU.\n",
    "\n",
    "* Para alojar memoria en la GPU se utiliza el llamado a [cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356) y para transferir datos del *host* al *device* o viceversa se llama a lafunción [cudaMemcpy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8) con respectivos parámetros como `cudaMemcpyHostToDevice` o `cudaMemcpyDeviceToHost`. Obsérvese el uso de `(void **)` por la definición de la función `cudaMalloc`.\n",
    "\n",
    "* Para desalojar memoria en la GPU se utiliza el llamado a [cudaFree](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al instalar el *CUDA toolkit* o con el contenedor de docker (detallado al inicio de la nota) se cuenta con la línea de comando [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) para perfilamiento (aunque en la documentación se menciona que será reemplazada tal línea de comando por [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) y [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==514== NVPROF is profiling process 514, command: ./suma_vectorial.out\n",
      "==514== Profiling application: ./suma_vectorial.out\n",
      "==514== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    37.98  1.57e-06         1  1.57e-06  1.57e-06  1.57e-06  suma_vect(int*, int*, int*)\n",
      "                    34.88  1.44e-06         2  7.20e-07  5.44e-07  8.96e-07  [CUDA memcpy HtoD]\n",
      "                    27.13  1.12e-06         1  1.12e-06  1.12e-06  1.12e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.44  0.092668         3  0.030889  4.63e-06  0.092656  cudaMalloc\n",
      "                     0.31  2.87e-04        97  2.96e-06  2.96e-07  1.55e-04  cuDeviceGetAttribute\n",
      "                     0.08  7.31e-05         1  7.31e-05  7.31e-05  7.31e-05  cuDeviceTotalMem\n",
      "                     0.06  5.52e-05         3  1.84e-05  3.40e-06  4.58e-05  cudaFree\n",
      "                     0.05  4.33e-05         1  4.33e-05  4.33e-05  4.33e-05  cuDeviceGetName\n",
      "                     0.03  3.19e-05         3  1.06e-05  5.39e-06  1.50e-05  cudaMemcpy\n",
      "                     0.02  1.83e-05         1  1.83e-05  1.83e-05  1.83e-05  cudaLaunchKernel\n",
      "                     0.00  4.12e-06         1  4.12e-06  4.12e-06  4.12e-06  cudaDeviceSynchronize\n",
      "                     0.00  2.23e-06         1  2.23e-06  2.23e-06  2.23e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.88e-06         3  6.27e-07  3.79e-07  1.11e-06  cuDeviceGetCount\n",
      "                     0.00  1.10e-06         2  5.50e-07  2.89e-07  8.11e-07  cuDeviceGet\n",
      "                     0.00  5.00e-07         1  5.00e-07  5.00e-07  5.00e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "nvprof --normalized-time-unit s ./suma_vectorial.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unidades en las que se reporta, s: second, ms: millisecond, us: microsecond, ns: nanosecond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y en lugar de lanzar $N$ bloques de $1$ thread se pueden lanzar $1$ bloque con $N$ threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting suma_vectorial_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file suma_vectorial_2.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void suma_vect(int *a, int *b, int *c){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    if(thread_id_x<N) //aquí se asume que el valor de N \n",
    "                     //es menor al número máximo de bloques que se pueden lanzar\n",
    "        c[thread_id_x] = a[thread_id_x]+b[thread_id_x];\n",
    "}\n",
    "int main(void){\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    //alojando en device con Unified Memory\n",
    "    cudaMallocManaged(&device_a, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_b, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_c, sizeof(int)*N);\n",
    "    //llenando los arreglos:\n",
    "    for(i=0;i<N;i++){\n",
    "        device_a[i]=i;\n",
    "        device_b[i]=i*i;\n",
    "    }\n",
    "    suma_vect<<<1,N>>>(device_a,device_b,device_c); //1 bloque con N threads\n",
    "    cudaDeviceSynchronize();\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",device_a[i],device_b[i],device_c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall suma_vectorial_2.cu -o suma_vectorial_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./suma_vectorial_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==614== NVPROF is profiling process 614, command: ./suma_vectorial_2.out\n",
      "==614== Profiling application: ./suma_vectorial_2.out\n",
      "==614== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  2.05e-06         1  2.05e-06  2.05e-06  2.05e-06  suma_vect(int*, int*, int*)\n",
      "      API calls:    99.20  0.114920         3  0.038307  1.13e-05  0.114888  cudaMallocManaged\n",
      "                     0.36  4.21e-04         1  4.21e-04  4.21e-04  4.21e-04  cudaLaunchKernel\n",
      "                     0.25  2.87e-04        97  2.96e-06  2.96e-07  1.56e-04  cuDeviceGetAttribute\n",
      "                     0.08  9.69e-05         3  3.23e-05  1.19e-05  6.09e-05  cudaFree\n",
      "                     0.06  7.37e-05         1  7.37e-05  7.37e-05  7.37e-05  cuDeviceTotalMem\n",
      "                     0.02  2.77e-05         1  2.77e-05  2.77e-05  2.77e-05  cuDeviceGetName\n",
      "                     0.01  1.35e-05         1  1.35e-05  1.35e-05  1.35e-05  cudaDeviceSynchronize\n",
      "                     0.00  2.08e-06         3  6.94e-07  3.58e-07  1.32e-06  cuDeviceGetCount\n",
      "                     0.00  1.92e-06         1  1.92e-06  1.92e-06  1.92e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.16e-06         2  5.77e-07  3.10e-07  8.45e-07  cuDeviceGet\n",
      "                     0.00  4.52e-07         1  4.52e-07  4.52e-07  4.52e-07  cuDeviceGetUuid\n",
      "\n",
      "==614== Unified Memory profiling result:\n",
      "Device \"GeForce GTX 750 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       1  8.0000KB  8.0000KB  8.0000KB  8.000000KB  1.7280e-06s  Host To Device\n",
      "       5  25.600KB  4.0000KB  60.000KB  128.0000KB  1.2992e-05s  Device To Host\n",
      "Total CPU Page faults: 2\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "nvprof --normalized-time-unit s ./suma_vectorial_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** obsérvese que el programa anterior utiliza la *Unified Memory* con la función [cudaMallocManaged](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla compuesta del rectángulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "double x=0.0;\n",
    "\n",
    "if(threadIdx.x<=n-1){\n",
    "x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "}\n",
    "    *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n=1e3; //número de subintervalos\n",
    "    double objetivo=0.7468241328124271;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    Rcf<<<1,n>>>(d_data, a,h_hat,n,d_suma); //1 bloque de n threads\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf.cu -o Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
      "Error relativo de la solución: 4.104931878976858e-08\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
      "Error relativo de la solución: 4.104931878976858e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==666== NVPROF is profiling process 666, command: ./Rcf.out\n",
      "==666== Profiling application: ./Rcf.out\n",
      "==666== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    99.41  2.33e-04         1  2.33e-04  2.33e-04  2.33e-04  Rcf(double*, double, double, int, double*)\n",
      "                     0.59  1.38e-06         1  1.38e-06  1.38e-06  1.38e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.19  0.093274         2  0.046637  5.95e-06  0.093269  cudaMalloc\n",
      "                     0.33  3.13e-04        97  3.23e-06  3.34e-07  1.67e-04  cuDeviceGetAttribute\n",
      "                     0.25  2.36e-04         1  2.36e-04  2.36e-04  2.36e-04  cudaDeviceSynchronize\n",
      "                     0.09  8.47e-05         1  8.47e-05  8.47e-05  8.47e-05  cuDeviceTotalMem\n",
      "                     0.05  5.16e-05         2  2.58e-05  6.01e-06  4.56e-05  cudaFree\n",
      "                     0.03  3.13e-05         1  3.13e-05  3.13e-05  3.13e-05  cuDeviceGetName\n",
      "                     0.02  2.03e-05         1  2.03e-05  2.03e-05  2.03e-05  cudaLaunchKernel\n",
      "                     0.02  1.43e-05         1  1.43e-05  1.43e-05  1.43e-05  cudaMemcpy\n",
      "                     0.00  2.20e-06         3  7.32e-07  4.60e-07  1.21e-06  cuDeviceGetCount\n",
      "                     0.00  1.81e-06         1  1.81e-06  1.81e-06  1.81e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.33e-06         2  6.65e-07  3.40e-07  9.91e-07  cuDeviceGet\n",
      "                     0.00  5.73e-07         1  5.73e-07  5.73e-07  5.73e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./Rcf.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "1. N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.\n",
    "\n",
    "2. [CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/C/extensiones_a_C/CUDA)\n",
    "\n",
    "3. [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb)\n",
    "\n",
    "Para más sobre *Unified Memory* revisar:\n",
    "\n",
    "* [Even easier introduction to cuda](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "\n",
    "* [Unified memory cuda beginners](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
