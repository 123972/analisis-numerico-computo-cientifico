{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas para contenedor de docker:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "```\n",
    "docker run --gpus all --rm -v <ruta a mi directorio>:/datos --name jupyterlab_nvidia_cuda_c_container -p 8888:8888 -d palmoreck/jupyterlab_nvidia_cuda_c:1.1.0_10.2\n",
    "```\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "```\n",
    "docker stop jupyterlab_nvidia_cuda_c_container\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación de la imagen de docker `palmoreck/jupyterlab_nvidia_cuda_c:1.1.0_10.2` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/nvidia/cuda_c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta nota utiliza métodos vistos en [1.5.Integracion_numerica](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/I.computo_cientifico/1.5.Integracion_numerica.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: si se desean ejecutar los ejemplos que se presentan a continuación, es necesario tener una tarjeta gráfica NVIDIA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA C y generalidades de CUDA y GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en extensiones al lenguaje C y en una *runtime library*. Ver [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb) para más información.\n",
    "\n",
    "### Kernel\n",
    "\n",
    "* En CUDA C se define una función que se ejecuta en el **device\\*** y que se le nombra **kernel**. El *kernel* inicia con la sintaxis:\n",
    "\n",
    "```\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "y siempre es tipo `void` (no hay `return`).\n",
    "\n",
    "* El llamado al *kernel* se realiza desde el **host\\*** y con una sintaxis en la que se define el número de threads, llamados **CUDA threads** (que son distintos a los *CPU threads*), y bloques, nombrados **CUDA blocks**, que serán utilizados para la ejecución del kernel. La sintaxis que se utiliza es con `<<< >>>` y en la primera entrada se coloca el número de *CUDA blocks* y en la segunda entrada el número de *CUDA threads*:\n",
    "\n",
    "\n",
    "```\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    int par;\n",
    "    mifun<<<N,5>>> (par); //N bloques de 5 threads\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\\* Los nombres de **host** y **device** hacen referencia a la *CPU* y a la *GPU*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Programa de hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "}\n",
    "int main(void){\n",
    "    func<<<1,1>>>(); //1 bloque de 1 thread\n",
    "    printf(\"Hello world!\\n\");\n",
    "return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall hello_world.cu -o hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La función `main` se ejecuta en la CPU.\n",
    "\n",
    "* `func` es un *kernel* y es ejecutada por los *CUDA threads* en el **device** (GPU). Obsérvese que tal función inicia con la sintaxis `__global__`. En este caso el *CUDA thread* que fue lanzado no realiza ninguna acción pues el cuerpo del kernel está vacío.\n",
    "\n",
    "* El *kernel* sólo puede tener un `return` tipo *void*: `__global__ void func` por lo que el *kernel* debe regresar sus resultados a través de sus argumentos.\n",
    " \n",
    "* `nvcc` es un *wrapper* para el compilador de programas escritos en `C`. El compilador instalado en el contenedor de docker descrito al inicio de ésta nota es `gcc`. \n",
    " \n",
    "* La extensión del archivo debe ser `.cu` aunque esto puede modificarse al compilar con `nvcc`: \n",
    "\n",
    "`$nvcc -x cu hello_world.c -o hello_world.out`\n",
    "\n",
    "* En ocasiones para tener funcionalidad de un determinado [compute capability](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) se especifica la *flag* de `-arch=sm_11` en la línea de `nvcc`. En este caso se le indica al compilador que compile el programa para un *compute capability* de $1.1$. Ver [liga](https://stackoverflow.com/questions/16954931/cuda-5-0-cudagetdeviceproperties-strange-grid-size-or-a-bug-in-my-code) y [liga2](https://stackoverflow.com/questions/35656294/cuda-how-to-use-arch-and-code-and-sm-vs-compute) para más sobre esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Bloques de threads? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *CUDA threads* son divididos los **CUDA blocks** y éstos se encuentran en un **grid**. En el lanzamiento del *kernel* se debe especificar al hardware cuántos *CUDA blocks* tendrá nuestro *grid* y cuántos *CUDA threads* estarán en cada bloque. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Programa de hello world 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_2.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    func<<<2,3>>>(); //2 bloques de 3 threads cada uno\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall hello_world_2.cu -o hello_world_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world del bloque 0 del thread 0!\n",
      "Hello world del bloque 0 del thread 1!\n",
      "Hello world del bloque 0 del thread 2!\n",
      "Hello world del bloque 1 del thread 0!\n",
      "Hello world del bloque 1 del thread 1!\n",
      "Hello world del bloque 1 del thread 2!\n",
      "Hola del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* En lo que continúa el nombre *thread* hace referencia a *CUDA thread* y el nombre bloque a *CUDA block*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El llamado a la ejecución del kernel se realizó en el **host** (CPU) y se lanzaron $2$ bloques (primera posición en la sintaxis <<<>>>), cada uno con $3$ *threads*.\n",
    "\n",
    "* Se utiliza la función [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d) para que el *cpu-thread* espere la finalización de la ejecución del kernel.\n",
    "\n",
    "* En el ejemplo anterior, las variables `blockIdx` y `threadIdx` hacen referencia a los **id**'s que tienen los bloques y los threads: el *id* del bloque dentro del *grid* y el *id* del thread dentro del bloque. La parte `.x` de las variables: `blockIdx.x` y `threadIdx.x` refieren a la **primera coordenada** del bloque en el *grid* y a la **primera coordenada** del *thread* en en el bloque. \n",
    "\n",
    "* La elección del número de bloques en un grid o el número de *threads* en un bloque no corresponde a alguna disposición del hardware, esto es, si se lanza un kernel con `<<< 1, 3 >>>` no implica que la GPU tenga en su hardware un bloque o 3 *threads*. Asimismo, las coordenadas que se obtienen vía `blockIdx` o `threadIdx` son meras abstracciones, no corresponden a algún ordenamiento en el hardware de la GPU.\n",
    "\n",
    "* Todos los *threads* de un bloque  ejecutan el kernel por lo que se tienen tantas copias del kernel como número de bloques sean lanzados. Aquí encontramos que en una GPU se tiene el modelo  **Single Instruction Multiple Threads [SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Grid's y bloques 3-dimensionales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una GPU podemos definir el *grid* de bloques y el bloque de *threads* utilizando el tipo de dato `dim3` el cual también es parte de CUDA C:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_3.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.y, threadIdx.z);\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(1,2,1); //2 bloques en el grid\n",
    "    dim3 dimBlock(1,1,3); //3 threads por bloque\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall hello_world_3.cu -o hello_world_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world del bloque 0 del thread 0!\n",
      "Hello world del bloque 0 del thread 1!\n",
      "Hello world del bloque 0 del thread 2!\n",
      "Hello world del bloque 1 del thread 0!\n",
      "Hello world del bloque 1 del thread 1!\n",
      "Hello world del bloque 1 del thread 2!\n",
      "Hola del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing thread_idxs.cu\n"
     ]
    }
   ],
   "source": [
    "%%file thread_idxs.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    if(threadIdx.x==0 && threadIdx.y==0 && threadIdx.z==0){\n",
    "        printf(\"blockIdx.x:%d\\n\",blockIdx.x);\n",
    "    }\n",
    "    printf(\"thread idx.x:%d\\n\",threadIdx.x);\n",
    "    printf(\"thread idx.y:%d\\n\",threadIdx.y);\n",
    "    printf(\"thread idx.z:%d\\n\",threadIdx.z);\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(1,1,1); //1 bloque en el grid\n",
    "    dim3 dimBlock(1,3,1); //3 threads por bloque\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall thread_idxs.cu -o thread_idxs.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockIdx.x:0\n",
      "thread idx.x:0\n",
      "thread idx.x:0\n",
      "thread idx.x:0\n",
      "thread idx.y:0\n",
      "thread idx.y:1\n",
      "thread idx.y:2\n",
      "thread idx.z:0\n",
      "thread idx.z:0\n",
      "thread idx.z:0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./thread_idxs.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing block_idxs.cu\n"
     ]
    }
   ],
   "source": [
    "%%file block_idxs.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"blockIdx.x:%d\\n\",blockIdx.x);\n",
    "    printf(\"blockIdx.y:%d\\n\",blockIdx.y);\n",
    "    printf(\"blockIdx.z:%d\\n\",blockIdx.z);\n",
    "\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(1,2,2); //4 bloques en el grid\n",
    "    dim3 dimBlock(1,1,1); //1 thread por bloque\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall block_idxs.cu -o block_idxs.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockIdx.x:0\n",
      "blockIdx.x:0\n",
      "blockIdx.x:0\n",
      "blockIdx.x:0\n",
      "blockIdx.y:1\n",
      "blockIdx.y:0\n",
      "blockIdx.y:0\n",
      "blockIdx.y:1\n",
      "blockIdx.z:0\n",
      "blockIdx.z:0\n",
      "blockIdx.z:1\n",
      "blockIdx.z:1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./block_idxs.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la variable `blockDim` para cada coordenada `x, y` o `z` y obtener la dimensión de los bloques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing block_dims.cu\n"
     ]
    }
   ],
   "source": [
    "%%file block_dims.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    if(threadIdx.x==0 && threadIdx.y==0 && threadIdx.z==0 && blockIdx.z==1){\n",
    "    printf(\"blockDim.x:%d\\n\",blockDim.x);\n",
    "    printf(\"blockDim.y:%d\\n\",blockDim.y);\n",
    "    printf(\"blockDim.z:%d\\n\",blockDim.z);\n",
    "    }\n",
    "\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(2,2,2); //8 bloques en el grid\n",
    "    dim3 dimBlock(3,1,2); //6 threads por bloque\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall block_dims.cu -o block_dims.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockDim.x:3\n",
      "blockDim.x:3\n",
      "blockDim.x:3\n",
      "blockDim.x:3\n",
      "blockDim.y:1\n",
      "blockDim.y:1\n",
      "blockDim.y:1\n",
      "blockDim.y:1\n",
      "blockDim.z:2\n",
      "blockDim.z:2\n",
      "blockDim.z:2\n",
      "blockDim.z:2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./block_dims.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alojamiento de memoria en el device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alojar memoria en el *device* se utiliza el llamado a [cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356) y para transferir datos del *host* al *device* o viceversa se llama a lafunción [cudaMemcpy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8) con respectivos parámetros como `cudaMemcpyHostToDevice` o `cudaMemcpyDeviceToHost`. \n",
    "\n",
    "Para desalojar memoria del *device* se utiliza el llamado a [cudaFree](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Programa de suma vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N bloques de 1 thread**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing suma_vectorial.cu\n"
     ]
    }
   ],
   "source": [
    "%%file suma_vectorial.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void suma_vect(int *a, int *b, int *c){\n",
    "    int block_id_x = blockIdx.x;\n",
    "    if(block_id_x<N) //aquí se asume que el valor de N \n",
    "                     //es menor al número máximo de bloques que se pueden lanzar\n",
    "                     //si fuese mayor, hay que hacer un ajuste\n",
    "        c[block_id_x] = a[block_id_x]+b[block_id_x];\n",
    "}\n",
    "int main(void){\n",
    "    int a[N], b[N],c[N];\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    dim3 dimGrid(N,1,1); //N bloques en el grid\n",
    "    dim3 dimBlock(1,1,1); //1 threads por bloque \n",
    "    //alojando en device\n",
    "    cudaMalloc((void **)&device_a, sizeof(int)*N); \n",
    "    cudaMalloc((void **)&device_b, sizeof(int)*N);\n",
    "    cudaMalloc((void **)&device_c, sizeof(int)*N);\n",
    "    //llenando los arreglos con datos dummy:\n",
    "    for(i=0;i<N;i++){\n",
    "        a[i]=i;\n",
    "        b[i]=i*i;\n",
    "    }\n",
    "    //copiamos arreglos a, b a la GPU\n",
    "    cudaMemcpy(device_a,a,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(device_b,b,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    //mandamos a llamar a suma_vect:\n",
    "    suma_vect<<<dimGrid,dimBlock>>>(device_a,device_b,device_c); //N bloques de 1 thread\n",
    "    cudaDeviceSynchronize();\n",
    "    //copia del resultado al arreglo c:\n",
    "    cudaMemcpy(c,device_c,N*sizeof(int),cudaMemcpyDeviceToHost);\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",a[i],b[i],c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall suma_vectorial.cu -o suma_vectorial.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./suma_vectorial.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obsérvese que se están utilizando apuntadores en la línea:\n",
    "\n",
    "```\n",
    "    int *device_a, *device_b, *device_c;\n",
    "```\n",
    "\n",
    "pero estos apuntadores no apuntan a una dirección de memoria en el *device* pues aunque NVIDIA añadió el *feature* de [Unified Memory](https://devblogs.nvidia.com/unified-memory-cuda-beginners/) (un espacio de memoria accesible para el *host* y el *device*) aquí no se está usando tal *feature*. Más bien se están utilizando los apuntadores anteriores para apuntar a un [struct](https://en.wikipedia.org/wiki/Struct_(C_programming_language)) de C en el que uno de sus tipos de datos es una dirección de memoria en el *device*.\n",
    "\n",
    "* Obsérvese el uso de `(void **)` por la definición de la función `cudaMalloc`.\n",
    "\n",
    "* Obsérvese que en el programa anterior se coloca en comentario que se asume que $N$ el número de datos en el arreglo es menor al número de bloques que es posible lanzar. Esto como veremos más adelante es importante considerar pues aunque en un *device* se pueden lanzar muchos bloques y muchos threads, se tienen límites en el número de éstos que es posible lanzar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Perfilamiento en CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al instalar el *CUDA toolkit* en sus máquinas o bien si utilizan el contenedor de docker (descrito al inicio de la nota) se instala la línea de comando [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) para perfilamiento. Se puede ejecutar con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==342== NVPROF is profiling process 342, command: ./suma_vectorial.out\n",
      "==342== Profiling application: ./suma_vectorial.out\n",
      "==342== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    36.80  1.47e-06         1  1.47e-06  1.47e-06  1.47e-06  suma_vect(int*, int*, int*)\n",
      "                    35.20  1.41e-06         2  7.04e-07  5.44e-07  8.64e-07  [CUDA memcpy HtoD]\n",
      "                    28.00  1.12e-06         1  1.12e-06  1.12e-06  1.12e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.55  0.087959         3  0.029320  4.69e-06  0.087948  cudaMalloc\n",
      "                     0.25  2.18e-04        97  2.25e-06  1.79e-07  1.36e-04  cuDeviceGetAttribute\n",
      "                     0.06  5.50e-05         3  1.83e-05  3.40e-06  4.55e-05  cudaFree\n",
      "                     0.05  4.70e-05         1  4.70e-05  4.70e-05  4.70e-05  cuDeviceTotalMem\n",
      "                     0.03  2.69e-05         3  8.96e-06  5.04e-06  1.11e-05  cudaMemcpy\n",
      "                     0.02  1.92e-05         1  1.92e-05  1.92e-05  1.92e-05  cudaLaunchKernel\n",
      "                     0.02  1.86e-05         1  1.86e-05  1.86e-05  1.86e-05  cuDeviceGetName\n",
      "                     0.00  4.01e-06         1  4.01e-06  4.01e-06  4.01e-06  cudaDeviceSynchronize\n",
      "                     0.00  2.45e-06         1  2.45e-06  2.45e-06  2.45e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.65e-06         3  5.49e-07  2.53e-07  1.03e-06  cuDeviceGetCount\n",
      "                     0.00  9.27e-07         2  4.63e-07  1.84e-07  7.43e-07  cuDeviceGet\n",
      "                     0.00  2.72e-07         1  2.72e-07  2.72e-07  2.72e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "nvprof --normalized-time-unit s ./suma_vectorial.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* Las unidades en las que se reporta son s: second, ms: millisecond, us: microsecond, ns: nanosecond.\n",
    "\n",
    "* En la documentación de NVIDIA se menciona que `nvprof` será reemplazada próximamente por [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) y [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior se lanzaron $N$ bloques con $1$ *thread* cada uno y a continuación se lanza $1$ bloque con $N$ *threads*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing suma_vectorial_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file suma_vectorial_2.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void suma_vect(int *a, int *b, int *c){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    if(thread_id_x<N) //aquí se asume que el valor de N \n",
    "                     //es menor al número máximo de threads que se pueden lanzar\n",
    "                    //si fuese mayor, hay que hacer un ajuste\n",
    "        c[thread_id_x] = a[thread_id_x]+b[thread_id_x];\n",
    "}\n",
    "int main(void){\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    dim3 dimGrid(1,1,1); //1 bloques en el grid\n",
    "    dim3 dimBlock(N,1,1); //N threads por bloque \n",
    "    //alojando en device con Unified Memory\n",
    "    cudaMallocManaged(&device_a, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_b, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_c, sizeof(int)*N);\n",
    "    //llenando los arreglos:\n",
    "    for(i=0;i<N;i++){\n",
    "        device_a[i]=i;\n",
    "        device_b[i]=i*i;\n",
    "    }\n",
    "    suma_vect<<<dimGrid,dimBlock>>>(device_a,device_b,device_c); //1 bloque con N threads\n",
    "    cudaDeviceSynchronize();\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",device_a[i],device_b[i],device_c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall suma_vectorial_2.cu -o suma_vectorial_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==389== NVPROF is profiling process 389, command: ./suma_vectorial_2.out\n",
      "==389== Profiling application: ./suma_vectorial_2.out\n",
      "==389== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  2.05e-06         1  2.05e-06  2.05e-06  2.05e-06  suma_vect(int*, int*, int*)\n",
      "      API calls:    99.40  0.151464         3  0.050488  1.09e-05  0.151432  cudaMallocManaged\n",
      "                     0.27  4.08e-04         1  4.08e-04  4.08e-04  4.08e-04  cudaLaunchKernel\n",
      "                     0.19  2.82e-04        97  2.91e-06  3.09e-07  1.61e-04  cuDeviceGetAttribute\n",
      "                     0.06  9.38e-05         3  3.13e-05  1.15e-05  5.90e-05  cudaFree\n",
      "                     0.05  7.66e-05         1  7.66e-05  7.66e-05  7.66e-05  cuDeviceTotalMem\n",
      "                     0.02  2.95e-05         1  2.95e-05  2.95e-05  2.95e-05  cuDeviceGetName\n",
      "                     0.01  1.31e-05         1  1.31e-05  1.31e-05  1.31e-05  cudaDeviceSynchronize\n",
      "                     0.00  2.76e-06         3  9.18e-07  3.62e-07  1.29e-06  cuDeviceGetCount\n",
      "                     0.00  1.91e-06         1  1.91e-06  1.91e-06  1.91e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.46e-06         2  7.30e-07  4.97e-07  9.63e-07  cuDeviceGet\n",
      "                     0.00  4.90e-07         1  4.90e-07  4.90e-07  4.90e-07  cuDeviceGetUuid\n",
      "\n",
      "==389== Unified Memory profiling result:\n",
      "Device \"GeForce GTX 750 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       1  8.0000KB  8.0000KB  8.0000KB  8.000000KB  1.6960e-06s  Host To Device\n",
      "       5  25.600KB  4.0000KB  60.000KB  128.0000KB  1.2960e-05s  Device To Host\n",
      "Total CPU Page faults: 2\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "nvprof --normalized-time-unit s ./suma_vectorial_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* El programa anterior utiliza la [Unified Memory](https://devblogs.nvidia.com/unified-memory-cuda-beginners/) con la función [cudaMallocManaged](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e). La *Unified Memory* es un *feature* que se añadió a CUDA desde las arquitecturas de **Kepler** y **Maxwell** pero que ha ido mejorando (por ejemplo añadiendo [page faulting](https://en.wikipedia.org/wiki/Page_fault) and [migration](https://www.kernel.org/doc/html/latest/vm/page_migration.html)) en las arquitecturas siguientes a la de *Kepler*: la arquitectura Pascal y Volta. Por esto en el *output* anterior de *nvprof* aparece una sección de *page fault*. \n",
    "\n",
    "* Obsérvese que en el programa anterior se comenta que se asume que $N$ el número de datos en el arreglo es menor al número de *threads* que es posible lanzar. Esto como veremos más adelante es importante considerar pues aunque en el *device* se pueden lanzar muchos bloques y muchos threads, se tienen límites en el número de éstos que es posible lanzar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Tenemos que inicializar los datos en la CPU y copiarlos hacia la GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad no tenemos que realizarlo para el ejemplo de `suma_vectorial.cu` anterior. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing suma_vectorial_3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file suma_vectorial_3.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void init(int *a, int *b){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    a[thread_id_x]=thread_id_x;\n",
    "    b[thread_id_x]=thread_id_x*thread_id_x;\n",
    "}\n",
    "\n",
    "__global__ void suma_vect(int *a, int *b, int *c){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    if(thread_id_x<N) //aquí se asume que el valor de N \n",
    "                     //es menor al número máximo de threads que se pueden lanzar\n",
    "                    //si fuese mayor, hay que hacer un ajuste\n",
    "        c[thread_id_x] = a[thread_id_x]+b[thread_id_x];\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    dim3 dimGrid(1,1,1); //1 bloques en el grid\n",
    "    dim3 dimBlock(N,1,1); //N threads por bloque \n",
    "    //alojando en device con Unified Memory\n",
    "    cudaMallocManaged(&device_a, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_b, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_c, sizeof(int)*N);\n",
    "    //llenando los arreglos:\n",
    "    init<<<dimGrid,dimBlock>>>(device_a,device_b);\n",
    "    //llamando al kernel suma_vect\n",
    "    suma_vect<<<dimGrid,dimBlock>>>(device_a,device_b,device_c); //1 bloque con N threads\n",
    "    cudaDeviceSynchronize();\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",device_a[i],device_b[i],device_c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall suma_vectorial_3.cu -o suma_vectorial_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==436== NVPROF is profiling process 436, command: ./suma_vectorial_3.out\n",
      "==436== Profiling application: ./suma_vectorial_3.out\n",
      "==436== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    51.28  1.92e-06         1  1.92e-06  1.92e-06  1.92e-06  suma_vect(int*, int*, int*)\n",
      "                    48.72  1.82e-06         1  1.82e-06  1.82e-06  1.82e-06  init(int*, int*)\n",
      "      API calls:    99.37  0.149377         3  0.049792  1.17e-05  0.149330  cudaMallocManaged\n",
      "                     0.28  4.28e-04         2  2.14e-04  1.33e-05  4.15e-04  cudaLaunchKernel\n",
      "                     0.19  2.93e-04        97  3.02e-06  3.01e-07  1.60e-04  cuDeviceGetAttribute\n",
      "                     0.07  1.05e-04         3  3.50e-05  1.25e-05  6.47e-05  cudaFree\n",
      "                     0.05  7.66e-05         1  7.66e-05  7.66e-05  7.66e-05  cuDeviceTotalMem\n",
      "                     0.02  2.88e-05         1  2.88e-05  2.88e-05  2.88e-05  cuDeviceGetName\n",
      "                     0.01  1.25e-05         1  1.25e-05  1.25e-05  1.25e-05  cudaDeviceSynchronize\n",
      "                     0.00  1.99e-06         3  6.63e-07  3.55e-07  1.18e-06  cuDeviceGetCount\n",
      "                     0.00  1.96e-06         1  1.96e-06  1.96e-06  1.96e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.20e-06         2  6.00e-07  3.18e-07  8.83e-07  cuDeviceGet\n",
      "                     0.00  5.35e-07         1  5.35e-07  5.35e-07  5.35e-07  cuDeviceGetUuid\n",
      "\n",
      "==436== Unified Memory profiling result:\n",
      "Device \"GeForce GTX 750 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       3  21.333KB  4.0000KB  52.000KB  64.00000KB  6.8480e-06s  Device To Host\n",
      "Total CPU Page faults: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./suma_vectorial_3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de una GPU y límites en número de threads y bloques que podemos lanzar en el kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *device* está compuesto por arreglos de **streaming multiprocessors SM's** (también denotados como MP's) y en cada *SM* encontramos un número (determinado por la arquitectura del device) de **streaming processors SP's** que comparten el caché y unidades de control (que están dentro de cada SM):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/oxx55upoayfmliw/SMS_CUDA.png?dl=0\" heigth=\"700\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dibujo anterior se muestran las SM's en color rojo y los SP's en morado. Hay dos SM's por cada bloque anaranjado y ocho SP's por cada SM. Así, una GPU es una máquina multicore. Aunque cada SM ejecuta las instrucciones de forma independiente a otra SM, comparten la **memoria global**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los bloques de threads son **asignados a cada SM por el CUDA runtime system**, puede asignar más de un bloque a una SM pero hay un límite de bloques que pueden ser asignados a cada SM. Ver [maximum number of blocks per multiprocessor](https://stackoverflow.com/questions/22520209/programmatically-retrieve-maximum-number-of-blocks-per-multiprocessor).\n",
    "\n",
    "**Comentarios:** \n",
    "\n",
    "* Por ejemplo para el modelo *GT200* el máximo número de bloques que podían asignarse a cada SM eran de $8$ bloques. Tal modelo tenía $30$ SM's lo que resultaban en $240$ bloques que en un instante podían asignarse al device para su ejecución simultánea (asignándose en cualquier orden en alguna SM disponible). Por supuesto que un grid podía contener más de $240$ bloques en este modelo y en este caso el *CUDA runtime system* lleva una lista de bloques que va asignando a cada SM y conforme cada SM terminan la ejecución, nuevos bloques son asignados a tales SM que finalizaron. Para visualizar esta situación, considérese una simplificación de lo anterior en donde se tiene un *device* con $2$ SM's y con un kernel se han lanzado $6$ bloques. El *CUDA runtime system* ha asignado $3$ bloques a cada SM, entonces se tiene un dibujo como el siguiente:\n",
    "\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/p0nu72ofmdjtck8/kernel_launch_example.png?dl=0\" heigth=\"600\" width=\"600\">\n",
    "\n",
    "\n",
    "* Los bloques asignados a una SM comparten recursos (por ejemplo memoria) y su ejecución es independiente entre ellos, no es posible sincronizar al \"bloque 1\" con el \"bloque 0\". También no es posible sincronizar a los *threads* de diferentes SM's pero sí es posible sincronizar a los *threads* dentro de un mismo bloque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué otros límites puedo encontrar en mi(s) device(s) de mi sistema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder lo anterior se puede utilizar el siguiente programa que está basado en [liga](https://devblogs.nvidia.com/how-query-device-properties-and-handle-errors-cuda-cc/) y [cudaDeviceProp Struct Reference](https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing device_properties.cu\n"
     ]
    }
   ],
   "source": [
    "%%file device_properties.cu\n",
    "\n",
    "#include<stdio.h>\n",
    "\n",
    "int main(void){\n",
    "    cudaDeviceProp properties;\n",
    "    int count;\n",
    "    int i;\n",
    "    cudaGetDeviceCount(&count);\n",
    "    for(i=0;i<count;i++){\n",
    "        printf(\"----------------------\\n\");\n",
    "        cudaGetDeviceProperties(&properties, i);\n",
    "        printf(\"----device %d ----\\n\",i); \n",
    "        printf(\"Device Name: %s\\n\", properties.name);\n",
    "        printf(\"Compute capability: %d.%d\\n\", properties.major, properties.minor);\n",
    "        printf(\"Clock rate: %d\\n\", properties.clockRate);\n",
    "        printf(\"Unified memory: %d\\n\", properties.unifiedAddressing);\n",
    "        printf(\" ---Memory Information for device %d (results on bytes)---\\n\", i);\n",
    "        printf(\"Total global mem: %ld\\n\", properties.totalGlobalMem); \n",
    "        printf(\"Total constant Mem: %ld\\n\", properties.totalConstMem);\n",
    "        printf(\"Shared memory per thread block: %ld\\n\", properties.sharedMemPerBlock);\n",
    "        printf(\"Shared memory per SM: %ld\\n\",properties.sharedMemPerMultiprocessor );\n",
    "        printf(\" ---MP Information for device %d ---\\n\", i);\n",
    "        printf(\"SM count: %d\\n\", properties.multiProcessorCount);\n",
    "        printf(\"Threads in warp: %d\\n\", properties.warpSize);\n",
    "        printf(\"Max threads per SM: %d\\n\", properties.maxThreadsPerMultiProcessor);\n",
    "        printf(\"Max warps per SM: %d\\n\",properties.maxThreadsPerMultiProcessor/properties.warpSize);\n",
    "        printf(\"Max threads per block: %d\\n\", properties.maxThreadsPerBlock);\n",
    "        printf(\"Max thread dimensions: (%d, %d, %d)\\n\", properties.maxThreadsDim[0], properties.maxThreadsDim[1], properties.maxThreadsDim[2]);\n",
    "        printf(\"Max grid dimensions: (%d, %d, %d)\\n\", properties.maxGridSize[0], properties.maxGridSize[1], properties.maxGridSize[2]); \n",
    "    }\n",
    "    return 0;\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nvcc --compiler-options -Wall device_properties.cu -o device_properties.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "----device 0 ----\n",
      "Device Name: Tesla K20Xm\n",
      "Compute capability: 3.5\n",
      "Clock rate: 732000\n",
      "Unified memory: 1\n",
      " ---Memory Information for device 0 (results on bytes)---\n",
      "Total global mem: 5977800704\n",
      "Total constant Mem: 65536\n",
      "Shared memory per thread block: 49152\n",
      "Shared memory per SM: 49152\n",
      " ---MP Information for device 0 ---\n",
      "SM count: 14\n",
      "Threads in warp: 32\n",
      "Max threads per SM: 2048\n",
      "Max warps per SM: 64\n",
      "Max threads per block: 1024\n",
      "Max thread dimensions: (1024, 1024, 64)\n",
      "Max grid dimensions: (2147483647, 65535, 65535)\n",
      "----------------------\n",
      "----device 1 ----\n",
      "Device Name: Tesla K20Xm\n",
      "Compute capability: 3.5\n",
      "Clock rate: 732000\n",
      "Unified memory: 1\n",
      " ---Memory Information for device 1 (results on bytes)---\n",
      "Total global mem: 5977800704\n",
      "Total constant Mem: 65536\n",
      "Shared memory per thread block: 49152\n",
      "Shared memory per SM: 49152\n",
      " ---MP Information for device 1 ---\n",
      "SM count: 14\n",
      "Threads in warp: 32\n",
      "Max threads per SM: 2048\n",
      "Max warps per SM: 64\n",
      "Max threads per block: 1024\n",
      "Max thread dimensions: (1024, 1024, 64)\n",
      "Max grid dimensions: (2147483647, 65535, 65535)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./device_properties.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* También en la documentación oficial de NVIDIA dentro de [compute-capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) se pueden revisar los valores anteriores y muchos más.\n",
    "\n",
    "* Obsérvese del *output* anterior que el sistema tiene dos *devices* con las mismas capacidades.\n",
    "\n",
    "* En un *device* encontramos diferentes tipos de memoria: global, constante, *shared* y *texture*. En esta nota únicamente trabajamos con la memoria global. Tenemos funciones en CUDA para poder comunicar/coordinar a los *threads* en un bloque por medio de la *shared memory*. Ver por ejemplo [Using Shared Memory in CUDA C/C++](https://devblogs.nvidia.com/using-shared-memory-cuda-cc/) para un pequeño *post* del $2013$ sobre *shared memory*.\n",
    "\n",
    "* Los bloques de threads que son asignados a una SM son divididos en **warps** que es la unidad de **thread scheduling\\*** que tiene el *CUDA run time system*. El *output* anterior indica que son divisiones de $32$ threads: un warp consta de $32$ threads.\n",
    "\n",
    "\\* El *thread scheduling* se puede pensar a la funcionalidad del hardware para seleccionar una instrucción del programa y asginar su ejecución por los *threads* en un *warp* ([SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)). Otro ejemplo es tener una instrucción que indica que se debe realizar lectura o escritura, entonces el hardware del *device* utiliza un *warp* de threads para tal operación mientras selecciona un *warp* de threads distinto para seleccionar otra instrucción diferente a la de I/O.\n",
    "\n",
    "* El número máximo de threads que pueden iniciarse de forma simultánea o en un instante por SM es de $2048$ o bien $2048/32 = 64$ warps.\n",
    "\n",
    "* El *output* anterior muestra los límites para número de bloques en las tres dimensiones de un grid y el número de threads en las tres dimensiones en un bloque.\n",
    "\n",
    "* Un bloque puede tener como máximo $1024$ threads en cualquier configuración: por ejemplo $(1024,1,1), (32,1,32), (4,4,64)$.\n",
    "\n",
    "* Por los puntos anteriores si lanzamos bloques de $1024$ threads entonces sólo $2$ bloques pueden residir en una SM en un instante. Con esta configuración alcanzaríamos $1024/32=32$ warps por cada bloque y como lanzamos $2$ bloques alcanzaríamos $64$ warps (que es el máximo de warps por SM que podemos tener en un instante). Otra configuración para alcanzar el máximo número de warps en un instante, es considerar $4$ bloques de $512$ threads pues tendríamos $512/32=16$ warps por bloque y en total serían $16*4$ (warps $\\times$ bloques) $=64$ warps. Entre los datos que hay que elegir en los programas de CUDA C se tienen las configuraciones en el número de threads y el número de bloques a lanzar. La idea es alcanzar o rebasar el máximo número de warps en cada SM que soporta nuestro device en un instante.\n",
    "\n",
    "* Por ejemplo para el dibujo en el que se asumió que el *CUDA runtime system* había asignado $3$ bloques a cada SM, se tendría una división de cada bloque en un *warp* de $32$ threads como sigue:\n",
    "\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/yngq4r66i2nk5mg/warp_division.png?dl=0\" heigth=\"600\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Configuration Choices?\n",
    "\n",
    "Los programas de CUDA C tienen la opción de elegir el número de *threads* y de *bloques* a ser lanzados. En la referencia \"Parallel Computing for Data Science. With Examples in R, C++ and CUDA\" de N. Matloff se enlistan algunas consideraciones para elegir tales parámetros:\n",
    "\n",
    "* Given that scheduling is done on a warp basis, block size should be a multiple of the warp size (32).\n",
    "\n",
    "* One wants to utilize all the SMs. If one sets the block size too large, not all will be used, as a block cannot be split across SM's.\n",
    "\n",
    "* ..., barrier synchronization can be done effectively only at the block level. The larger the block, the more the barrier delay, so one might want smaller blocks.\n",
    "\n",
    "* On the other hand, if one is using shared memory, this can only be done at the block level, and efficient use may indicate using a larger block.\n",
    "\n",
    "* Two threads doing unrelated work, or the same work but with many if/elses, would cause a lot of thread divergence if they were in the same block. In some cases, it may be known in advance which threads will do the \"ifs\" and which will do the \"elses\", in which case they should be placed in different blocks if possible.\n",
    "\n",
    "* A commonly-cited rule of thumb is to have between $128$ and $256$ *threads* per block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla compuesta del rectángulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el uso de CUDA se recomienda que:\n",
    "\n",
    "* Users escriban código de CUDA C simple.\n",
    "\n",
    "* Utilicen las librerías ya hechas por NVIDIA o terceros para mantener simplicidad y eficiencia en el código.\n",
    "\n",
    "Lo anterior para disminuir el tiempo y la cantidad de código que *users* tengan que hacer (o rehacer) y puesto que dominar la programación de CUDA C requiere una buena inversión de tiempo.\n",
    "\n",
    "Así, tenemos a [Thrust](https://docs.nvidia.com/cuda/thrust/index.html) una *template library* basada en la [Standard Template Library (STL)](https://en.wikipedia.org/wiki/Standard_Template_Library) de C++ construída por NVIDIA que de acuerdo a su documentación: \n",
    "\n",
    "\"Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically. As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.\" \n",
    "\n",
    "\n",
    "`Thrust` tiene la opción de utilizarse con [OpenMP](https://www.openmp.org/), [Thread Building Blocks (TBB)](https://www.threadingbuildingblocks.org/intel-tbb-tutorial) y con CUDA-C++. Ver por ejemplo [Device Backends](https://github.com/thrust/thrust/wiki/Device-Backends) para conocer cómo cambiar entre OpenMP y CUDA-C++, lo cual se realiza en la compilación y **sin hacer cambios en el código!**. A los sistemas de software que tienen este *feature* se les llama [Heterogeneous computing](https://en.wikipedia.org/wiki/Heterogeneous_computing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se instala el *CUDA toolkit* o se utiliza la imagen de docker descrita al inicio de la nota, los headers en la librería template de `Thrust` estarán disponibles para su uso.\n",
    "\n",
    "En el siguiente ejemplo de la regla del rectángulo compuesta se utiliza:\n",
    "\n",
    " * [Reductions](https://docs.nvidia.com/cuda/thrust/index.html#reductions)\n",
    " \n",
    " * Los headers: \n",
    " \n",
    "    * [thrust/execution_policy](https://thrust.github.io/doc/structthrust_1_1device__execution__policy.html)\n",
    "    \n",
    "    * [thhrust/reduce()](https://thrust.github.io/doc/group__reductions_ga43eea9a000f912716189687306884fc7.html#ga43eea9a000f912716189687306884fc7)\n",
    " \n",
    "\n",
    "y se hace explícito el uso de la política de ejecucion [thrust::device](https://thrust.github.io/doc/group__execution__policies_ga78249cb3aa4239b64e65aaf6e82ac2f8.html).\n",
    "\n",
    "Referencias para el programa siguiente se encuentran en [thrust inside user written kernels](https://stackoverflow.com/questions/5510715/thrust-inside-user-written-kernels) y [cuda how to sum all elements of an array into one number within the gpu](https://stackoverflow.com/questions/42525713/cuda-how-to-sum-all-elements-of-an-array-into-one-number-within-the-gpu) de stackoverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Los siguientes tiempos se calcularon con una GPU de las siguientes características:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "----device 0 ----\n",
      "Device Name: GeForce GTX 750\n",
      "Compute capability: 5.0\n",
      "Clock rate: 1293500\n",
      "Unified memory: 1\n",
      " ---Memory Information for device 0 (results on bytes)---\n",
      "Total global mem: 1025769472\n",
      "Total constant Mem: 65536\n",
      "Shared memory per thread block: 49152\n",
      "Shared memory per SM: 65536\n",
      " ---MP Information for device 0 ---\n",
      "SM count: 4\n",
      "Threads in warp: 32\n",
      "Max threads per SM: 2048\n",
      "Max warps per SM: 64\n",
      "Max threads per block: 1024\n",
      "Max thread dimensions: (1024, 1024, 64)\n",
      "Max grid dimensions: (2147483647, 65535, 65535)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "./device_properties.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$n=10^3$** subintervalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    if(threadIdx.x<=n-1){\n",
    "        x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "        data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n=1e3; //número de subintervalos\n",
    "    double objetivo=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<1,n>>>(d_data, a,h_hat,n,d_suma); //1 bloque de n threads\n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf.cu -o Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
      "Error relativo de la solución: 4.104931878976858e-08\n",
      "Tiempo de cálculo en la gpu 0.00009\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
      "Error relativo de la solución: 4.104931878976858e-08\n",
      "Tiempo de cálculo en la gpu 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==525== NVPROF is profiling process 525, command: ./Rcf.out\n",
      "==525== Profiling application: ./Rcf.out\n",
      "==525== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    98.00  6.89e-05         1  6.89e-05  6.89e-05  6.89e-05  Rcf(double*, double, double, int, double*)\n",
      "                     2.00  1.41e-06         1  1.41e-06  1.41e-06  1.41e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.49  0.088044         2  0.044022  6.13e-06  0.088038  cudaMalloc\n",
      "                     0.21  1.83e-04        97  1.89e-06  3.26e-07  7.10e-05  cuDeviceGetAttribute\n",
      "                     0.09  7.86e-05         1  7.86e-05  7.86e-05  7.86e-05  cuDeviceTotalMem\n",
      "                     0.08  7.15e-05         1  7.15e-05  7.15e-05  7.15e-05  cudaDeviceSynchronize\n",
      "                     0.06  5.31e-05         2  2.65e-05  6.30e-06  4.68e-05  cudaFree\n",
      "                     0.03  2.80e-05         1  2.80e-05  2.80e-05  2.80e-05  cuDeviceGetName\n",
      "                     0.02  1.97e-05         1  1.97e-05  1.97e-05  1.97e-05  cudaLaunchKernel\n",
      "                     0.02  1.44e-05         1  1.44e-05  1.44e-05  1.44e-05  cudaMemcpy\n",
      "                     0.00  2.11e-06         3  7.03e-07  3.96e-07  1.29e-06  cuDeviceGetCount\n",
      "                     0.00  2.10e-06         1  2.10e-06  2.10e-06  2.10e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.23e-06         2  6.17e-07  3.24e-07  9.10e-07  cuDeviceGet\n",
      "                     0.00  5.00e-07         1  5.00e-07  5.00e-07  5.00e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./Rcf.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=1025$ subintervalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf2.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    if(threadIdx.x<=n-1){\n",
    "        x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "        data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n=1025; //número de subintervalos\n",
    "    double objetivo=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<1,n>>>(d_data, a,h_hat,n,d_suma); //1 bloque de n threads\n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf2.cu -o Rcf2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 0.000000000000000e+00\n",
      "Error relativo de la solución: 1.000000000000000e+00\n",
      "Tiempo de cálculo en la gpu 0.00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obsérvese error relativo de $100\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo lo arreglamos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf3.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum) {\n",
    "    double x=0.0;\n",
    "    int stride=0;\n",
    "    if(threadIdx.x<=n-1){\n",
    "        x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "        data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        stride=blockDim.x;\n",
    "        x=a+(threadIdx.x+stride+1/2.0)*h_hat;\n",
    "        data[threadIdx.x+stride]=std::exp(-std::pow(x,2));\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_bloques=2;\n",
    "    int n=1025;//número de subintervalos\n",
    "    double objetivo=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<n_bloques,n_threads_per_block>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf3.cu -o Rcf3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241619918411e-01\n",
      "Error relativo de la solución: 3.907133247860604e-08\n",
      "Tiempo de cálculo en la gpu 0.00009\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero en la propuesta anterior lanzamos $2*1024$ (bloques $\\times$ número de *threads*) $=2048$ *threads* y sólo ocupamos $1025$ threads. Entonces podemos cambiar el código anterior para aprovechar los $2048$ *threads* como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf4.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf4.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum) {\n",
    "    double x=0.0;\n",
    "    int stride=0;\n",
    "    int i;\n",
    "    stride=blockDim.x;\n",
    "    for(i=threadIdx.x;i<=n-1;i+=stride){\n",
    "        if(i<=n-1){\n",
    "            x=a+(i+1/2.0)*h_hat;\n",
    "            data[i]=std::exp(-std::pow(x,2));\n",
    "        }\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_bloques=2;\n",
    "    int n=n_threads_per_block*n_bloques;//número de subintervalos\n",
    "    double objetivo=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<n_bloques,n_threads_per_block>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf4.cu -o Rcf4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241401215338e-01\n",
      "Error relativo de la solución: 9.786918140590463e-09\n",
      "Tiempo de cálculo en la gpu 0.00016\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf4.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero todavía podemos hacer aún mejor sin el *for*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf5.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf5.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(idx==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_bloques=2;\n",
    "    double objetivo=0.7468241328124271;\n",
    "    int n=n_bloques*n_threads_per_block;//número de subintervalos\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<n_bloques,n_threads_per_block>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf5.cu -o Rcf5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241401215338e-01\n",
      "Error relativo de la solución: 9.786918140590463e-09\n",
      "Tiempo de cálculo en la gpu 0.00014\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241401215338e-01\n",
      "Error relativo de la solución: 9.786918140590463e-09\n",
      "Tiempo de cálculo en la gpu 0.00014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==694== NVPROF is profiling process 694, command: ./Rcf5.out\n",
      "==694== Profiling application: ./Rcf5.out\n",
      "==694== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    98.76  1.17e-04         1  1.17e-04  1.17e-04  1.17e-04  Rcf(double*, double, double, int, double*)\n",
      "                     1.24  1.47e-06         1  1.47e-06  1.47e-06  1.47e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.36  0.093178         2  0.046589  4.18e-06  0.093174  cudaMalloc\n",
      "                     0.31  2.88e-04        97  2.96e-06  2.85e-07  1.57e-04  cuDeviceGetAttribute\n",
      "                     0.13  1.20e-04         1  1.20e-04  1.20e-04  1.20e-04  cudaDeviceSynchronize\n",
      "                     0.08  7.32e-05         1  7.32e-05  7.32e-05  7.32e-05  cuDeviceTotalMem\n",
      "                     0.06  5.18e-05         2  2.59e-05  5.26e-06  4.66e-05  cudaFree\n",
      "                     0.03  2.69e-05         1  2.69e-05  2.69e-05  2.69e-05  cuDeviceGetName\n",
      "                     0.02  1.98e-05         1  1.98e-05  1.98e-05  1.98e-05  cudaLaunchKernel\n",
      "                     0.02  1.45e-05         1  1.45e-05  1.45e-05  1.45e-05  cudaMemcpy\n",
      "                     0.00  2.12e-06         3  7.07e-07  3.85e-07  1.34e-06  cuDeviceGetCount\n",
      "                     0.00  1.82e-06         1  1.82e-06  1.82e-06  1.82e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  9.84e-07         2  4.92e-07  3.02e-07  6.82e-07  cuDeviceGet\n",
      "                     0.00  4.23e-07         1  4.23e-07  4.23e-07  4.23e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./Rcf5.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una visualización sobre la construcción del índice en el kernel utilizando `blockDim.x*blockIdx.x + threadIdx.x` ver [An Even Easier Introduction to CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Más nodos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso, incrementamos el número de bloques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Rcf6.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf6.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(idx==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_bloques=0; //inicializamos en 0\n",
    "    double objetivo=0.7468241328124271;\n",
    "    int n=0; //inicializamos en 0\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;\n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_bloques = 256 * properties.multiProcessorCount; //elegimos un múltiplo del número de SM's\n",
    "                                                    //y además que sea menor a 10^4 (para tener 10^6 aprox subintervalos)\n",
    "    n = n_bloques*n_threads_per_block;//número de subintervalos\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<n_bloques,n_threads_per_block>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf6.cu -o Rcf6.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241328124761e-01\n",
      "Error relativo de la solución: 6.555872157155699e-14\n",
      "Tiempo de cálculo en la gpu 0.06257\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf6.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241328124761e-01\n",
      "Error relativo de la solución: 6.555872157155699e-14\n",
      "Tiempo de cálculo en la gpu 0.06288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==963== NVPROF is profiling process 963, command: ./Rcf6.out\n",
      "==963== Profiling application: ./Rcf6.out\n",
      "==963== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  0.062851         1  0.062851  0.062851  0.062851  Rcf(double*, double, double, int, double*)\n",
      "                     0.00  1.57e-06         1  1.57e-06  1.57e-06  1.57e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    58.26  0.089315         2  0.044657  5.76e-05  0.089257  cudaMalloc\n",
      "                    41.00  0.062855         1  0.062855  0.062855  0.062855  cudaDeviceSynchronize\n",
      "                     0.41  6.22e-04         2  3.11e-04  5.54e-05  5.67e-04  cudaFree\n",
      "                     0.13  1.94e-04        97  2.00e-06  3.36e-07  7.47e-05  cuDeviceGetAttribute\n",
      "                     0.10  1.58e-04         1  1.58e-04  1.58e-04  1.58e-04  cudaGetDeviceProperties\n",
      "                     0.05  8.27e-05         1  8.27e-05  8.27e-05  8.27e-05  cuDeviceTotalMem\n",
      "                     0.02  2.92e-05         1  2.92e-05  2.92e-05  2.92e-05  cuDeviceGetName\n",
      "                     0.01  2.01e-05         1  2.01e-05  2.01e-05  2.01e-05  cudaLaunchKernel\n",
      "                     0.01  1.77e-05         1  1.77e-05  1.77e-05  1.77e-05  cudaMemcpy\n",
      "                     0.00  2.19e-06         1  2.19e-06  2.19e-06  2.19e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.99e-06         3  6.63e-07  3.77e-07  1.16e-06  cuDeviceGetCount\n",
      "                     0.00  1.33e-06         2  6.66e-07  3.45e-07  9.87e-07  cuDeviceGet\n",
      "                     0.00  5.68e-07         1  5.68e-07  5.68e-07  5.68e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./Rcf6.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=16777216 \\approx 10^7$ subintervalos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Rcf7.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf7.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(idx==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_bloques=0; //inicializamos en 0\n",
    "    double objetivo=0.7468241328124271;\n",
    "    int n=0; //inicializamos en 0\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;\n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_bloques = 4096 * properties.multiProcessorCount; //elegimos un múltiplo del número de SM's\n",
    "                                                    //y además que sea menor a 10^8 (para tener 10^7 aprox subintervalos)\n",
    "    n = n_bloques*n_threads_per_block;//número de subintervalos\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<n_bloques,n_threads_per_block>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf7.cu -o Rcf7.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241328123114e-01\n",
      "Error relativo de la solución: 1.549029203572843e-13\n",
      "Tiempo de cálculo en la gpu 0.94370\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf7.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241328123114e-01\n",
      "Error relativo de la solución: 1.549029203572843e-13\n",
      "Tiempo de cálculo en la gpu 0.94049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1118== NVPROF is profiling process 1118, command: ./Rcf7.out\n",
      "==1118== Profiling application: ./Rcf7.out\n",
      "==1118== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  0.939793         1  0.939793  0.939793  0.939793  Rcf(double*, double, double, int, double*)\n",
      "                     0.00  1.44e-06         1  1.44e-06  1.44e-06  1.44e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    90.36  0.939797         1  0.939797  0.939797  0.939797  cudaDeviceSynchronize\n",
      "                     8.80  0.091559         2  0.045780  6.07e-05  0.091499  cudaMalloc\n",
      "                     0.76  7.94e-03         2  3.97e-03  1.58e-04  7.78e-03  cudaFree\n",
      "                     0.03  3.02e-04        97  3.11e-06  3.18e-07  1.61e-04  cuDeviceGetAttribute\n",
      "                     0.03  2.61e-04         1  2.61e-04  2.61e-04  2.61e-04  cudaGetDeviceProperties\n",
      "                     0.01  8.03e-05         1  8.03e-05  8.03e-05  8.03e-05  cuDeviceTotalMem\n",
      "                     0.00  2.99e-05         1  2.99e-05  2.99e-05  2.99e-05  cuDeviceGetName\n",
      "                     0.00  2.02e-05         1  2.02e-05  2.02e-05  2.02e-05  cudaLaunchKernel\n",
      "                     0.00  1.82e-05         1  1.82e-05  1.82e-05  1.82e-05  cudaMemcpy\n",
      "                     0.00  2.19e-06         3  7.29e-07  4.02e-07  1.35e-06  cuDeviceGetCount\n",
      "                     0.00  2.10e-06         1  2.10e-06  2.10e-06  2.10e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  1.33e-06         2  6.65e-07  3.42e-07  9.89e-07  cuDeviceGet\n",
      "                     0.00  5.26e-07         1  5.26e-07  5.26e-07  5.26e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./Rcf7.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=117440512 \\approx 10^8$ subintervalos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Este caso se ejecutó en una GPU con las siguientes características:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "----------------------\n",
    "----device 0 ----\n",
    "Device Name: Tesla K20Xm\n",
    "Compute capability: 3.5\n",
    "Clock rate: 732000\n",
    "Unified memory: 1\n",
    " ---Memory Information for device 0 (results on bytes)---\n",
    "Total global mem: 5977800704\n",
    "Total constant Mem: 65536\n",
    "Shared memory per thread block: 49152\n",
    "Shared memory per SM: 49152\n",
    " ---MP Information for device 0 ---\n",
    "SM count: 14\n",
    "Threads in warp: 32\n",
    "Max threads per SM: 2048\n",
    "Max warps per SM: 64\n",
    "Max threads per block: 1024\n",
    "Max thread dimensions: (1024, 1024, 64)\n",
    "Max grid dimensions: (2147483647, 65535, 65535)\n",
    "----------------------\n",
    "----device 1 ----\n",
    "Device Name: Tesla K20Xm\n",
    "Compute capability: 3.5\n",
    "Clock rate: 732000\n",
    "Unified memory: 1\n",
    " ---Memory Information for device 1 (results on bytes)---\n",
    "Total global mem: 5977800704\n",
    "Total constant Mem: 65536\n",
    "Shared memory per thread block: 49152\n",
    "Shared memory per SM: 49152\n",
    " ---MP Information for device 1 ---\n",
    "SM count: 14\n",
    "Threads in warp: 32\n",
    "Max threads per SM: 2048\n",
    "Max warps per SM: 64\n",
    "Max threads per block: 1024\n",
    "Max thread dimensions: (1024, 1024, 64)\n",
    "Max grid dimensions: (2147483647, 65535, 65535)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf8.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf8.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, long int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    //alternativa si el número de bloques rebasa el límite:\n",
    "    //int num_threads=gridDim.x * blockDim.x;\n",
    "    //int stride = num_threads;\n",
    "    //for(i=idx; i<=n-1; i+=stride){\n",
    "    //    if(idx<=n-1){\n",
    "    //        x=a+(idx+1/2.0)*h_hat;\n",
    "    //        data[idx]=std::exp(-std::pow(x,2));\n",
    "    //    }\n",
    "    //}\n",
    "    \n",
    "    if(idx==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    long int n_bloques=0; //inicializamos en 0\n",
    "    double objetivo=0.7468241328124271;\n",
    "    long int n=0; //inicializamos en 0\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;    \n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_bloques = 8192 * properties.multiProcessorCount; //elegimos un múltiplo del número de SM's\n",
    "                                                    //y además que sea menor a 10^9 (para tener 10^8 aprox subintervalos)\n",
    "    n = n_bloques*n_threads_per_block;//número de subintervalos\n",
    "    dim3 dimGrid(n_bloques,1,1);\n",
    "    dim3 dimBlock(n_threads_per_block,1,1);\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<dimGrid,dimBlock>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf8.cu -o Rcf8.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241328125712e-01\n",
      "Error relativo de la solución: 1.929596838999568e-13\n",
      "Tiempo de cálculo en la gpu 12.95516\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf8.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241328125712e-01\n",
      "Error relativo de la solución: 1.929596838999568e-13\n",
      "Tiempo de cálculo en la gpu 13.00077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==173== NVPROF is profiling process 173, command: ./Rcf8.out\n",
      "==173== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
      "==173== Profiling application: ./Rcf8.out\n",
      "==173== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  12.99435         1  12.99435  12.99435  12.99435  Rcf(double*, double, double, long, double*)\n",
      "                     0.00  2.46e-06         1  2.46e-06  2.46e-06  2.46e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    97.08  12.99439         1  12.99439  12.99439  12.99439  cudaDeviceSynchronize\n",
      "                     2.21  0.296135         2  0.148068  1.54e-04  0.295981  cudaMalloc\n",
      "                     0.69  0.092310         2  0.046155  1.33e-03  0.090979  cudaFree\n",
      "                     0.00  5.54e-04       194  2.85e-06  2.41e-07  1.05e-04  cuDeviceGetAttribute\n",
      "                     0.00  5.52e-04         2  2.76e-04  2.57e-04  2.95e-04  cuDeviceTotalMem\n",
      "                     0.00  3.02e-04         1  3.02e-04  3.02e-04  3.02e-04  cudaLaunchKernel\n",
      "                     0.00  2.43e-04         1  2.43e-04  2.43e-04  2.43e-04  cudaGetDeviceProperties\n",
      "                     0.00  1.02e-04         2  5.10e-05  2.03e-05  8.17e-05  cuDeviceGetName\n",
      "                     0.00  8.62e-05         1  8.62e-05  8.62e-05  8.62e-05  cudaMemcpy\n",
      "                     0.00  6.53e-06         2  3.27e-06  2.73e-06  3.80e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  2.42e-06         4  6.06e-07  2.69e-07  1.19e-06  cuDeviceGet\n",
      "                     0.00  1.94e-06         3  6.48e-07  3.04e-07  1.05e-06  cuDeviceGetCount\n",
      "                     0.00  8.75e-07         2  4.37e-07  3.71e-07  5.04e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof --normalized-time-unit s ./Rcf8.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** en la programación con CUDA-C es importante checar posibles errores de alojamiento de memoria. Una forma es con los tipos [cudaError_t](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6) y `cudaSuccess` . Ver [liga](https://stackoverflow.com/questions/58902166/why-do-i-have-insufficient-buffer-space-when-i-put-allocation-code-in-a-functi):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf9.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf9.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, long int n, double *sum ) {\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    int num_threads=gridDim.x * blockDim.x;\n",
    "    int stride = num_threads;\n",
    "    int i;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    for(i=idx; i<=n-1; i+=stride){\n",
    "        if(idx<=n-1){\n",
    "            x=a+(idx+1/2.0)*h_hat;\n",
    "            data[idx]=std::exp(-std::pow(x,2));\n",
    "        }\n",
    "}\n",
    "    \n",
    "    if(idx==0){\n",
    "        *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "cudaError_t fun_checa_error(cudaError_t result) {\n",
    "    if (result != cudaSuccess) {\n",
    "        fprintf(stderr, \"Error: %s\\n\", cudaGetErrorString(result));\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    long int n_bloques=0; //inicializamos en 0\n",
    "    double objetivo=0.7468241328124271;\n",
    "    long int n=0; //inicializamos en 0\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;    \n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_bloques = 32768 * properties.multiProcessorCount; //elegimos un múltiplo del número de SM's\n",
    "                                                    //y además que sea menor a 10^9 (para tener 10^8 aprox subintervalos)\n",
    "    n = n_bloques*n_threads_per_block;//número de subintervalos\n",
    "    dim3 dimGrid(n_bloques,1,1);\n",
    "    dim3 dimBlock(n_threads_per_block,1,1);\n",
    "    fun_checa_error(cudaMalloc((void **)&d_data,sizeof(double)*n));\n",
    "    fun_checa_error(cudaMalloc((void**)&d_suma,sizeof(double)));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<dimGrid,dimBlock>>>(d_data, a,h_hat,n,d_suma); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    fun_checa_error(cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost));\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_suma);\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall Rcf9.cu -o Rcf9.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 0.000000000000000e+00\n",
      "Error relativo de la solución: 1.000000000000000e+00\n",
      "Tiempo de cálculo en la gpu 0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: out of memory\n",
      "Error: an illegal memory access was encountered\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf9.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión:**\n",
    "\n",
    "1)¿Qué es y en qué consiste CUDA C?\n",
    "    \n",
    "2)¿Qué es un kernel?\n",
    "\n",
    "3)¿Qué pieza de CUDA se encarga de asignar los bloques de cuda-threads a las SM’s?\n",
    "\n",
    "4)¿Qué características tienen los bloques que se asignan a una SM en al lanzarse y ejecutarse un kernel?\n",
    "\n",
    "5)¿Qué es un warp?\n",
    "\n",
    "6)Menciona los tipos de memorias que existen en las GPU’s.\n",
    "\n",
    "7) Supón que tienes una tarjeta GT200 cuyas características son:\n",
    "\n",
    "    * Máximo número de threads que soporta una SM en un mismo instante en el tiempo: 1024\n",
    "    * Máximo número de threads en un bloque: 512\n",
    "    * Máximo número de bloques por SM: 8\n",
    "    * Número de SM’s que tiene esta GPU: 30\n",
    "\n",
    "Responde:\n",
    "\n",
    "    a) ¿Cuál es la máxima cantidad de threads que puede soportar esta GPU en un mismo instante en el tiempo?\n",
    "    b) ¿Cuál es la máxima cantidad de warps por SM que puede soportar esta GPU en un mismo instante en el tiempo?\n",
    "    c) ¿Cuáles configuraciones de bloques y threads siguientes aprovechan la máxima cantidad de warps en una SM de esta GPU para un mismo instante en el tiempo?\n",
    "    \n",
    "        1.Una configuración del tipo: bloques de 64 threads y 16 bloques.\n",
    "        2.Una configuración del tipo: bloques de 1024 threads y 1 bloque.\n",
    "        3.Una configuración del tipo: bloques de 256 threads y 4 bloques.\n",
    "        4.Una configuración del tipo: bloques de 512 threads y 8 bloques.\n",
    "\n",
    "\\*Debes considerar las restricciones/características de la GPU dadas para responder pues algunas configuraciones infringen las mismas. No estamos considerando registers o shared memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "1. N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.\n",
    "\n",
    "2. [CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/C/extensiones_a_C/CUDA)\n",
    "\n",
    "3. [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb)\n",
    "\n",
    "Para más sobre *Unified Memory* revisar:\n",
    "\n",
    "* [Even easier introduction to cuda](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "\n",
    "* [Unified memory cuda beginners](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)\n",
    "\n",
    "Es importante el manejo de errores por ejemplo en el alojamiento de memoria en la GPU. En este caso es útil revisar:\n",
    "\n",
    "* [How to Query Device Properties and Handle Errors in CUDA C/C++](https://devblogs.nvidia.com/how-query-device-properties-and-handle-errors-cuda-cc/)\n",
    "\n",
    "En stackoverflow encontramos a personas desarrolladoras de CUDA que resuelven preguntas muy útiles para continuar con el aprendizaje de CUDA C. Por ejemplo: \n",
    "\n",
    "* [Parallel reduction over one axis](https://stackoverflow.com/questions/51526082/cuda-parallel-reduction-over-one-axis)\n",
    "\n",
    "Otros sistemas de software para el [Heterogeneous computing](https://en.wikipedia.org/wiki/Heterogeneous_computing) son:\n",
    "\n",
    "* [OpenCl](https://en.wikipedia.org/wiki/OpenCL). Ver [NVIDIA OpenCL SDK Code Samples](https://developer.nvidia.com/opencl) para ejemplos con NVIDIA GPU's.\n",
    "\n",
    "* [Rth](https://github.com/matloff/Rth). Ver también [rdrr.io matloff/Rth](https://rdrr.io/github/matloff/Rth/f/README.md).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
