{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas para contenedor de docker:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker.\n",
    "\n",
    "```\n",
    "docker run --gpus all --rm -v $(pwd):/datos --name jupyterlab_nvidia_cuda_c_container -p 8888:8888 -d palmoreck/jupyterlab_nvidia_cuda_c:1.1.0\n",
    "```\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "```\n",
    "docker stop jupyterlab_nvidia_cuda_c_container\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación de la imagen de docker `palmoreck/jupyterlab_nvidia_cuda_c:1.1.0` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/nvidia/cuda_c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: si se desean ejecutar los ejemplos que se presentan a continuación, es necesario tener una tarjeta gráfica NVIDIA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA C y generalidades de CUDA y GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en extensiones al lenguaje C y en una *runtime library*. Ver [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb) para más información.\n",
    "\n",
    "### Kernel\n",
    "\n",
    "* En CUDA C se define una función que se ejecuta en el device y que se le nombra **kernel**. El *kernel* inicia con la sintaxis:\n",
    "\n",
    "```\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "y siempre es tipo `void` (no hay `return`).\n",
    "\n",
    "* El llamado al *kernel* se realiza desde el host y con una sintaxis en la que se define el número de threads y bloques que serán utilizados para la ejecución del kernel. La sintaxis que se utiliza es con `<<< >>>` y en la primera entrada se coloca el número de bloques y en la segunda entrada el número de *threads*:\n",
    "\n",
    "\n",
    "```\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    int par;\n",
    "    mifun<<<N,5>>> (par); //N bloques de 5 threads\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Programa de hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "}\n",
    "int main(void){\n",
    "    func<<<1,1>>>(); //1 bloque de 1 thread\n",
    "    printf(\"Hello world!\\n\");\n",
    "return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc --compiler-options -Wall hello_world.cu -o hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La función `main` se ejecuta en la CPU.\n",
    "\n",
    "* La función `func` es un *kernel* y es ejecutada por *threads* en el *device* (GPU), también llamados *CUDA threads*. Obsérvese que la función `func` inicia con `__global__` para diferenciarla de `main`.\n",
    "\n",
    "* El *kernel* sólo puede tener un `return` tipo *void*: `__global__ void func` por lo que el *kernel* debe regresar sus resultados a través de sus argumentos.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Programa de hello world 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_2.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    func<<<2,3>>>(); //2 bloques de 3 threads cada uno\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall hello_world_2.cu -o hello_world_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world del bloque 1 del thread 0!\n",
      "Hello world del bloque 1 del thread 1!\n",
      "Hello world del bloque 1 del thread 2!\n",
      "Hello world del bloque 0 del thread 0!\n",
      "Hello world del bloque 0 del thread 1!\n",
      "Hello world del bloque 0 del thread 2!\n",
      "Hola del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios:**\n",
    "\n",
    "* La extensión del archivo debe ser `.cu` aunque esto puede modificarse al compilar con `nvcc`: \n",
    "\n",
    "`$nvcc -x cu hello_world.c -o hello_world.out`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El llamado a la ejecución del kernel se realizó en el *host* y se lanzaron $2$ bloques (primera posición), cada uno con $3$ *threads*.\n",
    "\n",
    "* Se utiliza la función [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d) para que el *cpu-thread* espere la finalización de la ejecución del kernel.\n",
    "\n",
    "* Los *CUDA threads* son divididos en bloques y todos los bloques se encuentran en un *grid*. En el lanzamiento del *kernel*  se debe especificar al hardware cuántos bloques tendrá nuestro *grid* y cuántos *threads* estarán en cada bloque. Las variables `blockIdx` y `threadIdx` hacen referencia a los **id**'s que tienen los bloques y los threads. El *id* del bloque dentro del *grid* y el *id* del thread dentro del bloque. La parte `.x` de `blockIdx.x` y `threadIdx.x` refiere a la primera coordenada del bloque y del *thread* en el *grid* y en el bloque respectivamente. \n",
    "\n",
    "* La elección del número de bloques en un grid o el número de *threads* en un bloque no corresponde a alguna disposición del hardware, esto es, si se lanza un kernel con `<<< 1, 3 >>>` no implica que la GPU tenga un bloque o 3 *threads*.\n",
    "\n",
    "* En una GPU podemos definir un *grid* de bloques y un bloque de *threads* utilizando un tipo de dato `dim3` el cual también es parte de CUDA C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_3.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world del bloque %d del thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(2,1); //2 bloques en el grid\n",
    "    dim3 dimBlock(3,1,1); //3 threads por bloque\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hola del cpu thread\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "nvcc --compiler-options -Wall hello_world_3.cu -o hello_world_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world del bloque 1 del thread 0!\n",
      "Hello world del bloque 1 del thread 1!\n",
      "Hello world del bloque 1 del thread 2!\n",
      "Hello world del bloque 0 del thread 0!\n",
      "Hello world del bloque 0 del thread 1!\n",
      "Hello world del bloque 0 del thread 2!\n",
      "Hola del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Programa de suma vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file suma_vectorial.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void suma_vect(int *a, int *b, int *c){\n",
    "    int tid = blockIdx.x;\n",
    "    if(tid<N) //suposición N menor al número máximo de bloques que se pueden lanzar\n",
    "    c[tid] = a[tid]+b[tid];\n",
    "}\n",
    "int main(void){\n",
    "    int a[N], b[N],c[N];\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    //alojando en device\n",
    "    cudaMalloc((void **)&device_a, sizeof(int)*N);\n",
    "    cudaMalloc((void **)&device_b, sizeof(int)*N);\n",
    "    cudaMalloc((void **)&device_c, sizeof(int)*N);\n",
    "    //llenando los arreglos:\n",
    "    for(i=0;i<N;i++){\n",
    "        a[i]=i;\n",
    "        b[i]=i*i;\n",
    "    }\n",
    "    //copiamos arreglos a, b a la GPU\n",
    "    cudaMemcpy(device_a,a,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(device_b,b,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    //mandamos a llamar a suma_vect:\n",
    "    suma_vect<<<N,1>>>(device_a,device_b,device_c); //N bloques de 1 thread\n",
    "    cudaDeviceSynchronize();\n",
    "    //copia del resultado al arreglo c:\n",
    "    cudaMemcpy(c,device_c,N*sizeof(int),cudaMemcpyDeviceToHost);\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",a[i],b[i],c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nvcc suma_vectorial.cu -o suma_vectorial.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios respecto al programa de `suma_vectorial.cu` anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla compuesta del rectángulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcc --compiler-options -Wall Rcf.cu -o Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum ) {\n",
    "double x=0.0;\n",
    "\n",
    "if(threadIdx.x<=n-1){\n",
    "x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "}\n",
    "    *sum = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double suma=0.0;\n",
    "    double *d_data;\n",
    "    double *d_suma;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n=1e3; //número de subintervalos\n",
    "    double objetivo=0.7468241328124271;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_suma,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    Rcf<<<1,n>>>(d_data, a,h_hat,n,d_suma); //1 bloque de n threads\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(&suma, d_suma, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    suma=h_hat*suma;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_suma) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,suma);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(suma-objetivo)/fabs(objetivo));\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
    "\n",
    "Error relativo de la solución: 4.104931878976858e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "1. N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.\n",
    "\n",
    "2. [CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/C/extensiones_a_C/CUDA)\n",
    "\n",
    "3. [2.3.CUDA](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/II.computo_paralelo/2.3.CUDA.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
