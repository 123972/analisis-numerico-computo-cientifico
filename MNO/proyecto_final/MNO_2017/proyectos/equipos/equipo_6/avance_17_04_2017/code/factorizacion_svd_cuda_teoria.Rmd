---
title: 'SVD en GPU usando CUDA'
author: "Equipo_6 Adrian Vazquez, Ricardo Lastra"
date: "19 de Abril del 2017"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
####Antecedentes

Para entender un poco mas las referencias de __Singular Value Decomposition on GPU using CUDA__ y __Singular Value Decomposition and Least Squares Solutions__ sugeridas por Erick, primero analizamos la factorizacion SVD según notas __3.2.2.Factorizaciones_matriciales_SVD_Cholesky_QR.pdf__ de la clase del Lunes 17 de Abril, asi como otras referencias (ver referencias en README.md)

###SVD

Dominando la factorización tenemos lo siguiente:

La forma de $SVD$ es: $M=U\Sigma V^*$ 

Donde:
$U$ es una matriz unitaria $mxm$ (entonces $K=R$ las matrices unitarias son matrices ortogonales)

$\Sigma$ es una matriz diagonal $mxn$ con numeros reales no negativos en la diagonal

$V$ es una matriz unitaria $nxn$ sobre $K$

$V^*$ es la matriz unitaria transpuesta ortogonal $nxn$ de $V$

Las entradas diagonales $Sigma i$ de $\Sigma$ son conocidos como los valores singulares de $M$, los $Ui$ son los vectores singulares izquierdos de $M$, los $Vi$ son los vectores singulares derechos de $M$

Pero realmente, para que sirve $SDV$?

Para entender la factorizacion $SVD$ y su interpretación geometrica, revisamos la siguiente imagen: 


```{r fig.width=4, fig.height=4,echo=FALSE}
library(png)
library(grid)
img <- readPNG("C:/Users/FORANEA110/Desktop/METODOS_MULTIVARIADOS/EXAMEN/pregunta1/svd.png")
 grid.raster(img)
```

Entonces confirmamos que la interpetación geométrica de $SVD$ no es otra más que como se muestra en la imagen de arriba, vemos que multiplicar por la matriz $V^*$ ortogonal  significa una rotación de ejes, luego al multiplicar por la matriz diagonal $\Sigma$ lo que sucede es un redimensionamiento de los ejes canónicos y finalmente al multiplicar por $U$ se hace una rotación nuevamente pero de la elipse. Análogamente sucede similar para el tema de diagonalización.

Suponiendo que la matriz $A$ es centrada y $\Sigma(A)={\frac{1}{n}}A^TA$, entonces sea la descomposición SVD de $A=U \Sigma V^T$ tal que $U,V$ son matrices ortogonales y $\Sigma$ es una matriz casi diagonal salvo por el excedente de dimensiones y con diagonal  mayor que cero.

Entonces tenemos lo siguiente: 
$$\Sigma(A)={\frac{1}{n}}A^TA={\frac{1}{n}}(U \Sigma V^T)^T(U \Sigma V^T)\\
={\frac{1}{n}}V \Sigma^T (U^TU) \Sigma V^T\ \  \ \ (U^TU)=I\ \ por\ ortogonalidad\\
={\frac{1}{n}}V \Sigma^T  \Sigma V^T\\ 
entonces: \\ {\frac{1}{n}}A^TA={\frac{1}{n}}V \Sigma^T  \Sigma V^T\\
y\ V^TA^TAV=\Sigma^T \Sigma
$$
y dado que $\Sigma$ es una matriz de $mxn$ diagonal entonces $\Sigma^T \Sigma$ es una matriz diagonal de $nxn$ cuyas entradas diagonales son las mismas de $\Sigma$ pero al cuadrado.

Asi pues se ha encontrado una diagonalización de la matriz ${\frac{1}{n}}A^TA$ y los eigenvalores correspondientes son ${\frac{\lambda_{i}^2}{n}}=(\Sigma^T \Sigma)_{ii}$ y por tanto $(\Sigma_{ii}={\frac{\lambda_{i}}{\sqrt n}})$ entonces los valores de  la matriz $\Sigma$ de valores singulares de $A$  son las raices de los valores positivos de $\Sigma(A)$ notar que las columnas de $V$  son los eigenvectores de $\Sigma(A)$ puesto que $V^TA^TAV=\Sigma^T \Sigma$ entonces $A^TAV=\Sigma^T \Sigma V$.

Por tanto la relación de $SVD$ de $A$ y la diagonalización de su matriz de covarianzas es que $V$ es la matriz de eigenvectores de $\Sigma(A)$ y los eigenvalores de $\Sigma(A)$ son el cuadrado de los valores singulares de $A$. 

Entonces, despues de revisar las referencias y tener claro la factorización $SVD$, decimos que la base del algoritmo de Golub es la "Bidiagonalizacion" la cual es $B = Q^T AP$, y esto computacionalmente nos ayuda a encontrar  una solucion completa de un sistema lineal indeterminado, es decir, i.e. $rank(M)=m<n$.

Tambien indica en las referencias sobre CUDA que las transformaciones y las transformaciones inversas se hacen en "GPU" segun los algoritmos presentados, lo cual brinda la eficiencia del computo.

Asi pues la Diagonalizacion en GPU nos brinda un excelente performance para largas matrices, y segun la nota tambien funciona bien en las de mediano tamaño.




