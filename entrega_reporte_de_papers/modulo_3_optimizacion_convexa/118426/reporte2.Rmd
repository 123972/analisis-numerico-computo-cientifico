---
title: "Convex Optimization for Big Data"
author: "Rafael Larrazolo de la Cruz"
output: html_document
---

# Reporte 2

Este artículo trata de revisar los avances hechos en los algoritmos de optimización convexa para problemas de grandes datos (Big Data), los cuales buscan reducir los llamados *cuello de botella* que surgen en el tema de costo computacional, almacenamiento y comunicación. El paper busca brinda un panorama de las técnicas usadas actualmente como lo son los métodos de primer orden y aleatorización para escabilidad. Algo que resaltan los autores es el hecho de que estos nuevos algoritmos para Big Data están basados en principios básicos y que aceleran la solución de problemas clásicos.

Dada la existencia de algoritmos eficientes para el cómputo eficiente de soluciones óptimas y la habilidad para usar geometría convexa para la formulación y optimización de problemas en diversos campos de estudio, ha sido natural hacer una transición para tratar de utilizar estas técnicas a problemas de grandes conjuntos de datos. Estos últimos problemas no cuentan con precedentes ya que son problemas de texto, imágenes, internet que se manejan en niveles de terabytes o exabytes; estos problemas, a pesar de la existencia de cómputo en paralelo y distribuio, no pueden ser atacados con algoritmos clásicos, es por esto que los algoritmos de optimización convexa han tenido un resurgimiento.

La formulación básica de optimización para grandes datos se basa en:

$$F^* = min_{x} \{F(x) = f(x) + g(x) : x \in R^p\},̣$$

donde $f$ y $g$ son funciones convexas.

Por otra parte, los algoritmos de optimización para Big Data descansan sobre los siguientes aspectos:

+ Métodos de primer orden. Éstos obtienen soluciones numéricas de precisión media o incluso baja usando orden del objetivo, como lo pueden ser estimaciones de gradiente. Estos métodos poseen tasas de convergencia prácticamente independiente a la dimensión del problema, son robustos teóricamente y descansas en principios que son ideales para el cómputo distribuido y en paralelo.

+ Aleatorización: Estas técnicas resaltan de las demás para mejorar la escabilidad de los métodos de primer orden ya que se puede controlar su comportamiento esperado. Las ideas principales se basan en actualizaciones parciales aleatorias de variables de optimización, remplazando el gradiente determinístico con estimadores estadísticos computacionalmente baratos, e incrementando la velocidad de rutinas de álgebra lineal por medio de aleatorización.

+ Cómputo en paralelo y distribuido: Estos métodos pueden aumentar su capacidad y escabilidad por medio de algoritmos asíncronos con comunicaciones decentralizadas.

Estos tres conceptos mencionados logran complementarse entre sí ofreciendo beneficios y escabilidad para la optimización de problemas de gran escala.


De igual modo resulta que estos enfoques también brindan en cierto punto problemáticas. Por ejemplo, en el caso de métodos de primer orden usando hardware heterogéneo y distribuido surgen problemáticas de comunicación y sincronización. En el artículo mencionan una diversidad de algoritmos con ciertas modificaciones para un mejor funcionamiento cuando se trata de procesar grandes cantidades de datos. Existe, por ejemplo, el método *ADMM* el cual está bien adecuado para optimización distribuida; también están los métodos de descenso coordinado el cual logra mejores resultados mediante métodos de aleatorización.

Para problemas de gran escala y grandes datos, las operaciones básicas de álgebra lineal como descomposiciones y multiplicaciones resultan ser computacionalmente complicadas debido a las grandes dimensiones en las que se está trabajando. Sin embargo, la existencia de representaciones matriciales de rango bajo mejoran de gran manera la eficiencia de los métodos de aleatorización. Este tipo de procedimientos acelaran el cálculo de las aproximaciones dependiendo de los valores espectrales de la matriz, también esto descansa en el hecho de encontrar gradientes estimados (insesgados) para matrices, y finalmente permite aproximar con el fin de obtener iteraciones más baratas computacionalmente.

En general, los problemas de Big Data están confinados de manera epistémica y natural por la disponibilidad de hardware y problemas de comunicación, lo cual dictamina la elección del algoritmo a implementar. En el artículo comentan que se espera que en el futuro se puedan desarrollar herramientas o algoritmos de convexidad que puedan adaptarse a la hetoregeindad de las plataformas de cómputo.






