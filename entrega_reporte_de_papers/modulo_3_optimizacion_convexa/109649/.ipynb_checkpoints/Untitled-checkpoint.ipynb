{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Reporte Módulo 3: Cómputo matricial</h1> \n",
    "\n",
    "<h2 align=\"center\">Convex Optimization for Big Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se anaaliza en este artículo temas específicos de optimización convexacon el objetivo de reducir los cuellos de botella tanto de transferencia de datos como de almacenamiento y comunicación. Se busca dar un análisis a métodos de primer órden y métodos de escalabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema básico de los algoritmos convexos de optimización ha sido planteado desde diferentes contextos: desde el incremento exponencial que ha tenido la capacidad de generar, almacenar e implementar información asociada a problemas relevantes para hacer modelado de problemas de optimización, así como la necesidad de escalar aún másla capacidad de los algoritmos tradicionales para hacerlo. Por ptra parte el creciente desarrollo que ha tenido la teoría de matrices ralas y la aparición de nuevos algoritmos de optimización para entornos sparce.\n",
    "\n",
    "Por otro lado la aparición de nuevos campos de implementación de algoritmos de optimización tales como servicios en nube y dinámicas poblacionales y de audiencias en entornos en línea y fuera de internet con métricas sociales nuevas y entornos sociales distintos.\n",
    "\n",
    "Es por esto que la teoría de optimización convexa busca reinventarse para entornos de datos a gran escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El problema de optimización con datos de gran escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se da la formulación básica de un problema de optimización con grandes volúmenes de datos:\n",
    "\n",
    "$ F^{*} = min_{x} \\{ F(x) = f(x) + g(x) : x \\in \\mathbb{R}^{p} \\} $\n",
    "\n",
    "donde $ f $ y $ g $ son funciones convexas.\n",
    "\n",
    "El entendimiento básico de los algoritmos de optimización para datos de gran escala son:\n",
    "\n",
    " - Métodos de primer orden: Obtienen baja o mediana precisión numérica enlas soluciones utilizando solamente información de promer orden de la función objetivo, tales como estimaciones del gradiente.\n",
    " \n",
    " - Aleatorización: Mejoran la escalabilidad de los métodos de primer orden ya que su comportamiento puede ser controlado. Se puede aleatorizar mediante actualizaciones parciales de variables y la sustitución de gradientes deterministas  con estimaores estadísticos de bajo costo computacional.\n",
    " \n",
    " - Cómputo paralelo y distribuido: Se utilizan para aumentar laescabilidad que proveen los previos dos tipos de algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de primer orden:\n",
    "\n",
    "Tienen fundamento en la resolución de problemas lineales, loscuales se plantean como primera aproximación a la resolución de un problema, han sido largamente estudiados y cuentan ya conn una teoría muy robusta y extensa. Para entornos de datos de gran escala en los cuales se cuenta con grandes secciones sparce pueden utilizarce subespaciois de Krylov y paradigmas LASSO (least absolute shrinkage and selection operator).\n",
    "\n",
    "Para mejorar el rendimiento y la escalabilidad de un problema se implementa la aproximación estocastica con métodos de primer orden sustituyendo un método tan simple como el descenso por gradiente con tamaño de paso fijo con una aproximación estadística muy simple, con lo cual se establece la base de un metodo escalable de forma muy simple.\n",
    "\n",
    "En el caso de optimización para funciones que no son derivables en algún punto el uso del gradiente proximal juega un papel muy relevante en la resolución de estos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Suaves\n",
    "\n",
    "La forma usual de abordar un problema simple y lineal será el de aproximación por gradiente con un tamaño de paso adecuado para asegurar convergencia, aunque no es el método más eficiente en cuanto al número de iteraciones necesarias para llegar a una solución del problema sí es el que requiere menos información acerca de la función objetivo para poder encontrar el óptimo.\n",
    "Para este tipo de algoritmos solo se hará uso del cálculo del gradiente para lograrlo, métodos Newton requerirán de mayor cantidad de cálculos para poder lograr una solución con menos iteraciones. Aunque requieran de un maytor número de cálculos estos métodos son preferidos ya que estos cálculos tienen un costo computacional muy bajo, más bajo que las iteraciones adicionales que se requerírían si no se hicieran de esta forma.\n",
    "\n",
    "Otra forma estudiada para llegar a un óptimoha sido el de hacer especulaciones sobra la función objetivo. Analizando métodos de descenso por gradiente una de las cosas que se suele asumir es que el gradiente del objetivo es Lipschitz contínuo para alguna constante $ L $:\n",
    "\n",
    "$ \\forall x, y \\in \\mathbb{R}^{p}, ||\\nabla f(x) - \\nabla f(y)|| \\leq L || x - y ||_{2} $\n",
    "\n",
    "cuando f es dos veces diferenciable es condición suficiente para que $ L $ sea cota superior de los valores propios de la matriz Hessiana de f.\n",
    "\n",
    "Puedes usarse este tipo de especulaciones para seguir con otro tipo de problmeas, como los no restringidos.çejemplo de esto es el algoritmo de aceleración de gradiente de Nesterov para optimización libre. Este algoritmo alcanza el mejor caso de proporción de error y es considerado un método de primer orden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cap1.png\" width=\"750\">\n",
    "\n",
    "Con lo cual se reduce la complejidad computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones no suaves \n",
    "\n",
    "En el caso de funciones que sean no diferenciables en cierto puntos, se presenta una reducción significativa de la eficiencia de los métodos anteriores.\n",
    "Los métodos de grandientes proximales toman ventaja de la estructura del problema para mantener los mismos índices de convergencia a la solución.\n",
    "\n",
    "<img src=\"Cap2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalamiento vía randomización\n",
    "\n",
    "Un gran ejemoplo de este paradigma es el PAGE_RAN de google.\n",
    "Los algoritmos funcionales para este enfoque se basan en descenso coordinado por gradiente relacionados con métodos clásicos como el deGauss - Siedel de reduccion cíclica para solucionar sistemas de ecuaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cap3.png\" width=\"350\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
